{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "X0vUp7zpg3qF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shuyu\\Desktop\\论文复现\\Online_LLM_Selection\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\shuyu\\AppData\\Local\\Temp\\ipykernel_28184\\3909842351.py:8: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "import tiktoken\n",
    "import google.generativeai as genai\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_FEATURE_DIM = 384  # Dimension for question embeddings\n",
    "ANSWER_EMBED_DIM = 384  # Dimension for answer embeddings\n",
    "CASCADE_LENGTH = 5      # Number of attempts in the cascade (K)\n",
    "UPDATE_FREQUENCY = 1    # Update JSON records after every question\n",
    "USE_EMBEDDINGS = True   # Use embeddings instead of TF-IDF\n",
    "ALPHA = 0.675           # Exploration parameter for LinUCB\n",
    "LAMBDA_REG = 0.45       # Regularization parameter for LinUCB matrix initialization\n",
    "TRAIN_RATIO = 0.2\n",
    "BATCH_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h33tyfzEg7e7"
   },
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY =  \"\"\n",
    "OPENROUTER_BASE_URL =  \"https://openrouter.ai/api/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_zzs9BMtfLgr"
   },
   "outputs": [],
   "source": [
    "MODELS_CONFIG = {\n",
    "    \"arcee-ai/trinity-large-preview:free\": {\"input_cost\": 0, \"output_cost\": 0},\n",
    "    \"mistralai/mistral-small-3.1-24b-instruct\": {\"input_cost\": 0.05 / 1e6, \"output_cost\": 0.15 / 1e6},\n",
    "    \"microsoft/phi-4\": {\"input_cost\": 0.07 / 1e6, \"output_cost\": 0.14 / 1e6},\n",
    "    \"meta-llama/llama-4-maverick\": {\"input_cost\": 0.17 / 1e6, \"output_cost\": 0.16 / 1e6},\n",
    "    \"google/gemini-2.0-flash-001\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
    "    \"deepseek/deepseek-chat\": {\"input_cost\": 0.38 / 1e6, \"output_cost\": 0.89 / 1e6},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Original models\n",
    "# MODELS_CONFIG = {\n",
    "#     \"microsoft/phi-3.5-mini-128k-instruct\": {\"input_cost\": 0.03 / 1e6, \"output_cost\": 0.09 / 1e6},\n",
    "#     \"mistralai/mistral-small-3.1-24b-instruct\": {\"input_cost\": 0.05 / 1e6, \"output_cost\": 0.15 / 1e6},\n",
    "#     \"microsoft/phi-4\": {\"input_cost\": 0.07 / 1e6, \"output_cost\": 0.14 / 1e6},\n",
    "#     \"meta-llama/llama-4-maverick\": {\"input_cost\": 0.17 / 1e6, \"output_cost\": 0.16 / 1e6},\n",
    "#     \"google/gemini-2.0-flash-001\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
    "#     \"openai/gpt-4.1-nano\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
    "#     \"deepseek/deepseek-chat\": {\"input_cost\": 0.38 / 1e6, \"output_cost\": 0.89 / 1e6},\n",
    "# }\n",
    "\n",
    "GRADER_MODEL_NAME = \"google/gemini-2.0-flash-lite-001\"\n",
    "\n",
    "AVAILABLE_LLMS = list(MODELS_CONFIG.keys())\n",
    "LLM_ID_DIM = len(AVAILABLE_LLMS)\n",
    "\n",
    "# Feature dimensions are recalculated based on the number of available LLMs\n",
    "CONTEXT_FEATURE_DIM = ANSWER_EMBED_DIM + LLM_ID_DIM + ANSWER_EMBED_DIM\n",
    "TOTAL_FEATURE_DIM = BASE_FEATURE_DIM + 1 + CONTEXT_FEATURE_DIM\n",
    "\n",
    "# Initialize OpenRouter client\n",
    "openrouter_client = OpenAI(\n",
    "    base_url=OPENROUTER_BASE_URL,\n",
    "    api_key=OPENROUTER_API_KEY,\n",
    "    # You might need to add default headers if required by your OpenRouter setup/account\n",
    "    # default_headers={\"HTTP-Referer\": \"YOUR_SITE_URL\", \"X-Title\": \"YOUR_APP_NAME\"}\n",
    ")\n",
    "\n",
    "# File paths\n",
    "INPUT_JSON = os.path.join(\"..\", \"Data\", \"HotpotQA.json\")\n",
    "RECORDS_PATH = \"HP.json\"\n",
    "LINUCB_MODEL_PATH = \"HP.npz\"\n",
    "SUMMARY_STATS_PATH = \"HP.txt\"\n",
    "BACKUP_SUFFIX = \".bak\"\n",
    "\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, feature_dim=BASE_FEATURE_DIM, use_embeddings=USE_EMBEDDINGS):\n",
    "        self.feature_dim = feature_dim\n",
    "        self.use_embeddings = use_embeddings\n",
    "        if use_embeddings:\n",
    "            try:\n",
    "                self.embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
    "                print(\"Initialized sentence transformer embedding model.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error initializing sentence transformer: {e}\\nFalling back to TF-IDF.\")\n",
    "                self.use_embeddings = False\n",
    "        if not self.use_embeddings:\n",
    "            self.vectorizer = TfidfVectorizer()\n",
    "            self.svd = None\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, questions):\n",
    "        \"\"\"Initialize the vectorizer with the corpus of questions.\"\"\"\n",
    "        if not self.use_embeddings:\n",
    "            from sklearn.decomposition import TruncatedSVD\n",
    "            all_text = [q[\"question\"] for q in questions]\n",
    "            dtm = self.vectorizer.fit_transform(all_text)\n",
    "            n_components = min(self.feature_dim, dtm.shape[1])\n",
    "            self.svd = TruncatedSVD(n_components=n_components)\n",
    "            self.svd.fit(dtm)\n",
    "            print(f\"Using TF-IDF with SVD dimensionality reduction to {n_components} features.\")\n",
    "        self.initialized = True\n",
    "\n",
    "    def extract_features(self, question):\n",
    "        \"\"\"Extract features from a question.\"\"\"\n",
    "        if not self.initialized:\n",
    "            raise ValueError(\"Feature extractor not initialized. Call initialize() first.\")\n",
    "        text = question[\"question\"]\n",
    "        if self.use_embeddings:\n",
    "            return self.embedding_model.encode([text])[0]\n",
    "        else:\n",
    "            tfidf_vector = self.vectorizer.transform([text])\n",
    "            features = self.svd.transform(tfidf_vector)[0]\n",
    "            if len(features) < self.feature_dim:\n",
    "                padding = np.zeros(self.feature_dim - len(features))\n",
    "                features = np.concatenate([features, padding])\n",
    "            return features\n",
    "\n",
    "    def extract_answer_features(self, answer_text):\n",
    "        \"\"\"Extract features from an answer string.\"\"\"\n",
    "        if not answer_text:\n",
    "            return np.zeros(ANSWER_EMBED_DIM)\n",
    "        if self.use_embeddings:\n",
    "            try:\n",
    "                features = self.embedding_model.encode([answer_text])[0]\n",
    "                # Handle cases where embedding dimension might not match\n",
    "                if len(features) != ANSWER_EMBED_DIM:\n",
    "                    if len(features) > ANSWER_EMBED_DIM:\n",
    "                        features = features[:ANSWER_EMBED_DIM]\n",
    "                    else:\n",
    "                        padding = np.zeros(ANSWER_EMBED_DIM - len(features))\n",
    "                        features = np.concatenate([features, padding])\n",
    "                return features\n",
    "            except Exception as e:\n",
    "                print(f\"Error embedding answer: {e}\")\n",
    "                return np.zeros(ANSWER_EMBED_DIM)\n",
    "        else:\n",
    "            # For simplicity, use zero vector if not using embeddings for answers\n",
    "            return np.zeros(ANSWER_EMBED_DIM)\n",
    "\n",
    "    def construct_feature_vector(self, base_features, step_i, failed_answers, failed_llm_ids, model_name_to_index):\n",
    "        \"\"\"\n",
    "        Construct the augmented feature vector for LinUCB.\n",
    "        Features include: base question features, normalized step, last answer embedding,\n",
    "        one-hot encoding of the last failed LLM, and an average of all previous failed answer embeddings.\n",
    "        \"\"\"\n",
    "        normalized_step = np.array([step_i / CASCADE_LENGTH])\n",
    "\n",
    "        last_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
    "        if step_i > 1 and failed_answers:\n",
    "            last_answer_features = self.extract_answer_features(failed_answers[-1])\n",
    "\n",
    "        last_llm_onehot = np.zeros(LLM_ID_DIM)\n",
    "        if step_i > 1 and failed_llm_ids:\n",
    "            last_llm_name = failed_llm_ids[-1]\n",
    "            if last_llm_name in model_name_to_index:\n",
    "                last_llm_onehot[model_name_to_index[last_llm_name]] = 1.0\n",
    "\n",
    "        avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
    "        if step_i > 1 and failed_answers:\n",
    "            all_answer_features = [self.extract_answer_features(ans) for ans in failed_answers]\n",
    "            if all_answer_features:\n",
    "                avg_answer_features = np.mean(all_answer_features, axis=0)\n",
    "        if avg_answer_features.shape == (): # Handle scalar result from mean\n",
    "            avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
    "\n",
    "        context_features = np.concatenate([last_answer_features, last_llm_onehot, avg_answer_features])\n",
    "        augmented_features = np.concatenate([base_features, normalized_step, context_features])\n",
    "\n",
    "        if augmented_features.shape[0] != TOTAL_FEATURE_DIM:\n",
    "            raise ValueError(f\"Constructed feature vector dimension {augmented_features.shape[0]} != expected {TOTAL_FEATURE_DIM}\")\n",
    "        return augmented_features\n",
    "\n",
    "\n",
    "class LinUCBModel:\n",
    "    def __init__(self, model_names, feature_dim=TOTAL_FEATURE_DIM, alpha=ALPHA, lambda_reg=LAMBDA_REG):\n",
    "        self.model_names = model_names\n",
    "        self.feature_dim = feature_dim\n",
    "        self.alpha = alpha\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.model_name_to_index = {name: i for i, name in enumerate(model_names)}\n",
    "        self.models = {\n",
    "            model_name: {\n",
    "                'A': np.identity(feature_dim) * lambda_reg,\n",
    "                'b': np.zeros(feature_dim),\n",
    "                'last_call_time': 0,\n",
    "            } for model_name in model_names\n",
    "        }\n",
    "\n",
    "    def update_reward_only(self, model_name, feature_vector, reward):\n",
    "        \"\"\"Update the LinUCB model parameters based on observed reward.\"\"\"\n",
    "        model = self.models[model_name]\n",
    "        model['A'] += np.outer(feature_vector, feature_vector)\n",
    "        model['b'] += feature_vector * reward\n",
    "\n",
    "    def calculate_ucb_scores(self, feature_vector):\n",
    "        \"\"\"Calculate UCB scores for model selection.\"\"\"\n",
    "        scores = {}\n",
    "        for model_name in self.model_names:\n",
    "            model = self.models[model_name]\n",
    "            try:\n",
    "                # Use Cholesky decomposition for faster and more stable inversion\n",
    "                L = np.linalg.cholesky(model['A'])\n",
    "                theta = np.linalg.solve(model['A'], model['b'])\n",
    "                z = np.linalg.solve(L, feature_vector)\n",
    "                ucb_term = self.alpha * np.sqrt(np.sum(z**2))\n",
    "                expected_reward = feature_vector.dot(theta)\n",
    "                scores[model_name] = {\n",
    "                    \"p_ia\": float(expected_reward),\n",
    "                    \"e_ia\": float(ucb_term),\n",
    "                    \"ucb_score\": float(expected_reward + ucb_term)\n",
    "                }\n",
    "            except np.linalg.LinAlgError:\n",
    "                # Fallback to standard matrix inversion if Cholesky fails\n",
    "                try:\n",
    "                    A_inv = np.linalg.inv(model['A'])\n",
    "                    theta = A_inv.dot(model['b'])\n",
    "                    ucb_term = self.alpha * np.sqrt(feature_vector.dot(A_inv).dot(feature_vector))\n",
    "                    expected_reward = feature_vector.dot(theta)\n",
    "                    scores[model_name] = {\n",
    "                        \"p_ia\": float(expected_reward),\n",
    "                        \"e_ia\": float(ucb_term),\n",
    "                        \"ucb_score\": float(expected_reward + ucb_term)\n",
    "                    }\n",
    "                except:\n",
    "                    scores[model_name] = {\"p_ia\": 0.0, \"e_ia\": 0.0, \"ucb_score\": 0.0}\n",
    "        return scores\n",
    "\n",
    "    def select_model_ucb(self, feature_vector):\n",
    "        \"\"\"Select a model using standard LinUCB algorithm.\"\"\"\n",
    "        scores = self.calculate_ucb_scores(feature_vector)\n",
    "        if not scores:\n",
    "            return None, {}\n",
    "        chosen_model = max(scores.items(), key=lambda x: x[1][\"ucb_score\"])[0]\n",
    "        return chosen_model, scores\n",
    "\n",
    "    def register_model_call(self, model_name):\n",
    "        self.models[model_name]['last_call_time'] = time.time()\n",
    "\n",
    "    def respect_rate_limit(self, model_name):\n",
    "        \"\"\"Wait if necessary to respect a model's RPM if defined in config.\"\"\"\n",
    "        # For the specified OpenRouter models, 'rpm' is not defined, so this block will be skipped.\n",
    "        model_cfg = MODELS_CONFIG.get(model_name)\n",
    "        if model_cfg and \"rpm\" in model_cfg:\n",
    "            model_state = self.models[model_name]\n",
    "            min_seconds_between_calls = 60.0 / model_cfg[\"rpm\"]\n",
    "            time_since_last_call = time.time() - model_state['last_call_time']\n",
    "            if time_since_last_call < min_seconds_between_calls:\n",
    "                time.sleep(min_seconds_between_calls - time_since_last_call)\n",
    "\n",
    "    def save_model_state(self, file_path):\n",
    "        \"\"\"Save the model state to a compressed numpy file.\"\"\"\n",
    "        save_dict = {f'A_{model_name}': model['A'] for model_name, model in self.models.items()}\n",
    "        for model_name, model in self.models.items():\n",
    "            save_dict[f'b_{model_name}'] = model['b']\n",
    "        np.savez_compressed(file_path, **save_dict)\n",
    "\n",
    "    def load_model_state(self, file_path):\n",
    "        \"\"\"Load the model state from a file.\"\"\"\n",
    "        try:\n",
    "            loaded = np.load(file_path)\n",
    "            for model_name in self.models.keys():\n",
    "                if f'A_{model_name}' in loaded and f'b_{model_name}' in loaded:\n",
    "                    self.models[model_name]['A'] = loaded[f'A_{model_name}']\n",
    "                    self.models[model_name]['b'] = loaded[f'b_{model_name}']\n",
    "            print(f\"Loaded LinUCB model state from {file_path}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model state: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "class BatchBudgetCascade:\n",
    "    def __init__(self, feature_extractor, linucb_model, cascade_length=CASCADE_LENGTH):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.linucb_model = linucb_model\n",
    "        self.cascade_length = cascade_length\n",
    "\n",
    "    def format_prompt(self, question, failed_answers=None, failed_llm_ids=None):\n",
    "        \"\"\"Generate a prompt for HotpotQA-style short-answer QA.\"\"\"\n",
    "        prompt = \"You are a question answering assistant.\\n\\n\"\n",
    "        prompt += \"Answer the question with ONLY the final short answer.\\n\"\n",
    "        prompt += \"Do NOT provide reasoning, steps, or extra text.\\n\"\n",
    "        prompt += \"If the answer is yes/no, respond with exactly 'yes' or 'no'.\\n\"\n",
    "        prompt += f\"Question: {question['question']} \\n\"\n",
    "        prompt += \"Answer:\"\n",
    "        \n",
    "        if failed_answers and failed_llm_ids:\n",
    "            prompt += \"\\n\\nNote: The following previous attempts were incorrect. Please provide a different solution:\\n\"\n",
    "            for i, answer_info in enumerate(failed_answers):\n",
    "                prompt += f\"- Attempt {i+1} (by {failed_llm_ids[i]}) led to: {answer_info}\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def parse_llm_answer(self, answer_text):\n",
    "        \"\"\"\n",
    "        Extract a short answer string from the model output.\n",
    "        Returns a plain string (NOT a tuple) so downstream embeddings/history work.\n",
    "        \"\"\"\n",
    "        if answer_text is None:\n",
    "            return \"\"\n",
    "        s = str(answer_text).strip()\n",
    "        if not s:\n",
    "            return \"\"\n",
    "\n",
    "        # Prefer content after an 'Answer:' marker if present\n",
    "        m = re.search(r\"(?i)\\banswer\\s*:\\s*(.*)\", s)\n",
    "        if m:\n",
    "            s = m.group(1).strip()\n",
    "\n",
    "        # Some models respond with JSON-ish snippets\n",
    "        m_json = re.search(r'(?i)\"answer\"\\s*:\\s*\"([^\"]+)\"', s)\n",
    "        if m_json:\n",
    "            s = m_json.group(1).strip()\n",
    "\n",
    "        # Keep first non-empty line\n",
    "        lines = [ln.strip() for ln in s.splitlines() if ln.strip()]\n",
    "        s = lines[0] if lines else \"\"\n",
    "\n",
    "        # Strip common wrappers\n",
    "        s = s.strip(\" \\t\\r\\n\\\"'`“”‘’\")\n",
    "        if s.startswith('- '):\n",
    "            s = s[2:].strip()\n",
    "        return s\n",
    "\n",
    "    def _coerce_to_text(self, ans):\n",
    "        \"\"\"Coerce different answer representations into a plain string.\"\"\"\n",
    "        if ans is None:\n",
    "            return \"\"\n",
    "        # Backward compatibility: older versions returned (parsed, raw)\n",
    "        if isinstance(ans, (tuple, list)):\n",
    "            for part in ans:\n",
    "                if part is None:\n",
    "                    continue\n",
    "                if isinstance(part, str) and part.strip():\n",
    "                    return part\n",
    "            return str(ans[0]) if len(ans) > 0 else \"\"\n",
    "        if isinstance(ans, str):\n",
    "            return ans\n",
    "        return str(ans)\n",
    "\n",
    "    def normalize_hotpotqa_answer(self, s):\n",
    "        \"\"\"Normalize answers for HotpotQA-style EM matching (SQuAD-like).\"\"\"\n",
    "        if s is None:\n",
    "            return \"\"\n",
    "        s = self._coerce_to_text(s)\n",
    "        s = s.strip()\n",
    "        if not s:\n",
    "            return \"\"\n",
    "        # Drop common prefixes\n",
    "        s = re.sub(r\"^(answer|final answer)\\s*:\\s*\", \"\", s, flags=re.IGNORECASE).strip()\n",
    "        # Keep first line to reduce extra chatter\n",
    "        s = s.splitlines()[0].strip()\n",
    "        # Strip surrounding quotes/backticks\n",
    "        s = s.strip(\" \\t\\r\\n\\\"'`“”‘’\")\n",
    "        s = s.lower()\n",
    "        # Normalize yes/no variants early\n",
    "        if s in {\"yes.\", \"yes!\", \"yes,\"}:\n",
    "            s = \"yes\"\n",
    "        if s in {\"no.\", \"no!\", \"no,\"}:\n",
    "            s = \"no\"\n",
    "        # Remove punctuation (unicode-aware) -> space\n",
    "        s = re.sub(r\"[^\\w\\s]\", \" \", s, flags=re.UNICODE)\n",
    "        # Remove articles\n",
    "        s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
    "        # Collapse whitespace\n",
    "        s = \" \".join(s.split())\n",
    "        return s\n",
    "\n",
    "    def hotpotqa_f1(self, pred, gold):\n",
    "        \"\"\"Token-level F1 (SQuAD-style) over normalized answers.\"\"\"\n",
    "        pred_norm = self.normalize_hotpotqa_answer(pred)\n",
    "        gold_norm = self.normalize_hotpotqa_answer(gold)\n",
    "        pred_tokens = pred_norm.split() if pred_norm else []\n",
    "        gold_tokens = gold_norm.split() if gold_norm else []\n",
    "        if not pred_tokens or not gold_tokens:\n",
    "            return 0.0\n",
    "\n",
    "        pred_counts = {}\n",
    "        for t in pred_tokens:\n",
    "            pred_counts[t] = pred_counts.get(t, 0) + 1\n",
    "        gold_counts = {}\n",
    "        for t in gold_tokens:\n",
    "            gold_counts[t] = gold_counts.get(t, 0) + 1\n",
    "\n",
    "        num_same = 0\n",
    "        for t, pc in pred_counts.items():\n",
    "            gc = gold_counts.get(t, 0)\n",
    "            if gc:\n",
    "                num_same += min(pc, gc)\n",
    "\n",
    "        if num_same == 0:\n",
    "            return 0.0\n",
    "        precision = num_same / len(pred_tokens)\n",
    "        recall = num_same / len(gold_tokens)\n",
    "        return (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    def grade_with_gemma12b(self, llm_answer, ground_truth_answer):\n",
    "        \"\"\"\n",
    "        HotpotQA grading: normalized Exact Match (EM) OR token-level F1 >= threshold.\n",
    "        NOTE: Kept the old method name for minimal downstream changes.\n",
    "        \"\"\"\n",
    "        pred = self.normalize_hotpotqa_answer(llm_answer)\n",
    "        gold = self.normalize_hotpotqa_answer(ground_truth_answer)\n",
    "        if not pred or not gold:\n",
    "            return False\n",
    "        if pred == gold:\n",
    "            return True\n",
    "        return self.hotpotqa_f1(pred, gold) >= 0.8\n",
    "\n",
    "    def calculate_token_cost(self, model_name, prompt, response_text, usage_info=None):\n",
    "        \"\"\"Calculate the actual cost. For OpenRouter, relies on usage_info from API response.\"\"\"\n",
    "        total_cost = 0.0\n",
    "        error_message = None\n",
    "        if model_name in MODELS_CONFIG:\n",
    "            model_cfg = MODELS_CONFIG[model_name]\n",
    "            if usage_info:\n",
    "                input_tokens = usage_info.get(\"prompt_tokens\", 0)\n",
    "                output_tokens = usage_info.get(\"completion_tokens\", 0)\n",
    "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
    "            else:\n",
    "                error_message = f\"Usage info not available for {model_name}. Cost is a rough estimate.\"\n",
    "                input_tokens = len(prompt) // 4\n",
    "                output_tokens = len(response_text) // 4 if response_text else 0\n",
    "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
    "        else:\n",
    "            error_message = f\"Model {model_name} not found in config for cost calculation.\"\n",
    "        result = {\"total_cost\": total_cost}\n",
    "        if error_message:\n",
    "            result[\"error\"] = error_message\n",
    "            print(f\"Cost calculation warning for {model_name}: {error_message}\")\n",
    "        return result\n",
    "\n",
    "    def query_llm(self, model_name, prompt):\n",
    "        \"\"\"Query the specified LLM and return its response and cost.\"\"\"\n",
    "        answer_text, parsed_answer, cost_data = \"\", None, {}\n",
    "        try:\n",
    "            self.linucb_model.respect_rate_limit(model_name)\n",
    "            if model_name in MODELS_CONFIG:\n",
    "                api_response = openrouter_client.chat.completions.create(\n",
    "                    model=model_name,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                )\n",
    "                answer_text = api_response.choices[0].message.content.strip()\n",
    "                usage_info = api_response.usage.model_dump() if api_response.usage else None\n",
    "                cost_data = self.calculate_token_cost(model_name, prompt, answer_text, usage_info=usage_info)\n",
    "            else:\n",
    "                raise ValueError(f\"Model {model_name} is not configured in MODELS_CONFIG.\")\n",
    "            self.linucb_model.register_model_call(model_name)\n",
    "            parsed_answer = self.parse_llm_answer(answer_text)\n",
    "            return answer_text, parsed_answer, cost_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error querying LLM {model_name}: {e}\")\n",
    "            self.linucb_model.register_model_call(model_name)\n",
    "            cost_data = self.calculate_token_cost(model_name, prompt, \"\", usage_info=None)\n",
    "            return \"\", None, cost_data\n",
    "\n",
    "    def run_cascade_single_question(self, question):\n",
    "        \"\"\"Run the standard LinUCB cascade for a single question.\"\"\"\n",
    "        base_features = self.feature_extractor.extract_features(question)\n",
    "        failed_answers, failed_llm_ids, current_attempts_log = [], [], []\n",
    "        question_total_cost = 0.0\n",
    "        final_status = \"Failure\"\n",
    "\n",
    "        for i in range(1, self.cascade_length + 1):\n",
    "            print(f\"Step {i}\")\n",
    "            # 1. Construct feature vector with history\n",
    "            x_i = self.feature_extractor.construct_feature_vector(base_features, i, failed_answers, failed_llm_ids, self.linucb_model.model_name_to_index)\n",
    "            # 2. Select model based on UCB score\n",
    "            chosen_model, scores = self.linucb_model.select_model_ucb(x_i)\n",
    "            if not chosen_model:\n",
    "                print(\"Error: LinUCB failed to select a model.\")\n",
    "                break\n",
    "            print(f\"Selected: {chosen_model}, UCB: {scores[chosen_model]['ucb_score']:.4f}\")\n",
    "            # 3. Query the chosen model\n",
    "            prompt = self.format_prompt(question, failed_answers, failed_llm_ids)\n",
    "            raw_response, model_answer, cost_data = self.query_llm(chosen_model, prompt)\n",
    "            actual_cost = cost_data.get(\"total_cost\", 0.0)\n",
    "            question_total_cost += actual_cost\n",
    "            # 4. Grade the answer\n",
    "            is_correct = self.grade_with_gemma12b(model_answer, question['ground_truth_answer'])\n",
    "            reward = 1 if is_correct else 0\n",
    "            print(f\"Answer: {model_answer}, Correct: {is_correct}, Cost: ${actual_cost:.8f}\")\n",
    "            # 5. Update LinUCB model\n",
    "            self.linucb_model.update_reward_only(chosen_model, x_i, reward)\n",
    "            # 6. Log the attempt\n",
    "            current_attempts_log.append({\n",
    "                \"step\": i, \"chosen_model\": chosen_model, \"chosen_model_cost\": cost_data,\n",
    "                \"llm_answer\": model_answer, \"is_correct\": is_correct, \"reward_ri\": reward,\n",
    "                \"raw_response\": raw_response, \"scores_per_arm\": scores\n",
    "            })\n",
    "            # 7. Check for success\n",
    "            if is_correct:\n",
    "                final_status = \"Success\"\n",
    "                print(f\"Success in step {i}!\")\n",
    "                break\n",
    "            else:\n",
    "                failed_answers.append(model_answer if model_answer else \"ParsingFailed/NoAnswer\")\n",
    "                failed_llm_ids.append(chosen_model)\n",
    "\n",
    "        return {\n",
    "            \"question\": question[\"question\"],\n",
    "            \"ground_truth_answer\": question[\"ground_truth_answer\"],\n",
    "            \"unique_id\": question.get(\"unique_id\"),\n",
    "            \"subject\": question.get(\"subject\"),\n",
    "            \"level\": question.get(\"level\"),\n",
    "            \"final_status\": final_status,\n",
    "            \"total_cost\": question_total_cost,\n",
    "            \"steps_taken\": len(current_attempts_log),\n",
    "            \"attempts\": current_attempts_log\n",
    "        }\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "llh_P_4Yh7kb"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_math500_dataset(json_path, split_key=\"test\"):\n",
    "    \"\"\"\n",
    "    Load and process the HotpotQA dataset (kept function name for compatibility).\n",
    "\n",
    "    Expected JSON format:\n",
    "      {\"test\": [{\"problem\": \"Question: ...\", \"answer\": \"...\", ...}, ...]}\n",
    "\n",
    "    Returns a list of dicts with keys:\n",
    "      - question\n",
    "      - ground_truth_answer\n",
    "      - subject / level / unique_id (if present)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        if isinstance(raw_data, dict):\n",
    "            items = raw_data.get(split_key)\n",
    "            if items is None:\n",
    "                # Fallback: if split_key doesn't exist, try common alternatives\n",
    "                for candidate in (\"test\", \"dev\", \"validation\", \"train\"):\n",
    "                    if candidate in raw_data:\n",
    "                        items = raw_data[candidate]\n",
    "                        break\n",
    "            if items is None:\n",
    "                raise KeyError(f\"No split found in dataset JSON. Tried '{split_key}' and common keys.\")\n",
    "        elif isinstance(raw_data, list):\n",
    "            # Rare case: dataset is directly a list\n",
    "            items = raw_data\n",
    "        else:\n",
    "            raise TypeError(f\"Unexpected dataset JSON type: {type(raw_data)}\")\n",
    "\n",
    "        processed_dataset = []\n",
    "        for item in items:\n",
    "            problem = (item.get(\"problem\") or \"\").strip()\n",
    "            # HotpotQA-style export often prefixes the question with \"Question:\"\n",
    "            if problem.lower().startswith(\"question:\"):\n",
    "                problem = problem.split(\":\", 1)[1].strip()\n",
    "\n",
    "            processed_dataset.append({\n",
    "                \"question\": problem,\n",
    "                \"options\": [],\n",
    "                \"ground_truth_answer\": (item.get(\"answer\") or \"\").strip(),\n",
    "                \"subject\": item.get(\"subject\", \"Unknown\"),\n",
    "                \"level\": item.get(\"level\", \"Unknown\"),\n",
    "                \"unique_id\": item.get(\"unique_id\", \"Unknown\"),\n",
    "            })\n",
    "\n",
    "        print(f\"Loaded {len(processed_dataset)} questions from {json_path} (split={split_key})\")\n",
    "        return processed_dataset\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading HotpotQA dataset: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_records_with_backup(records, json_path):\n",
    "    \"\"\"Save records to a JSON file with backup of previous file\"\"\"\n",
    "    # Create backup of existing file if it exists\n",
    "    if os.path.exists(json_path):\n",
    "        backup_path = json_path + BACKUP_SUFFIX\n",
    "        try:\n",
    "            os.replace(json_path, backup_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create backup: {e}\")\n",
    "\n",
    "    # Save new data\n",
    "    try:\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(records, f, indent=4)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving records: {e}\")\n",
    "\n",
    "        # Try to restore from backup if save failed\n",
    "        if os.path.exists(json_path + BACKUP_SUFFIX):\n",
    "            try:\n",
    "                os.replace(json_path + BACKUP_SUFFIX, json_path)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        return False\n",
    "\n",
    "def initialize_json_files():\n",
    "    \"\"\"Initialize the JSON files for records with validation\"\"\"\n",
    "    if not os.path.exists(RECORDS_PATH):\n",
    "        with open(RECORDS_PATH, 'w') as f:\n",
    "            json.dump([], f)  # Empty array\n",
    "    else:\n",
    "        # Validate existing file\n",
    "        try:\n",
    "            with open(RECORDS_PATH, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            if not isinstance(data, list):\n",
    "                os.rename(RECORDS_PATH, RECORDS_PATH + BACKUP_SUFFIX)\n",
    "                with open(RECORDS_PATH, 'w') as f:\n",
    "                    json.dump([], f)\n",
    "        except json.JSONDecodeError:\n",
    "            os.rename(RECORDS_PATH, RECORDS_PATH + BACKUP_SUFFIX)\n",
    "            with open(RECORDS_PATH, 'w') as f:\n",
    "                json.dump([], f)\n",
    "\n",
    "def calculate_metrics(records):\n",
    "    \"\"\"Helper function to calculate metrics for a subset of records\"\"\"\n",
    "    if not records:\n",
    "        return {\n",
    "            \"total_questions\": 0,\n",
    "            \"successful_questions\": 0,\n",
    "            \"success_rate\": 0,\n",
    "            \"total_steps\": 0,\n",
    "            \"avg_steps\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"avg_cost\": 0,\n",
    "            \"avg_cost_success\": 0,\n",
    "            \"successes_by_position\": [0] * CASCADE_LENGTH,\n",
    "            \"per_position_success\": [0] * CASCADE_LENGTH,\n",
    "            \"model_metrics\": {}\n",
    "        }\n",
    "\n",
    "    total_questions = len(records)\n",
    "    successful_questions = sum(1 for r in records if r[\"final_status\"] == \"Success\")\n",
    "    success_rate = successful_questions / total_questions if total_questions > 0 else 0\n",
    "\n",
    "    # Calculate success by position\n",
    "    successes_by_position = [0] * CASCADE_LENGTH\n",
    "    for record in records:\n",
    "        if record[\"final_status\"] == \"Success\":\n",
    "            position = len(record[\"attempts\"]) - 1  # 0-indexed\n",
    "            if position < CASCADE_LENGTH:\n",
    "                successes_by_position[position] += 1\n",
    "\n",
    "    per_position_success = [count/total_questions for count in successes_by_position]\n",
    "\n",
    "    # Calculate average steps and costs\n",
    "    total_steps = sum(r[\"steps_taken\"] for r in records)\n",
    "    total_cost = sum(r[\"total_cost\"] for r in records)\n",
    "\n",
    "    avg_steps = total_steps / total_questions if total_questions > 0 else 0\n",
    "    avg_cost = total_cost / total_questions if total_questions > 0 else 0\n",
    "\n",
    "    # Calculate average cost for successful questions only\n",
    "    if successful_questions > 0:\n",
    "        success_cost = sum(r[\"total_cost\"] for r in records if r[\"final_status\"] == \"Success\")\n",
    "        avg_cost_success = success_cost / successful_questions\n",
    "    else:\n",
    "        avg_cost_success = 0\n",
    "\n",
    "    # Analyze model performance\n",
    "    model_metrics = {model: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0, \"by_position\": defaultdict(lambda: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0})}\n",
    "                    for model in AVAILABLE_LLMS}\n",
    "\n",
    "    for record in records:\n",
    "        for attempt in record[\"attempts\"]:\n",
    "            position = attempt[\"step\"]\n",
    "            model = attempt[\"chosen_model\"]\n",
    "            is_correct = attempt[\"is_correct\"]\n",
    "            cost = attempt[\"chosen_model_cost\"][\"total_cost\"]\n",
    "\n",
    "            # Update overall stats\n",
    "            model_metrics[model][\"calls\"] += 1\n",
    "            model_metrics[model][\"total_cost\"] += cost\n",
    "            if is_correct:\n",
    "                model_metrics[model][\"successes\"] += 1\n",
    "\n",
    "            # Update position-specific stats\n",
    "            model_metrics[model][\"by_position\"][position][\"calls\"] += 1\n",
    "            model_metrics[model][\"by_position\"][position][\"total_cost\"] += cost\n",
    "            if is_correct:\n",
    "                model_metrics[model][\"by_position\"][position][\"successes\"] += 1\n",
    "\n",
    "    # Calculate success rates and average costs for models\n",
    "    for model, data in model_metrics.items():\n",
    "        if data[\"calls\"] > 0:\n",
    "            data[\"success_rate\"] = data[\"successes\"] / data[\"calls\"]\n",
    "            data[\"avg_cost\"] = data[\"total_cost\"] / data[\"calls\"]\n",
    "        else:\n",
    "            data[\"success_rate\"] = 0\n",
    "            data[\"avg_cost\"] = 0\n",
    "\n",
    "        for position, pos_data in data[\"by_position\"].items():\n",
    "            if pos_data[\"calls\"] > 0:\n",
    "                pos_data[\"success_rate\"] = pos_data[\"successes\"] / pos_data[\"calls\"]\n",
    "                pos_data[\"avg_cost\"] = pos_data[\"total_cost\"] / pos_data[\"calls\"]\n",
    "            else:\n",
    "                pos_data[\"success_rate\"] = 0\n",
    "                pos_data[\"avg_cost\"] = 0\n",
    "\n",
    "    return {\n",
    "        \"total_questions\": total_questions,\n",
    "        \"successful_questions\": successful_questions,\n",
    "        \"success_rate\": success_rate,\n",
    "        \"total_steps\": total_steps,\n",
    "        \"avg_steps\": avg_steps,\n",
    "        \"total_cost\": total_cost,\n",
    "        \"avg_cost\": avg_cost,\n",
    "        \"avg_cost_success\": avg_cost_success,\n",
    "        \"successes_by_position\": successes_by_position,\n",
    "        \"per_position_success\": per_position_success,\n",
    "        \"model_metrics\": model_metrics\n",
    "    }\n",
    "\n",
    "def analyze_results(records):\n",
    "    \"\"\"Analyze and print results from the records, separating train and test sets\"\"\"\n",
    "    if not records:\n",
    "        print(\"No records to analyze\")\n",
    "        return\n",
    "\n",
    "    # Separate records into train and test sets\n",
    "    train_records = [r for r in records if r.get(\"phase\") == \"train\"]\n",
    "    test_records = [r for r in records if r.get(\"phase\") == \"test\"]\n",
    "    overall_records = records\n",
    "\n",
    "    print(f\"Train Records: {len(train_records)}\")\n",
    "    print(f\"Test Records: {len(test_records)}\")\n",
    "    print(f\"Total Records: {len(overall_records)}\")\n",
    "\n",
    "    # Calculate metrics for each set\n",
    "    train_metrics = calculate_metrics(train_records)\n",
    "    test_metrics = calculate_metrics(test_records)\n",
    "    overall_metrics = calculate_metrics(overall_records)\n",
    "\n",
    "    # Analyze batch performance\n",
    "    batch_metrics = []\n",
    "    for i in range(0, len(records), BATCH_SIZE):\n",
    "        batch = records[i:i+BATCH_SIZE]\n",
    "        batch_success = sum(1 for r in batch if r[\"final_status\"] == \"Success\")\n",
    "        batch_success_rate = batch_success / len(batch) if batch else 0\n",
    "        batch_cost = sum(r[\"total_cost\"] for r in batch)\n",
    "        batch_avg_cost = batch_cost / len(batch) if batch else 0\n",
    "        batch_train = sum(1 for r in batch if r.get(\"phase\") == \"train\")\n",
    "        batch_test = sum(1 for r in batch if r.get(\"phase\") == \"test\")\n",
    "\n",
    "        batch_metrics.append({\n",
    "            \"batch_idx\": i // BATCH_SIZE,\n",
    "            \"batch_size\": len(batch),\n",
    "            \"train_count\": batch_train,\n",
    "            \"test_count\": batch_test,\n",
    "            \"success_count\": batch_success,\n",
    "            \"success_rate\": batch_success_rate,\n",
    "            \"total_cost\": batch_cost,\n",
    "            \"avg_cost\": batch_avg_cost\n",
    "        })\n",
    "\n",
    "    # Generate summary text\n",
    "    summary = \"=== LinUCB CASCADE WITH HOTPOTQA DATASET ===\\n\\n\"\n",
    "    summary += f\"Train Ratio: {TRAIN_RATIO*100}%\\n\"\n",
    "    summary += f\"LinUCB updated on BOTH Train and Test sets.\\n\"\n",
    "    summary += f\"Batch Size: {BATCH_SIZE}\\n\\n\"\n",
    "\n",
    "    # TRAIN SET RESULTS\n",
    "    summary += \"=== TRAIN SET RESULTS ===\\n\"\n",
    "    summary += f\"Total Train Questions: {train_metrics['total_questions']}\\n\"\n",
    "    summary += f\"Success Rate: {train_metrics['success_rate']:.4f}\\n\"\n",
    "    summary += f\"Average Steps: {train_metrics['avg_steps']:.4f}\\n\"\n",
    "    summary += f\"Average Cost per Question: ${train_metrics['avg_cost']:.8f}\\n\"\n",
    "    summary += f\"Average Cost per Successful Question: ${train_metrics['avg_cost_success']:.8f}\\n\"\n",
    "    summary += \"Success Rate by Position:\\n\"\n",
    "    for i, rate in enumerate(train_metrics['per_position_success']):\n",
    "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
    "\n",
    "    summary += \"\\nTrain Set Model Performance:\\n\"\n",
    "    for model, metrics in train_metrics['model_metrics'].items():\n",
    "        if metrics[\"calls\"] > 0:\n",
    "            summary += f\"\\n{model}:\\n\"\n",
    "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
    "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
    "            summary += \"  By Position:\\n\"\n",
    "            for position, pos_data in sorted(metrics[\"by_position\"].items()):\n",
    "                if pos_data[\"calls\"] > 0:\n",
    "                    summary += f\"    Pos {position}: {pos_data['successes']}/{pos_data['calls']} = {pos_data['success_rate']:.4f}, Avg Cost: ${pos_data['avg_cost']:.8f}\\n\"\n",
    "\n",
    "    # TEST SET RESULTS\n",
    "    summary += \"\\n\\n=== TEST SET RESULTS ===\\n\"\n",
    "    summary += f\"Total Test Questions: {test_metrics['total_questions']}\\n\"\n",
    "    summary += f\"Success Rate: {test_metrics['success_rate']:.4f}\\n\"\n",
    "    summary += f\"Average Steps: {test_metrics['avg_steps']:.4f}\\n\"\n",
    "    summary += f\"Average Cost per Question: ${test_metrics['avg_cost']:.8f}\\n\"\n",
    "    summary += f\"Average Cost per Successful Question: ${test_metrics['avg_cost_success']:.8f}\\n\"\n",
    "    summary += \"Success Rate by Position:\\n\"\n",
    "    for i, rate in enumerate(test_metrics['per_position_success']):\n",
    "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
    "\n",
    "    summary += \"\\nTest Set Model Performance:\\n\"\n",
    "    for model, metrics in test_metrics['model_metrics'].items():\n",
    "        if metrics[\"calls\"] > 0:\n",
    "            summary += f\"\\n{model}:\\n\"\n",
    "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
    "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
    "            summary += \"  By Position:\\n\"\n",
    "            for position, pos_data in sorted(metrics[\"by_position\"].items()):\n",
    "                if pos_data[\"calls\"] > 0:\n",
    "                    summary += f\"    Pos {position}: {pos_data['successes']}/{pos_data['calls']} = {pos_data['success_rate']:.4f}, Avg Cost: ${pos_data['avg_cost']:.8f}\\n\"\n",
    "\n",
    "    # OVERALL RESULTS\n",
    "    summary += \"\\n\\n=== OVERALL RESULTS (TRAIN + TEST) ===\\n\"\n",
    "    summary += f\"Total Overall Questions: {overall_metrics['total_questions']}\\n\"\n",
    "    summary += f\"Success Rate: {overall_metrics['success_rate']:.4f}\\n\"\n",
    "    summary += f\"Average Steps: {overall_metrics['avg_steps']:.4f}\\n\"\n",
    "    summary += f\"Average Cost per Question: ${overall_metrics['avg_cost']:.8f}\\n\"\n",
    "    summary += f\"Average Cost per Successful Question: ${overall_metrics['avg_cost_success']:.8f}\\n\"\n",
    "    summary += \"Success Rate by Position:\\n\"\n",
    "    for i, rate in enumerate(overall_metrics['per_position_success']):\n",
    "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
    "\n",
    "    summary += \"\\nOverall Model Performance:\\n\"\n",
    "    for model, metrics in overall_metrics['model_metrics'].items():\n",
    "        if metrics[\"calls\"] > 0:\n",
    "            summary += f\"\\n{model}:\\n\"\n",
    "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
    "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
    "            summary += \"  By Position:\\n\"\n",
    "            for position, pos_data in sorted(metrics[\"by_position\"].items()):\n",
    "                if pos_data[\"calls\"] > 0:\n",
    "                    summary += f\"    Pos {position}: {pos_data['successes']}/{pos_data['calls']} = {pos_data['success_rate']:.4f}, Avg Cost: ${pos_data['avg_cost']:.8f}\\n\"\n",
    "\n",
    "    summary += \"\\n=== BATCH PERFORMANCE ===\\n\"\n",
    "    for batch in batch_metrics:\n",
    "        summary += (\n",
    "            f\"Batch {batch['batch_idx']}: {batch['success_count']}/{batch['batch_size']} = {batch['success_rate']:.4f}, \"\n",
    "            f\"Train/Test: {batch['train_count']}/{batch['test_count']}, \"\n",
    "            f\"Total Cost: ${batch['total_cost']:.8f}, Avg Cost: ${batch['avg_cost']:.8f}\\n\"\n",
    "        )\n",
    "\n",
    "    summary += f\"\\nTotal Overall Cost (All Questions): ${overall_metrics['total_cost']:.8f}\\n\"\n",
    "    summary += f\"Total Train Cost: ${train_metrics['total_cost']:.8f}\\n\"\n",
    "    summary += f\"Total Test Cost: ${test_metrics['total_cost']:.8f}\\n\"\n",
    "\n",
    "    # Add interpretation note about test set updates\n",
    "    summary += \"\\n=== IMPORTANT NOTES ON INTERPRETATION ===\\n\"\n",
    "    summary += \"Since the LinUCB model is updated using data from both the train and test sets,\\n\"\n",
    "    summary += \"the test set performance metrics do not represent the performance of a fixed,\\n\"\n",
    "    summary += \"pre-trained model on unseen data. Instead, they reflect the model's performance\\n\"\n",
    "    summary += \"while it is still learning and adapting to the test data (online evaluation).\\n\"\n",
    "    summary += \"This setup allows us to observe how the bandit algorithm performs and adapts\\n\"\n",
    "    summary += \"over time across the entire dataset, comparing performance between an initial\\n\"\n",
    "    summary += \"phase (train) and a later phase (test).\\n\"\n",
    "\n",
    "    # Print and save summary\n",
    "    print(summary)\n",
    "\n",
    "    with open(SUMMARY_STATS_PATH, 'w') as f:\n",
    "        f.write(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SN8rbZZ0hPvt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LinUCB Cascade with HotpotQA Dataset\n",
      "Loaded 500 questions from ..\\Data\\HotpotQA.json (split=test)\n",
      "Using 100 questions: 20 train, 80 test.\n",
      "Initialized sentence transformer embedding model.\n",
      "Loaded 0 existing records. 100 new questions to process.\n",
      "\n",
      "--- Q1/100 (train) ---\n",
      "Question: Who did the Star and Dagger bass player marry?...\n",
      "Step 1\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.0262\n",
      "Answer: Rachel Barton Pine, Correct: False, Cost: $0.00000000\n",
      "Step 2\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 2.0523\n",
      "Answer: Suzanne Ciani, Correct: False, Cost: $0.00000615\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 2.0455\n",
      "Answer: Heather Koniges., Correct: False, Cost: $0.00001029\n",
      "Step 4\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 2.0938\n",
      "Answer: Heather Koniges, Correct: False, Cost: $0.00002766\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 2.1816\n",
      "Answer: Heather Koniges, Correct: False, Cost: $0.00002020\n",
      "\n",
      "--- Q2/100 (train) ---\n",
      "Question: What national historic district is located near a village in the town of Philipstown, New York?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0262\n",
      "Answer: Hudson Highlands Historic District, Correct: False, Cost: $0.00003359\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 2.0290\n",
      "Answer: yes, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 1.8748\n",
      "Answer: Putnam County Village of Fishkill Historic District. Also known as Hudson Highlands National Scenic Area, which provides protection., Correct: False, Cost: $0.00001281\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.9682\n",
      "Answer: Cold Spring Historic District, Correct: False, Cost: $0.00000940\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.9727\n",
      "Answer: Hudson Highlands State Park Historic District, Correct: False, Cost: $0.00003630\n",
      "\n",
      "--- Q3/100 (train) ---\n",
      "Question: A medieval fortress in Dirleton, East Lothian, Scotland borders on the south side of what coastal ar...\n",
      "Step 1\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 0.9959\n",
      "Answer: Firth of Forth, Correct: False, Cost: $0.00000980\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 2.0100\n",
      "Answer: Firth of Forth, Correct: False, Cost: $0.00004929\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 1.8548\n",
      "Answer: North Berwick Law, Correct: False, Cost: $0.00001071\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.9301\n",
      "Answer: Firth of Forth, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.8594\n",
      "Answer: North Berwick coast, more specifically, it is near the Firth of Forth but the correct answer is: North Berwick Law is not correct and Firth of Forth is too broad. However, the specific coastal area is the:, Correct: False, Cost: $0.00004191\n",
      "\n",
      "--- Q4/100 (train) ---\n",
      "Question: Which was fought earlier in our nation's history, the Seven Days Battles or the Battle of Manila?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 0.9877\n",
      "Answer: Seven Days Battles, Correct: True, Cost: $0.00000450\n",
      "Success in step 1!\n",
      "\n",
      "--- Q5/100 (train) ---\n",
      "Question: In what city did the \"Prince of tenors\" star in a film based on an opera by Giacomo Puccini?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1997\n",
      "Answer: New York City, Correct: False, Cost: $0.00000455\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.9016\n",
      "Answer: Rome, Correct: True, Cost: $0.00004852\n",
      "Success in step 2!\n",
      "\n",
      "--- Q6/100 (train) ---\n",
      "Question: What color clothing do people of the Netherlands wear during Oranjegekte or to celebrate the nationa...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0931\n",
      "Answer: Orange, Correct: True, Cost: $0.00000420\n",
      "Success in step 1!\n",
      "\n",
      "--- Q7/100 (train) ---\n",
      "Question: \"Ew!\" is a song by a television host born where?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1875\n",
      "Answer: Canada, Correct: False, Cost: $0.00000370\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 2.1052\n",
      "Answer: United States, Correct: False, Cost: $0.00004409\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.8355\n",
      "Answer: Canada, Correct: False, Cost: $0.00001440\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.8474\n",
      "Answer: United States, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.7836\n",
      "Answer: Canada, Correct: False, Cost: $0.00006980\n",
      "\n",
      "--- Q8/100 (train) ---\n",
      "Question: Which post DC Extended Universe actress will also play a role in what is intended to be the fifth in...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1227\n",
      "Answer: Gal Gadot, Correct: True, Cost: $0.00000465\n",
      "Success in step 1!\n",
      "\n",
      "--- Q9/100 (train) ---\n",
      "Question: Robert Suettinger was the national intelligence officer under which former Governor of Arkansas?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.2357\n",
      "Answer: Bill Clinton, Correct: False, Cost: $0.00000395\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.8562\n",
      "Answer: Bill Clinton, Correct: False, Cost: $0.00004523\n",
      "Step 3\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.8318\n",
      "Answer: Bill Clinton, Correct: False, Cost: $0.00002411\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.8779\n",
      "Answer: Bill Clinton, Correct: False, Cost: $0.00001169\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.8900\n",
      "Answer: Bill Clinton, Correct: False, Cost: $0.00001920\n",
      "\n",
      "--- Q10/100 (train) ---\n",
      "Question: What Danish eurodance group had a single called \"Barbie Girl\" which resulted in a lawsuit in 2002?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1678\n",
      "Answer: Aqua, Correct: True, Cost: $0.00000450\n",
      "Success in step 1!\n",
      "\n",
      "--- Q11/100 (train) ---\n",
      "Question: The Indian actor and philanthropist, who starred in \"Mera Damad,\" entered the film industry in what ...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1442\n",
      "Answer: 1969, Correct: False, Cost: $0.00000465\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.7964\n",
      "Answer: 1969, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.7728\n",
      "Answer: 1988, Correct: False, Cost: $0.00001780\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.7654\n",
      "Answer: 1979, Correct: False, Cost: $0.00001267\n",
      "Step 5\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.8831\n",
      "Answer: 1950, Correct: False, Cost: $0.00007639\n",
      "\n",
      "--- Q12/100 (train) ---\n",
      "Question: Which band is from England, Fireflight or Dirty Pretty Things?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3167\n",
      "Answer: Dirty Pretty Things, Correct: True, Cost: $0.00000395\n",
      "Success in step 1!\n",
      "\n",
      "--- Q13/100 (train) ---\n",
      "Question: Where does the hotel and casino located in which Bill Cosby's third album was recorded?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1170\n",
      "Answer: The Flamingo, Correct: False, Cost: $0.00000435\n",
      "Step 2\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.7854\n",
      "Answer: Las Vegas, Correct: False, Cost: $0.00001330\n",
      "Step 3\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.6778\n",
      "Answer: Harrah's Lake Tahoe, Correct: False, Cost: $0.00003785\n",
      "Step 4\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.8005\n",
      "Answer: Harrah's Lake Tahoe, Correct: False, Cost: $0.00007032\n",
      "Step 5\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.7452\n",
      "Answer: Harrah's Reno, Correct: False, Cost: $0.00001085\n",
      "\n",
      "--- Q14/100 (train) ---\n",
      "Question: The Tennessee Volunteers football team plays as a member for a conference in what city?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1947\n",
      "Answer: Knoxville, Correct: False, Cost: $0.00000415\n",
      "Step 2\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.7006\n",
      "Answer: Indianapolis, Correct: False, Cost: $0.00000625\n",
      "Step 3\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.6102\n",
      "Answer: Knoxville, Correct: False, Cost: $0.00005625\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5368\n",
      "Answer: Indianapolis, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.5792\n",
      "Answer: Indianapolis, Correct: False, Cost: $0.00002000\n",
      "\n",
      "--- Q15/100 (train) ---\n",
      "Question: Who programmed a cartridge-based development system made by a company noted for authoring Atari's BA...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1986\n",
      "Answer: David H. Ahl, Correct: False, Cost: $0.00000455\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.6576\n",
      "Answer: Al Alcorn, Correct: False, Cost: $0.00004878\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 1.5585\n",
      "Answer: David H. Ahl., Correct: False, Cost: $0.00001092\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5682\n",
      "Answer: David H. Ahl, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.5998\n",
      "Answer: Warren Robinett, Correct: False, Cost: $0.00003293\n",
      "\n",
      "--- Q16/100 (train) ---\n",
      "Question: Are both Cypress and Ajuga genera?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3182\n",
      "Answer: yes, Correct: False, Cost: $0.00000345\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.6261\n",
      "Answer: no, Correct: True, Cost: $0.00000000\n",
      "Success in step 2!\n",
      "\n",
      "--- Q17/100 (train) ---\n",
      "Question: Haruki Murakami said that writing \"Dance Dance Dance\" had been a healing act after his unexpected fa...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1457\n",
      "Answer: 1987, Correct: True, Cost: $0.00000510\n",
      "Success in step 1!\n",
      "\n",
      "--- Q18/100 (train) ---\n",
      "Question: What British singer-songwriter hosted the 16th Annual Young Hollywood Awards?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1204\n",
      "Answer: Lily Allen, Correct: False, Cost: $0.00000405\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.8476\n",
      "Answer: no, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.6138\n",
      "Answer: no, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4733\n",
      "Answer: Lily Allen, Correct: False, Cost: $0.00000860\n",
      "Step 5\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.5870\n",
      "Answer: Sophia Grace Brownlee, Correct: False, Cost: $0.00007754\n",
      "\n",
      "--- Q19/100 (train) ---\n",
      "Question: Who was the captain of the only battleship to provide gunfire support during the Vietnam War?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1447\n",
      "Answer: Willard S. Brown, Correct: False, Cost: $0.00000455\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.6930\n",
      "Answer: Willard S. Brown, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 1.6173\n",
      "Answer: USS Newport News was the only battleship to provide gunfire support, although typically no battleship's captain facilitated actual gunfire support., Correct: False, Cost: $0.00001407\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.5858\n",
      "Answer: USS New Jersey (Military Expeditionary Force 1-64) was the captain of the only battleship to provide gunfire support., Correct: False, Cost: $0.00001708\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.5750\n",
      "Answer: Willard S. Brown, Correct: False, Cost: $0.00002560\n",
      "\n",
      "--- Q20/100 (train) ---\n",
      "Question: What is Opry Mills in Nashville, Tennessee?...\n",
      "Step 1\n",
      "Selected: microsoft/phi-4, UCB: 0.9644\n",
      "Answer: A shopping, dining, and entertainment complex., Correct: False, Cost: $0.00000609\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.7060\n",
      "Answer: A shopping mall, Correct: False, Cost: $0.00004270\n",
      "Step 3\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.5677\n",
      "Answer: A shopping, dining, and entertainment complex., Correct: False, Cost: $0.00002336\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.8155\n",
      "Answer: A shopping mall, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.5305\n",
      "Answer: A shopping center, Correct: False, Cost: $0.00000970\n",
      "\n",
      "--- Q21/100 (test) ---\n",
      "Question: What star of \"Attack of the Gryphon\" was also a voice artist in Johnny Bravo?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0858\n",
      "Answer: Frank Welker, Correct: False, Cost: $0.00000430\n",
      "Step 2\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.6297\n",
      "Answer: Frank Welker, Correct: False, Cost: $0.00002172\n",
      "Step 3\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.6877\n",
      "Answer: Jeff Bennett, Correct: False, Cost: $0.00000775\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5617\n",
      "Answer: Jeff Bennett, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.4945\n",
      "Answer: Jeff Bennett, Correct: False, Cost: $0.00007702\n",
      "\n",
      "--- Q22/100 (test) ---\n",
      "Question: Who was also an actor, Serri or John Fogerty?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 0.9899\n",
      "Answer: John Fogerty, Correct: False, Cost: $0.00000395\n",
      "Step 2\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.5917\n",
      "Answer: no, Correct: False, Cost: $0.00001250\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.5419\n",
      "Answer: no, Correct: False, Cost: $0.00001510\n",
      "Step 4\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.5343\n",
      "Answer: John Fogerty, Correct: False, Cost: $0.00006360\n",
      "Step 5\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.5652\n",
      "Answer: John Fogerty, Correct: False, Cost: $0.00007158\n",
      "\n",
      "--- Q23/100 (test) ---\n",
      "Question: Which came to market first, \"Hey Pa! There's a Goat on the Roof\" or \"Poleconomy\"...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1074\n",
      "Answer: Hey Pa! There's a Goat on the Roof, Correct: True, Cost: $0.00000630\n",
      "Success in step 1!\n",
      "\n",
      "--- Q24/100 (test) ---\n",
      "Question: Which lead actor/actress in War Chhod Na Yaar has also acted in Bengali and English-language films?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0318\n",
      "Answer: Ritwick Chakraborty, Correct: False, Cost: $0.00000520\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.6285\n",
      "Answer: Sharman Joshi, Correct: False, Cost: $0.00005081\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5427\n",
      "Answer: Soha Ali Khan, Correct: True, Cost: $0.00000000\n",
      "Success in step 3!\n",
      "\n",
      "--- Q25/100 (test) ---\n",
      "Question: The arena where the Lewiston Maineiacs played their home games can seat how many people?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1100\n",
      "Answer: 5,738, Correct: False, Cost: $0.00000455\n",
      "Step 2\n",
      "Selected: microsoft/phi-4, UCB: 1.6124\n",
      "Answer: 8,800, Correct: False, Cost: $0.00000903\n",
      "Step 3\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.5427\n",
      "Answer: 6,500, Correct: False, Cost: $0.00002529\n",
      "Step 4\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.4922\n",
      "Answer: 3,977, Correct: False, Cost: $0.00001990\n",
      "Step 5\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.6504\n",
      "Answer: 4,600, Correct: False, Cost: $0.00001115\n",
      "\n",
      "--- Q26/100 (test) ---\n",
      "Question: When did the game which held three times in  in East Asia first held...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1351\n",
      "Answer: 1986, Correct: False, Cost: $0.00000420\n",
      "Step 2\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.5179\n",
      "Answer: 1990, Correct: False, Cost: $0.00003244\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5144\n",
      "Answer: 1990, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.4066\n",
      "Answer: 1987, Correct: False, Cost: $0.00001204\n",
      "Step 5\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4061\n",
      "Answer: 1988, Correct: False, Cost: $0.00001045\n",
      "\n",
      "--- Q27/100 (test) ---\n",
      "Question: Yoann Lemoine, a French video director, has created music videos for Lana Del Rey, Katy Perry, and a...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0813\n",
      "Answer: Taylor Swift, Correct: True, Cost: $0.00000500\n",
      "Success in step 1!\n",
      "\n",
      "--- Q28/100 (test) ---\n",
      "Question: Martin Patterson \"Pat\" Hingle was a close friend of an actor who achieved success in what Western TV...\n",
      "Step 1\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 0.9527\n",
      "Answer: Bonanza, Correct: False, Cost: $0.00001459\n",
      "Step 2\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.5999\n",
      "Answer: The Waltons, Correct: False, Cost: $0.00002156\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 1.4965\n",
      "Answer: Alamo, Correct: False, Cost: $0.00001435\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.3640\n",
      "Answer: Rawhide, Correct: True, Cost: $0.00000000\n",
      "Success in step 4!\n",
      "\n",
      "--- Q29/100 (test) ---\n",
      "Question: Jamie Harris is best known for his role as The Hook-Handed Man in a movie directed by who ?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0111\n",
      "Answer: M. Night Shyamalan, Correct: False, Cost: $0.00000485\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.6491\n",
      "Answer: Barry Sonnenfeld, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.4394\n",
      "Answer: Brad Silberling, Correct: True, Cost: $0.00006107\n",
      "Success in step 3!\n",
      "\n",
      "--- Q30/100 (test) ---\n",
      "Question: The Polar Bears featuring the voice of Armie Hammer, is also the same voice actor for which characte...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9520\n",
      "Answer: Jackson Storm, Correct: True, Cost: $0.00003142\n",
      "Success in step 1!\n",
      "\n",
      "--- Q31/100 (test) ---\n",
      "Question: At what venue was the January 2, 2012 game featured the University of Louisville team?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9683\n",
      "Answer: Sun Life Stadium, Correct: False, Cost: $0.00003130\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.7103\n",
      "Answer: Sun Life Stadium, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.4810\n",
      "Answer: Papa John's Cardinal Stadium, Correct: False, Cost: $0.00003598\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.5315\n",
      "Answer: KFC Yum! Center, Correct: False, Cost: $0.00000930\n",
      "Step 5\n",
      "Selected: microsoft/phi-4, UCB: 1.4381\n",
      "Answer: The basketball component of Papa John's Cardinal Stadium at KFC Yum! Center. Note: more accurate naming could be \"The basketball game was at KFC Yum! Center., Correct: False, Cost: $0.00001904\n",
      "\n",
      "--- Q32/100 (test) ---\n",
      "Question: What major city is the Faith Lutheran Middle School and High School located by?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0728\n",
      "Answer: Las Vegas, Correct: True, Cost: $0.00000390\n",
      "Success in step 1!\n",
      "\n",
      "--- Q33/100 (test) ---\n",
      "Question: In what century did this Native warrior and chief, whose brother Tenskwatawa led the Tippecanoe orde...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0287\n",
      "Answer: 19th century, Correct: False, Cost: $0.00003928\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.7129\n",
      "Answer: 19th century, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.4649\n",
      "Answer: 19th, Correct: False, Cost: $0.00002751\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4535\n",
      "Answer: 18th, Correct: False, Cost: $0.00000990\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.4326\n",
      "Answer: 19th century, Correct: False, Cost: $0.00002410\n",
      "\n",
      "--- Q34/100 (test) ---\n",
      "Question: Where is the company that Sachin Warrier worked for as a software engineer headquartered?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9475\n",
      "Answer: Bengaluru, Correct: False, Cost: $0.00003016\n",
      "Step 2\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.5503\n",
      "Answer: Mountain View, Correct: False, Cost: $0.00000595\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4185\n",
      "Answer: Seattle, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.2990\n",
      "Answer: Mountain View, Correct: False, Cost: $0.00001750\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.4478\n",
      "Answer: Kochi, Correct: False, Cost: $0.00003210\n",
      "\n",
      "--- Q35/100 (test) ---\n",
      "Question: Meaning \"reddish water\" in Hawaiian, which location was Butch Van Artsdalen known for surfing 25-foo...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1419\n",
      "Answer: Waimea Bay, Correct: True, Cost: $0.00000510\n",
      "Success in step 1!\n",
      "\n",
      "--- Q36/100 (test) ---\n",
      "Question: This singer of A Rather Blustery Day also voiced what hedgehog?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1470\n",
      "Answer: Sonic, Correct: True, Cost: $0.00000380\n",
      "Success in step 1!\n",
      "\n",
      "--- Q37/100 (test) ---\n",
      "Question: The movies The Boatniks and The Great Locomotive Chase were both made by which production company?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1712\n",
      "Answer: Walt Disney Productions, Correct: True, Cost: $0.00003168\n",
      "Success in step 1!\n",
      "\n",
      "--- Q38/100 (test) ---\n",
      "Question: who is the younger brother of The episode guest stars of The Hard Easy...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0657\n",
      "Answer: Jason London, Correct: False, Cost: $0.00002813\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.6766\n",
      "Answer: no, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.3705\n",
      "Answer: Jeremy London, Correct: False, Cost: $0.00002224\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.4574\n",
      "Answer: No. Husband of Mitch Cohen, Bruce McGill., Correct: False, Cost: $0.00001211\n",
      "Step 5\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4477\n",
      "Answer: no, Correct: False, Cost: $0.00000000\n",
      "\n",
      "--- Q39/100 (test) ---\n",
      "Question: How long is the bridge in the Öresund Region that connect Copenhagen, Denmark and Malmo, Sweden?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.2156\n",
      "Answer: 7.8 kilometers, Correct: False, Cost: $0.00000455\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.4718\n",
      "Answer: 7,845 meters, Correct: False, Cost: $0.00005043\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.4679\n",
      "Answer: 7.8 kilometers, Correct: False, Cost: $0.00001730\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.4787\n",
      "Answer: 7,845 meters (604 feet), Correct: False, Cost: $0.00001344\n",
      "Step 5\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5863\n",
      "Answer: 16 kilometers, Correct: False, Cost: $0.00000000\n",
      "\n",
      "--- Q40/100 (test) ---\n",
      "Question: Marcus Wayne Garland spent nine seasons with an American professional baseball team that is based in...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9812\n",
      "Answer: 1901, Correct: True, Cost: $0.00003826\n",
      "Success in step 1!\n",
      "\n",
      "--- Q41/100 (test) ---\n",
      "Question: Erica Packer was the second wife of what Australian businessman?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0864\n",
      "Answer: James Packer, Correct: True, Cost: $0.00002826\n",
      "Success in step 1!\n",
      "\n",
      "--- Q42/100 (test) ---\n",
      "Question: From March 631 to April 631, Farrukhzad Khosrau V was the king of an empire that succeeded which emp...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1117\n",
      "Answer: Sasanian Empire, Correct: False, Cost: $0.00003599\n",
      "Step 2\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.7058\n",
      "Answer: Sasanian Empire, Correct: False, Cost: $0.00000690\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 1.4976\n",
      "Answer: Khosrow II succeeded the Sasanian Empire's Bahram VI., Correct: False, Cost: $0.00001288\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.6008\n",
      "Answer: Sasanian Empire, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.4702\n",
      "Answer: Sasanian Empire, Correct: False, Cost: $0.00002380\n",
      "\n",
      "--- Q43/100 (test) ---\n",
      "Question: Are Broughtonia and Laeliocattleya both orchids?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.2984\n",
      "Answer: yes, Correct: True, Cost: $0.00000375\n",
      "Success in step 1!\n",
      "\n",
      "--- Q44/100 (test) ---\n",
      "Question: When was the Western Germanic language spoken from which the small settlement situated on the river ...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0481\n",
      "Answer: Early Middle Ages, Correct: False, Cost: $0.00003320\n",
      "Step 2\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.5049\n",
      "Answer: 5th-8th century, Correct: False, Cost: $0.00003308\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3650\n",
      "Answer: Early Middle Ages, Correct: False, Cost: $0.00001610\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4907\n",
      "Answer: 8th century, Correct: False, Cost: $0.00000925\n",
      "Step 5\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3889\n",
      "Answer: 8th century, Correct: False, Cost: $0.00001085\n",
      "\n",
      "--- Q45/100 (test) ---\n",
      "Question: Which football team does Lewis Young's older brother, Ashley Young, play for?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9760\n",
      "Answer: Everton., Correct: False, Cost: $0.00002889\n",
      "Step 2\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.4810\n",
      "Answer: Everton, Correct: False, Cost: $0.00001210\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5015\n",
      "Answer: Aston Villa, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.3839\n",
      "Answer: Manchester United., Correct: False, Cost: $0.00001113\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.3766\n",
      "Answer: Aston Villa, Correct: False, Cost: $0.00002988\n",
      "\n",
      "--- Q46/100 (test) ---\n",
      "Question: What year was the film that starred the composer for \"The Naked Brothers Band\"?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1979\n",
      "Answer: 2005, Correct: False, Cost: $0.00002927\n",
      "Step 2\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4812\n",
      "Answer: 2007, Correct: False, Cost: $0.00000620\n",
      "Step 3\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.3463\n",
      "Answer: 2007, Correct: False, Cost: $0.00002462\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4390\n",
      "Answer: 2007, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: microsoft/phi-4, UCB: 1.3280\n",
      "Answer: 2005, Correct: False, Cost: $0.00001351\n",
      "\n",
      "--- Q47/100 (test) ---\n",
      "Question: Who had the best singles ranking, Roberta Vinci or Jorge Lozano?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.2881\n",
      "Answer: Roberta Vinci, Correct: True, Cost: $0.00000400\n",
      "Success in step 1!\n",
      "\n",
      "--- Q48/100 (test) ---\n",
      "Question: The voice of Homer Simpson also voices what alien character in the television series \"The Simpsons\"?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1994\n",
      "Answer: Kang, Correct: False, Cost: $0.00003041\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.6327\n",
      "Answer: Kang, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.4706\n",
      "Answer: Kang, Correct: False, Cost: $0.00001420\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.5540\n",
      "Answer: Kodos, Correct: True, Cost: $0.00000840\n",
      "Success in step 4!\n",
      "\n",
      "--- Q49/100 (test) ---\n",
      "Question: Are Bamboo Mañalac and Danny Jones both musicians?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.2631\n",
      "Answer: yes, Correct: True, Cost: $0.00000365\n",
      "Success in step 1!\n",
      "\n",
      "--- Q50/100 (test) ---\n",
      "Question: What is the name of the detective novelist parent of the author of '\"Q\" Is for Quarry'?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1274\n",
      "Answer: Erle Stanley Gardner, Correct: False, Cost: $0.00000455\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.6038\n",
      "Answer: Sue Grafton, Correct: False, Cost: $0.00005094\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4249\n",
      "Answer: Robert B. Parker, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.3475\n",
      "Answer: Sue Grafton, Correct: False, Cost: $0.00006943\n",
      "Step 5\n",
      "Selected: microsoft/phi-4, UCB: 1.4004\n",
      "Answer: Erle Stanley Gardner, Correct: False, Cost: $0.00001393\n",
      "\n",
      "--- Q51/100 (test) ---\n",
      "Question: How many copies of Roald Dahl's variation on a popular anecdote sold?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3007\n",
      "Answer: 100,000, Correct: False, Cost: $0.00000470\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.5736\n",
      "Answer: The exact number of copies sold is not publicly available., Correct: False, Cost: $0.00005349\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.4255\n",
      "Answer: unavailable, Correct: False, Cost: $0.00001620\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4579\n",
      "Answer: unknown, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.2738\n",
      "Answer: unknown, Correct: False, Cost: $0.00007436\n",
      "\n",
      "--- Q52/100 (test) ---\n",
      "Question: Bordan Tkachuk was the CEO of a company that provides what sort of products?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1174\n",
      "Answer: software, Correct: False, Cost: $0.00000390\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.5513\n",
      "Answer: technology, Correct: False, Cost: $0.00004472\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4353\n",
      "Answer: hardware, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.2928\n",
      "Answer: telecommunications, Correct: False, Cost: $0.00002768\n",
      "Step 5\n",
      "Selected: microsoft/phi-4, UCB: 1.3999\n",
      "Answer: telecommunications equipment, Correct: False, Cost: $0.00001323\n",
      "\n",
      "--- Q53/100 (test) ---\n",
      "Question: What town host an annual media finance conference that has featured an Australian-born American medi...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0773\n",
      "Answer: Sun Valley, Correct: True, Cost: $0.00002914\n",
      "Success in step 1!\n",
      "\n",
      "--- Q54/100 (test) ---\n",
      "Question: Which film starring Patrick Tatten is based on a book written by Steve Lopez?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.2259\n",
      "Answer: The Soloist, Correct: True, Cost: $0.00002889\n",
      "Success in step 1!\n",
      "\n",
      "--- Q55/100 (test) ---\n",
      "Question: Which year and which conference was the 14th season for this conference as part of the NCAA Division...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0743\n",
      "Answer: 1999, Big 12 Conference, Correct: False, Cost: $0.00000635\n",
      "Step 2\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.4434\n",
      "Answer: 2014, Pac-12 Conference, Correct: False, Cost: $0.00001960\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 1.4336\n",
      "Answer: 2014, Pac-12 Conference, Correct: False, Cost: $0.00001344\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4168\n",
      "Answer: 2014, Pac-12 Conference, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.2770\n",
      "Answer: 2002, Big 12 Conference, Correct: False, Cost: $0.00004072\n",
      "\n",
      "--- Q56/100 (test) ---\n",
      "Question: Considered the strongest recorded tropical cyclone, which cyclone had a film made about it in 2007?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.2932\n",
      "Answer: Typhoon: When the Storm Hit Tokyo, Correct: False, Cost: $0.00000535\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.6722\n",
      "Answer: Hurricane Katrina, Correct: False, Cost: $0.00005030\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4542\n",
      "Answer: Typhoon Yolanda, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.3793\n",
      "Answer: Typhoon Tip., Correct: False, Cost: $0.00001288\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.4201\n",
      "Answer: Typhoon Haiyan, Correct: False, Cost: $0.00003361\n",
      "\n",
      "--- Q57/100 (test) ---\n",
      "Question: What officially ended the first phase of the military conflict between the British Raj and the Emira...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9481\n",
      "Answer: The Treaty of Gandamak., Correct: True, Cost: $0.00003866\n",
      "Success in step 1!\n",
      "\n",
      "--- Q58/100 (test) ---\n",
      "Question: Which board game was published most recently, Pirate's Cove or Catan?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1922\n",
      "Answer: Catan, Correct: False, Cost: $0.00002889\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5309\n",
      "Answer: Catan, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4184\n",
      "Answer: Pirate's Cove, Correct: True, Cost: $0.00000735\n",
      "Success in step 3!\n",
      "\n",
      "--- Q59/100 (test) ---\n",
      "Question: What major truck road is located in Backford Cross?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0980\n",
      "Answer: A41, Correct: True, Cost: $0.00002610\n",
      "Success in step 1!\n",
      "\n",
      "--- Q60/100 (test) ---\n",
      "Question: What city did the characters from Gargoyles the Movie: The Heroes Awaken wake up in after being in a...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1645\n",
      "Answer: Manhattan., Correct: False, Cost: $0.00003611\n",
      "Step 2\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4986\n",
      "Answer: New York City, Correct: False, Cost: $0.00000680\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.3223\n",
      "Answer: New York City, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.2877\n",
      "Answer: Manhattan, Correct: False, Cost: $0.00001288\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.4138\n",
      "Answer: Manhattan, Correct: False, Cost: $0.00002130\n",
      "\n",
      "--- Q61/100 (test) ---\n",
      "Question: What podcast was the cheif executive officer of Nerdist Industries a guest on?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0405\n",
      "Answer: The Nerdist Podcast., Correct: False, Cost: $0.00003194\n",
      "Step 2\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.5127\n",
      "Answer: The Joe Rogan Experience, Correct: False, Cost: $0.00000640\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 1.3631\n",
      "Answer: Uproar Radio's Chief Executive Officer was a guest on Uproar Radio., Correct: False, Cost: $0.00001232\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4032\n",
      "Answer: The Nerdist Podcast., Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3575\n",
      "Answer: The Nerdist Podcast, Correct: False, Cost: $0.00002280\n",
      "\n",
      "--- Q62/100 (test) ---\n",
      "Question: What type of media does Bitter Jester and Sicko have in common?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1537\n",
      "Answer: Music, Correct: False, Cost: $0.00000385\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4078\n",
      "Answer: Film, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.3229\n",
      "Answer: Music, Correct: False, Cost: $0.00005346\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.3567\n",
      "Answer: Comics, Correct: False, Cost: $0.00001127\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3512\n",
      "Answer: Film, Correct: False, Cost: $0.00001860\n",
      "\n",
      "--- Q63/100 (test) ---\n",
      "Question: The Livesey Hal War Memorial commemorates the fallen of which war, that had over 60 million casualti...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0805\n",
      "Answer: World War I, Correct: False, Cost: $0.00003206\n",
      "Step 2\n",
      "Selected: microsoft/phi-4, UCB: 1.3720\n",
      "Answer: World War I, Correct: False, Cost: $0.00000847\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4218\n",
      "Answer: World War II, Correct: True, Cost: $0.00000000\n",
      "Success in step 3!\n",
      "\n",
      "--- Q64/100 (test) ---\n",
      "Question: Who held the record for the longest service in the Australian Parliament for a woman, and was surpas...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.2172\n",
      "Answer: Bronwyn Bishop, Correct: False, Cost: $0.00003814\n",
      "Step 2\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.5539\n",
      "Answer: Susan Ryan, Correct: False, Cost: $0.00000705\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4951\n",
      "Answer: Kay Hull, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.2992\n",
      "Answer: Susan McDonald, Correct: False, Cost: $0.00000990\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3561\n",
      "Answer: Bronwyn Bishop, Correct: False, Cost: $0.00002360\n",
      "\n",
      "--- Q65/100 (test) ---\n",
      "Question: The director of the romantic comedy \"Big Stone Gap\" is based in what New York city?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1932\n",
      "Answer: New York City, Correct: False, Cost: $0.00003092\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.5425\n",
      "Answer: Adriana Trigiani, Correct: False, Cost: $0.00004600\n",
      "Step 3\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.3760\n",
      "Answer: New York City, Correct: False, Cost: $0.00002325\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3352\n",
      "Answer: Ithaca, Correct: False, Cost: $0.00000845\n",
      "Step 5\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3266\n",
      "Answer: Adirondack, Correct: False, Cost: $0.00001015\n",
      "\n",
      "--- Q66/100 (test) ---\n",
      "Question: Do the drinks Gibson and Zurracapote both contain gin?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1938\n",
      "Answer: yes, Correct: False, Cost: $0.00002686\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.6347\n",
      "Answer: no, Correct: True, Cost: $0.00000000\n",
      "Success in step 2!\n",
      "\n",
      "--- Q67/100 (test) ---\n",
      "Question: Are Skeptical Inquirer and The Progressive Populist both tabloid-style magazines?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4298\n",
      "Answer: no, Correct: True, Cost: $0.00000395\n",
      "Success in step 1!\n",
      "\n",
      "--- Q68/100 (test) ---\n",
      "Question: Do The Importance of Being Icelandic and The Five Obstructions belong to different film genres ?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1516\n",
      "Answer: yes, Correct: False, Cost: $0.00000395\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.6703\n",
      "Answer: yes, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.3999\n",
      "Answer: yes, Correct: False, Cost: $0.00005333\n",
      "Step 4\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3802\n",
      "Answer: yes, Correct: False, Cost: $0.00001710\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.4677\n",
      "Answer: yes, Correct: False, Cost: $0.00003143\n",
      "\n",
      "--- Q69/100 (test) ---\n",
      "Question: What year was the film \"Mom\" released, starring the voice of Shashaa Tirupati?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9681\n",
      "Answer: 2017, Correct: True, Cost: $0.00003079\n",
      "Success in step 1!\n",
      "\n",
      "--- Q70/100 (test) ---\n",
      "Question: What was position of the man who served Prince Andrew from 1990-2001?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0705\n",
      "Answer: Valet, Correct: False, Cost: $0.00002965\n",
      "Step 2\n",
      "Selected: microsoft/phi-4, UCB: 1.3838\n",
      "Answer: Yes, Correct: False, Cost: $0.00000784\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5780\n",
      "Answer: Valet, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.2267\n",
      "Answer: Equerry, Correct: False, Cost: $0.00000825\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.2884\n",
      "Answer: Valet, Correct: False, Cost: $0.00003091\n",
      "\n",
      "--- Q71/100 (test) ---\n",
      "Question: In what month is the annual documentary film festival, that is presented by the fortnightly publishe...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0670\n",
      "Answer: March, Correct: False, Cost: $0.00003091\n",
      "Step 2\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.3264\n",
      "Answer: November, Correct: False, Cost: $0.00002055\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 1.3342\n",
      "Answer: September, Correct: False, Cost: $0.00001001\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3288\n",
      "Answer: April, Correct: False, Cost: $0.00000820\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.2528\n",
      "Answer: March, Correct: False, Cost: $0.00001940\n",
      "\n",
      "--- Q72/100 (test) ---\n",
      "Question: Kam Heskin plays Paige Morgan in a 2004 film directed by who?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1511\n",
      "Answer: Ron Lagomarsino, Correct: False, Cost: $0.00003156\n",
      "Step 2\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.5864\n",
      "Answer: Uli Edel, Correct: False, Cost: $0.00000610\n",
      "Step 3\n",
      "Selected: microsoft/phi-4, UCB: 1.3093\n",
      "Answer: Kam Heskin plays Paige Morgan in a 2004 film directed by Ron Oliver., Correct: False, Cost: $0.00001267\n",
      "Step 4\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.3616\n",
      "Answer: Ron Oliver, Correct: False, Cost: $0.00000000\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.2942\n",
      "Answer: Ron Oliver, Correct: False, Cost: $0.00002180\n",
      "\n",
      "--- Q73/100 (test) ---\n",
      "Question: What was the nickname of the hitman hired by an Italian American Criminal Organization?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0617\n",
      "Answer: The Iceman, Correct: False, Cost: $0.00002978\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4451\n",
      "Answer: The Iceman, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4935\n",
      "Answer: The Cleaner, Correct: False, Cost: $0.00000725\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.3001\n",
      "Answer: The Enforcer, Correct: False, Cost: $0.00001190\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.3035\n",
      "Answer: The Iceman, Correct: False, Cost: $0.00003141\n",
      "\n",
      "--- Q74/100 (test) ---\n",
      "Question: Who is the current drummer of the band who did the song \"What Lovers Do\"?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1867\n",
      "Answer: Josh Dun, Correct: False, Cost: $0.00000405\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.3872\n",
      "Answer: Matt Flynn, Correct: True, Cost: $0.00000000\n",
      "Success in step 2!\n",
      "\n",
      "--- Q75/100 (test) ---\n",
      "Question: which writer has the highest number of skill  Langston Hughes or  Ian McEwan...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 0.9947\n",
      "Answer: Ian McEwan, Correct: False, Cost: $0.00000435\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4150\n",
      "Answer: Langston Hughes, Correct: True, Cost: $0.00000000\n",
      "Success in step 2!\n",
      "\n",
      "--- Q76/100 (test) ---\n",
      "Question: How large is the shopping mall where KGOT radio station has its studios ?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0648\n",
      "Answer: Unknown, Correct: False, Cost: $0.00002762\n",
      "Step 2\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3671\n",
      "Answer: 100,000 square feet, Correct: False, Cost: $0.00000665\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.2501\n",
      "Answer: Unknown, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.1859\n",
      "Answer: 50,000 square feet, Correct: False, Cost: $0.00001197\n",
      "Step 5\n",
      "Selected: microsoft/phi-4, UCB: 1.2935\n",
      "Answer: Unknown, Correct: False, Cost: $0.00001295\n",
      "\n",
      "--- Q77/100 (test) ---\n",
      "Question: Which travel parody series featured American actor, comedian, and radio host best known as the host ...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1309\n",
      "Answer: An Idiot Abroad, Correct: False, Cost: $0.00003714\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4285\n",
      "Answer: An Idiot Abroad, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4790\n",
      "Answer: Traveling Wilburys, Correct: False, Cost: $0.00000815\n",
      "Step 4\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3304\n",
      "Answer: The Joe Schmo Show, Correct: False, Cost: $0.00002020\n",
      "Step 5\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3861\n",
      "Answer: Traveling Wilburys, Correct: False, Cost: $0.00001130\n",
      "\n",
      "--- Q78/100 (test) ---\n",
      "Question: Which case was decided first, Selle v. Gibb or Reynolds v. Sims?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0446\n",
      "Answer: Reynolds v. Sims, Correct: False, Cost: $0.00003321\n",
      "Step 2\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.4671\n",
      "Answer: Selle v. Gibb, Correct: False, Cost: $0.00002033\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4473\n",
      "Answer: Reynolds v. Sims, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4178\n",
      "Answer: Selle v. Gibb, Correct: False, Cost: $0.00000915\n",
      "Step 5\n",
      "Selected: microsoft/phi-4, UCB: 1.3954\n",
      "Answer: Reynolds v. Sims was decided first., Correct: False, Cost: $0.00001533\n",
      "\n",
      "--- Q79/100 (test) ---\n",
      "Question: Is the Pakistan fast bowler who joined the Kent County Cricket Club in June, 2011 a left-hand or rig...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 0.9962\n",
      "Answer: right-hand, Correct: True, Cost: $0.00000460\n",
      "Success in step 1!\n",
      "\n",
      "--- Q80/100 (test) ---\n",
      "Question: When was the palace founded at which Columba de Dunbar died in 1435?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0762\n",
      "Answer: 1372, Correct: False, Cost: $0.00000445\n",
      "Step 2\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.3236\n",
      "Answer: 1390, Correct: False, Cost: $0.00002156\n",
      "Step 3\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.3146\n",
      "Answer: 1435, Correct: False, Cost: $0.00005574\n",
      "Step 4\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.2227\n",
      "Answer: 1370, Correct: False, Cost: $0.00001940\n",
      "Step 5\n",
      "Selected: microsoft/phi-4, UCB: 1.2784\n",
      "Answer: 1422, Correct: False, Cost: $0.00001372\n",
      "\n",
      "--- Q81/100 (test) ---\n",
      "Question: What screenwriter with credits for \"Evolution\" co-wrote a film starring Nicolas Cage and Téa Leoni?...\n",
      "Step 1\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 0.9558\n",
      "Answer: David Diamond, Correct: False, Cost: $0.00000000\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5816\n",
      "Answer: David Weissman, Correct: True, Cost: $0.00000000\n",
      "Success in step 2!\n",
      "\n",
      "--- Q82/100 (test) ---\n",
      "Question: Who was known by his stage name Aladin and helped organizations improve their performance as a consu...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9429\n",
      "Answer: Marcel van der Schroef, Correct: False, Cost: $0.00003359\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5229\n",
      "Answer: Aladin Akyurek, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4073\n",
      "Answer: Aladin Akyurek, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3413\n",
      "Answer: Alain de Botton, Correct: False, Cost: $0.00000915\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.3037\n",
      "Answer: Aladin Akyurek, Correct: False, Cost: $0.00004601\n",
      "\n",
      "--- Q83/100 (test) ---\n",
      "Question: What book provided the foundation for Masters and Johnson's research team?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9272\n",
      "Answer: Human Sexual Response, Correct: False, Cost: $0.00002953\n",
      "Step 2\n",
      "Selected: microsoft/phi-4, UCB: 1.3488\n",
      "Answer: Human Sexual Response, Correct: False, Cost: $0.00000784\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3865\n",
      "Answer: Human Sexual Response, Correct: False, Cost: $0.00001420\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.4454\n",
      "Answer: Kinsey Reports, Correct: False, Cost: $0.00000815\n",
      "Step 5\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.2948\n",
      "Answer: Kinsey Reports, Correct: False, Cost: $0.00000000\n",
      "\n",
      "--- Q84/100 (test) ---\n",
      "Question: How many laps did Harry Prowell run during the 10,000 metres race at the 1967 Pan American Games?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0509\n",
      "Answer: unknown, Correct: False, Cost: $0.00003091\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.3052\n",
      "Answer: 25 laps, Correct: True, Cost: $0.00000000\n",
      "Success in step 2!\n",
      "\n",
      "--- Q85/100 (test) ---\n",
      "Question: \"Kapitän zur See\" Ernst Lindemann was the commander of which one of two battleships built for Nazi G...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 0.9729\n",
      "Answer: Bismarck, Correct: True, Cost: $0.00000470\n",
      "Success in step 1!\n",
      "\n",
      "--- Q86/100 (test) ---\n",
      "Question: Las Vegas Festival Grounds is north of the hotel that is owned by which company?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 0.9002\n",
      "Answer: MGM Resorts International, Correct: True, Cost: $0.00000445\n",
      "Success in step 1!\n",
      "\n",
      "--- Q87/100 (test) ---\n",
      "Question: Július Satinský was born in a city that has a current population of what?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0087\n",
      "Answer: approximately 430,000, Correct: False, Cost: $0.00000510\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4722\n",
      "Answer: approximately 430,000, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.3317\n",
      "Answer: 430,000, Correct: False, Cost: $0.00005891\n",
      "Step 4\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.2916\n",
      "Answer: 432,272, Correct: False, Cost: $0.00002954\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.2955\n",
      "Answer: 432,801, Correct: False, Cost: $0.00003430\n",
      "\n",
      "--- Q88/100 (test) ---\n",
      "Question: What occupation do Chris Menges and Aram Avakian share?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0463\n",
      "Answer: Cinematographer, Correct: False, Cost: $0.00000400\n",
      "Step 2\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.4175\n",
      "Answer: Film editor, Correct: False, Cost: $0.00004396\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.3778\n",
      "Answer: Cinematographer, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3191\n",
      "Answer: Film editor, Correct: False, Cost: $0.00001760\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.4113\n",
      "Answer: Film editor, Correct: False, Cost: $0.00003227\n",
      "\n",
      "--- Q89/100 (test) ---\n",
      "Question: Which musical fantasy film is older, Bedknobs and Broomsticks or The Muppet Christmas Carol?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.1960\n",
      "Answer: Bedknobs and Broomsticks, Correct: True, Cost: $0.00003562\n",
      "Success in step 1!\n",
      "\n",
      "--- Q90/100 (test) ---\n",
      "Question: The 2011–12 VCU Rams men's basketball team, led by third year head coach Shaka Smart, represented Vi...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9545\n",
      "Answer: 1838, Correct: True, Cost: $0.00003522\n",
      "Success in step 1!\n",
      "\n",
      "--- Q91/100 (test) ---\n",
      "Question: Alvaro Mexia had a diplomatic mission with which tribe of indigenous people?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9900\n",
      "Answer: Timucua, Correct: False, Cost: $0.00002851\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4161\n",
      "Answer: Ais, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.3757\n",
      "Answer: Ais, Correct: False, Cost: $0.00000000\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.2707\n",
      "Answer: Apalachee, Correct: False, Cost: $0.00000840\n",
      "Step 5\n",
      "Selected: microsoft/phi-4, UCB: 1.2840\n",
      "Answer: Timucua and Apalachee, Correct: False, Cost: $0.00001414\n",
      "\n",
      "--- Q92/100 (test) ---\n",
      "Question: What year did the series on CBS, starring the actor who known for his role in \"Rebel Without a Cause...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0206\n",
      "Answer: 1956, Correct: False, Cost: $0.00003269\n",
      "Step 2\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3395\n",
      "Answer: 1961, Correct: False, Cost: $0.00001410\n",
      "Step 3\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3342\n",
      "Answer: 1962, Correct: False, Cost: $0.00000815\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.2095\n",
      "Answer: 1967, Correct: False, Cost: $0.00001246\n",
      "Step 5\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.2503\n",
      "Answer: 1957, Correct: False, Cost: $0.00003363\n",
      "\n",
      "--- Q93/100 (test) ---\n",
      "Question: What NIFL Premier Intermediate League team did Sean Connor play for?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.0449\n",
      "Answer: Queens University, Correct: False, Cost: $0.00002902\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.5704\n",
      "Answer: no, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.1994\n",
      "Answer: Ballyclare Comrades, Correct: False, Cost: $0.00001550\n",
      "Step 4\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.3381\n",
      "Answer: Lisburn Rafts, Correct: False, Cost: $0.00000885\n",
      "Step 5\n",
      "Selected: microsoft/phi-4, UCB: 1.1807\n",
      "Answer: Ballymena United Football Club, Correct: False, Cost: $0.00001400\n",
      "\n",
      "--- Q94/100 (test) ---\n",
      "Question: Who was Chamberlain and Colonel of the military staff of The Merrie Monarch?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 0.9032\n",
      "Answer: John Brown, Correct: False, Cost: $0.00000390\n",
      "Step 2\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.3166\n",
      "Answer: Boscawen, Correct: False, Cost: $0.00002103\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3236\n",
      "Answer: Boscawen, Correct: False, Cost: $0.00001630\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.2987\n",
      "Answer: Arthur Wellesley, 1st Duke of Wellington., Correct: False, Cost: $0.00001372\n",
      "Step 5\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.2775\n",
      "Answer: Edward Montagu, Correct: False, Cost: $0.00000000\n",
      "\n",
      "--- Q95/100 (test) ---\n",
      "Question: Which English musician is known for both the songs \"See Yourself\" and \"Withing You Without You\"?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0931\n",
      "Answer: Sting, Correct: False, Cost: $0.00000420\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4765\n",
      "Answer: George Harrison, Correct: True, Cost: $0.00000000\n",
      "Success in step 2!\n",
      "\n",
      "--- Q96/100 (test) ---\n",
      "Question: Tysons Galleria is located in what county?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.0951\n",
      "Answer: Fairfax County, Correct: True, Cost: $0.00000385\n",
      "Success in step 1!\n",
      "\n",
      "--- Q97/100 (test) ---\n",
      "Question: Where did the descendants of the group of black Indians associated with the Seminole people settle?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 0.9552\n",
      "Answer: Oklahoma, Correct: False, Cost: $0.00000405\n",
      "Step 2\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.3227\n",
      "Answer: Oklahoma is not the only correct answer, but one of the places is indeed Oklahoma, however a more specific answer would be: Florida and Oklahoma. Since that is not allowed, a shorter version is:, Correct: False, Cost: $0.00002777\n",
      "Step 3\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.3256\n",
      "Answer: Oklahoma., Correct: False, Cost: $0.00001930\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.2392\n",
      "Answer: Florida and Oklahoma., Correct: False, Cost: $0.00001498\n",
      "Step 5\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.2734\n",
      "Answer: Oklahoma, Correct: False, Cost: $0.00000000\n",
      "\n",
      "--- Q98/100 (test) ---\n",
      "Question: What is the 2010 population of the city 2.1 miles southwest of Marietta Air Force Station?...\n",
      "Step 1\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.1796\n",
      "Answer: 1,625, Correct: False, Cost: $0.00000485\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.2920\n",
      "Answer: The 2010 population of the city 2.1 miles southwest of Marietta Air Force Station is **Marietta, Georgia**, with a population of **56,579**., Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: deepseek/deepseek-chat, UCB: 1.3945\n",
      "Answer: 56,579, Correct: False, Cost: $0.00007272\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.1738\n",
      "Answer: 56,579, Correct: False, Cost: $0.00001477\n",
      "Step 5\n",
      "Selected: google/gemini-2.0-flash-001, UCB: 1.2225\n",
      "Answer: 56,579, Correct: False, Cost: $0.00002710\n",
      "\n",
      "--- Q99/100 (test) ---\n",
      "Question: Who wrote The Devils, and was an eninent English theater, opera and film director?...\n",
      "Step 1\n",
      "Selected: deepseek/deepseek-chat, UCB: 0.9712\n",
      "Answer: Jonathan Miller, Correct: False, Cost: $0.00002965\n",
      "Step 2\n",
      "Selected: arcee-ai/trinity-large-preview:free, UCB: 1.4508\n",
      "Answer: Ken Russell, Correct: False, Cost: $0.00000000\n",
      "Step 3\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 1.2626\n",
      "Answer: John Whiting, Correct: False, Cost: $0.00002359\n",
      "Step 4\n",
      "Selected: microsoft/phi-4, UCB: 1.2685\n",
      "Answer: John Whiting, Correct: False, Cost: $0.00001155\n",
      "Step 5\n",
      "Selected: mistralai/mistral-small-3.1-24b-instruct, UCB: 1.2438\n",
      "Answer: Ken Russell, Correct: False, Cost: $0.00000930\n",
      "\n",
      "--- Q100/100 (test) ---\n",
      "Question: Where was the world cup hosted that Algeria qualified for the first time into the round of 16?...\n",
      "Step 1\n",
      "Selected: meta-llama/llama-4-maverick, UCB: 0.8938\n",
      "Answer: Brazil, Correct: True, Cost: $0.00001392\n",
      "Success in step 1!\n",
      "Train Records: 20\n",
      "Test Records: 80\n",
      "Total Records: 100\n",
      "=== LinUCB CASCADE WITH HOTPOTQA DATASET ===\n",
      "\n",
      "Train Ratio: 20.0%\n",
      "LinUCB updated on BOTH Train and Test sets.\n",
      "Batch Size: 20\n",
      "\n",
      "=== TRAIN SET RESULTS ===\n",
      "Total Train Questions: 20\n",
      "Success Rate: 0.4000\n",
      "Average Steps: 3.5000\n",
      "Average Cost per Question: $0.00006265\n",
      "Average Cost per Successful Question: $0.00001043\n",
      "Success Rate by Position:\n",
      "  Position 1: 0.3000\n",
      "  Position 2: 0.1000\n",
      "  Position 3: 0.0000\n",
      "  Position 4: 0.0000\n",
      "  Position 5: 0.0000\n",
      "\n",
      "Train Set Model Performance:\n",
      "\n",
      "arcee-ai/trinity-large-preview:free:\n",
      "  Overall: 1/12 = 0.0833\n",
      "  Average Cost: $0.00000000\n",
      "  By Position:\n",
      "    Pos 1: 0/1 = 0.0000, Avg Cost: $0.00000000\n",
      "    Pos 2: 1/5 = 0.2000, Avg Cost: $0.00000000\n",
      "    Pos 3: 0/1 = 0.0000, Avg Cost: $0.00000000\n",
      "    Pos 4: 0/5 = 0.0000, Avg Cost: $0.00000000\n",
      "\n",
      "mistralai/mistral-small-3.1-24b-instruct:\n",
      "  Overall: 6/22 = 0.2727\n",
      "  Average Cost: $0.00000545\n",
      "  By Position:\n",
      "    Pos 1: 6/16 = 0.3750, Avg Cost: $0.00000430\n",
      "    Pos 2: 0/2 = 0.0000, Avg Cost: $0.00000620\n",
      "    Pos 4: 0/2 = 0.0000, Avg Cost: $0.00000900\n",
      "    Pos 5: 0/2 = 0.0000, Avg Cost: $0.00001028\n",
      "\n",
      "microsoft/phi-4:\n",
      "  Overall: 0/9 = 0.0000\n",
      "  Average Cost: $0.00001181\n",
      "  By Position:\n",
      "    Pos 1: 0/1 = 0.0000, Avg Cost: $0.00000609\n",
      "    Pos 3: 0/5 = 0.0000, Avg Cost: $0.00001176\n",
      "    Pos 4: 0/3 = 0.0000, Avg Cost: $0.00001381\n",
      "\n",
      "meta-llama/llama-4-maverick:\n",
      "  Overall: 0/7 = 0.0000\n",
      "  Average Cost: $0.00003202\n",
      "  By Position:\n",
      "    Pos 3: 0/3 = 0.0000, Avg Cost: $0.00002844\n",
      "    Pos 4: 0/1 = 0.0000, Avg Cost: $0.00002766\n",
      "    Pos 5: 0/3 = 0.0000, Avg Cost: $0.00003705\n",
      "\n",
      "google/gemini-2.0-flash-001:\n",
      "  Overall: 0/8 = 0.0000\n",
      "  Average Cost: $0.00001754\n",
      "  By Position:\n",
      "    Pos 1: 0/1 = 0.0000, Avg Cost: $0.00000980\n",
      "    Pos 2: 0/1 = 0.0000, Avg Cost: $0.00001330\n",
      "    Pos 3: 0/2 = 0.0000, Avg Cost: $0.00001610\n",
      "    Pos 5: 0/4 = 0.0000, Avg Cost: $0.00002125\n",
      "\n",
      "deepseek/deepseek-chat:\n",
      "  Overall: 1/12 = 0.0833\n",
      "  Average Cost: $0.00005521\n",
      "  By Position:\n",
      "    Pos 1: 0/1 = 0.0000, Avg Cost: $0.00003359\n",
      "    Pos 2: 1/6 = 0.1667, Avg Cost: $0.00004643\n",
      "    Pos 3: 0/1 = 0.0000, Avg Cost: $0.00005625\n",
      "    Pos 4: 0/1 = 0.0000, Avg Cost: $0.00007032\n",
      "    Pos 5: 0/3 = 0.0000, Avg Cost: $0.00007458\n",
      "\n",
      "\n",
      "=== TEST SET RESULTS ===\n",
      "Total Test Questions: 80\n",
      "Success Rate: 0.4625\n",
      "Average Steps: 3.4000\n",
      "Average Cost per Question: $0.00005785\n",
      "Average Cost per Successful Question: $0.00002156\n",
      "Success Rate by Position:\n",
      "  Position 1: 0.3125\n",
      "  Position 2: 0.0750\n",
      "  Position 3: 0.0500\n",
      "  Position 4: 0.0250\n",
      "  Position 5: 0.0000\n",
      "\n",
      "Test Set Model Performance:\n",
      "\n",
      "arcee-ai/trinity-large-preview:free:\n",
      "  Overall: 9/52 = 0.1731\n",
      "  Average Cost: $0.00000000\n",
      "  By Position:\n",
      "    Pos 1: 0/1 = 0.0000, Avg Cost: $0.00000000\n",
      "    Pos 2: 6/22 = 0.2727, Avg Cost: $0.00000000\n",
      "    Pos 3: 2/16 = 0.1250, Avg Cost: $0.00000000\n",
      "    Pos 4: 1/8 = 0.1250, Avg Cost: $0.00000000\n",
      "    Pos 5: 0/5 = 0.0000, Avg Cost: $0.00000000\n",
      "\n",
      "mistralai/mistral-small-3.1-24b-instruct:\n",
      "  Overall: 15/68 = 0.2206\n",
      "  Average Cost: $0.00000632\n",
      "  By Position:\n",
      "    Pos 1: 13/36 = 0.3611, Avg Cost: $0.00000445\n",
      "    Pos 2: 0/8 = 0.0000, Avg Cost: $0.00000651\n",
      "    Pos 3: 1/5 = 0.2000, Avg Cost: $0.00000773\n",
      "    Pos 4: 1/13 = 0.0769, Avg Cost: $0.00000887\n",
      "    Pos 5: 0/6 = 0.0000, Avg Cost: $0.00001053\n",
      "\n",
      "microsoft/phi-4:\n",
      "  Overall: 0/33 = 0.0000\n",
      "  Average Cost: $0.00001260\n",
      "  By Position:\n",
      "    Pos 2: 0/4 = 0.0000, Avg Cost: $0.00000829\n",
      "    Pos 3: 0/6 = 0.0000, Avg Cost: $0.00001261\n",
      "    Pos 4: 0/14 = 0.0000, Avg Cost: $0.00001265\n",
      "    Pos 5: 0/9 = 0.0000, Avg Cost: $0.00001443\n",
      "\n",
      "meta-llama/llama-4-maverick:\n",
      "  Overall: 1/31 = 0.0323\n",
      "  Average Cost: $0.00002789\n",
      "  By Position:\n",
      "    Pos 1: 1/2 = 0.5000, Avg Cost: $0.00001426\n",
      "    Pos 2: 0/9 = 0.0000, Avg Cost: $0.00002445\n",
      "    Pos 3: 0/7 = 0.0000, Avg Cost: $0.00002607\n",
      "    Pos 4: 0/2 = 0.0000, Avg Cost: $0.00002861\n",
      "    Pos 5: 0/11 = 0.0000, Avg Cost: $0.00003421\n",
      "\n",
      "google/gemini-2.0-flash-001:\n",
      "  Overall: 0/28 = 0.0000\n",
      "  Average Cost: $0.00001845\n",
      "  By Position:\n",
      "    Pos 2: 0/4 = 0.0000, Avg Cost: $0.00001458\n",
      "    Pos 3: 0/9 = 0.0000, Avg Cost: $0.00001602\n",
      "    Pos 4: 0/6 = 0.0000, Avg Cost: $0.00001862\n",
      "    Pos 5: 0/9 = 0.0000, Avg Cost: $0.00002250\n",
      "\n",
      "deepseek/deepseek-chat:\n",
      "  Overall: 12/60 = 0.2000\n",
      "  Average Cost: $0.00004002\n",
      "  By Position:\n",
      "    Pos 1: 11/41 = 0.2683, Avg Cost: $0.00003169\n",
      "    Pos 2: 0/8 = 0.0000, Avg Cost: $0.00004883\n",
      "    Pos 3: 1/6 = 0.1667, Avg Cost: $0.00005921\n",
      "    Pos 4: 0/2 = 0.0000, Avg Cost: $0.00006652\n",
      "    Pos 5: 0/3 = 0.0000, Avg Cost: $0.00007432\n",
      "\n",
      "\n",
      "=== OVERALL RESULTS (TRAIN + TEST) ===\n",
      "Total Overall Questions: 100\n",
      "Success Rate: 0.4500\n",
      "Average Steps: 3.4200\n",
      "Average Cost per Question: $0.00005881\n",
      "Average Cost per Successful Question: $0.00001958\n",
      "Success Rate by Position:\n",
      "  Position 1: 0.3100\n",
      "  Position 2: 0.0800\n",
      "  Position 3: 0.0400\n",
      "  Position 4: 0.0200\n",
      "  Position 5: 0.0000\n",
      "\n",
      "Overall Model Performance:\n",
      "\n",
      "arcee-ai/trinity-large-preview:free:\n",
      "  Overall: 10/64 = 0.1562\n",
      "  Average Cost: $0.00000000\n",
      "  By Position:\n",
      "    Pos 1: 0/2 = 0.0000, Avg Cost: $0.00000000\n",
      "    Pos 2: 7/27 = 0.2593, Avg Cost: $0.00000000\n",
      "    Pos 3: 2/17 = 0.1176, Avg Cost: $0.00000000\n",
      "    Pos 4: 1/13 = 0.0769, Avg Cost: $0.00000000\n",
      "    Pos 5: 0/5 = 0.0000, Avg Cost: $0.00000000\n",
      "\n",
      "mistralai/mistral-small-3.1-24b-instruct:\n",
      "  Overall: 21/90 = 0.2333\n",
      "  Average Cost: $0.00000610\n",
      "  By Position:\n",
      "    Pos 1: 19/52 = 0.3654, Avg Cost: $0.00000441\n",
      "    Pos 2: 0/10 = 0.0000, Avg Cost: $0.00000645\n",
      "    Pos 3: 1/5 = 0.2000, Avg Cost: $0.00000773\n",
      "    Pos 4: 1/15 = 0.0667, Avg Cost: $0.00000889\n",
      "    Pos 5: 0/8 = 0.0000, Avg Cost: $0.00001047\n",
      "\n",
      "microsoft/phi-4:\n",
      "  Overall: 0/42 = 0.0000\n",
      "  Average Cost: $0.00001243\n",
      "  By Position:\n",
      "    Pos 1: 0/1 = 0.0000, Avg Cost: $0.00000609\n",
      "    Pos 2: 0/4 = 0.0000, Avg Cost: $0.00000829\n",
      "    Pos 3: 0/11 = 0.0000, Avg Cost: $0.00001222\n",
      "    Pos 4: 0/17 = 0.0000, Avg Cost: $0.00001286\n",
      "    Pos 5: 0/9 = 0.0000, Avg Cost: $0.00001443\n",
      "\n",
      "meta-llama/llama-4-maverick:\n",
      "  Overall: 1/38 = 0.0263\n",
      "  Average Cost: $0.00002865\n",
      "  By Position:\n",
      "    Pos 1: 1/2 = 0.5000, Avg Cost: $0.00001426\n",
      "    Pos 2: 0/9 = 0.0000, Avg Cost: $0.00002445\n",
      "    Pos 3: 0/10 = 0.0000, Avg Cost: $0.00002678\n",
      "    Pos 4: 0/3 = 0.0000, Avg Cost: $0.00002829\n",
      "    Pos 5: 0/14 = 0.0000, Avg Cost: $0.00003481\n",
      "\n",
      "google/gemini-2.0-flash-001:\n",
      "  Overall: 0/36 = 0.0000\n",
      "  Average Cost: $0.00001825\n",
      "  By Position:\n",
      "    Pos 1: 0/1 = 0.0000, Avg Cost: $0.00000980\n",
      "    Pos 2: 0/5 = 0.0000, Avg Cost: $0.00001432\n",
      "    Pos 3: 0/11 = 0.0000, Avg Cost: $0.00001604\n",
      "    Pos 4: 0/6 = 0.0000, Avg Cost: $0.00001862\n",
      "    Pos 5: 0/13 = 0.0000, Avg Cost: $0.00002212\n",
      "\n",
      "deepseek/deepseek-chat:\n",
      "  Overall: 13/72 = 0.1806\n",
      "  Average Cost: $0.00004255\n",
      "  By Position:\n",
      "    Pos 1: 11/42 = 0.2619, Avg Cost: $0.00003174\n",
      "    Pos 2: 1/14 = 0.0714, Avg Cost: $0.00004780\n",
      "    Pos 3: 1/7 = 0.1429, Avg Cost: $0.00005878\n",
      "    Pos 4: 0/3 = 0.0000, Avg Cost: $0.00006778\n",
      "    Pos 5: 0/6 = 0.0000, Avg Cost: $0.00007445\n",
      "\n",
      "=== BATCH PERFORMANCE ===\n",
      "Batch 0: 8/20 = 0.4000, Train/Test: 20/0, Total Cost: $0.00125305, Avg Cost: $0.00006265\n",
      "Batch 1: 11/20 = 0.5500, Train/Test: 0/20, Total Cost: $0.00113478, Avg Cost: $0.00005674\n",
      "Batch 2: 10/20 = 0.5000, Train/Test: 0/20, Total Cost: $0.00122582, Avg Cost: $0.00006129\n",
      "Batch 3: 7/20 = 0.3500, Train/Test: 0/20, Total Cost: $0.00122610, Avg Cost: $0.00006131\n",
      "Batch 4: 9/20 = 0.4500, Train/Test: 0/20, Total Cost: $0.00104105, Avg Cost: $0.00005205\n",
      "\n",
      "Total Overall Cost (All Questions): $0.00588080\n",
      "Total Train Cost: $0.00125305\n",
      "Total Test Cost: $0.00462775\n",
      "\n",
      "=== IMPORTANT NOTES ON INTERPRETATION ===\n",
      "Since the LinUCB model is updated using data from both the train and test sets,\n",
      "the test set performance metrics do not represent the performance of a fixed,\n",
      "pre-trained model on unseen data. Instead, they reflect the model's performance\n",
      "while it is still learning and adapting to the test data (online evaluation).\n",
      "This setup allows us to observe how the bandit algorithm performs and adapts\n",
      "over time across the entire dataset, comparing performance between an initial\n",
      "phase (train) and a later phase (test).\n",
      "\n",
      "\n",
      "LinUCB Cascade with HotpotQA Dataset completed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    print(\"Starting LinUCB Cascade with HotpotQA Dataset\")\n",
    "    initialize_json_files()\n",
    "    dataset_full = load_math500_dataset(INPUT_JSON)\n",
    "    if not dataset_full:\n",
    "        print(\"Dataset is empty or could not be loaded. Exiting.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        num_to_process = int(input(f\"Enter the number of questions to process (max {len(dataset_full)}): \"))\n",
    "        num_to_process = min(num_to_process, len(dataset_full))\n",
    "    except ValueError:\n",
    "        num_to_process = len(dataset_full)\n",
    "        print(f\"Invalid input. Using all {num_to_process} questions.\")\n",
    "\n",
    "    random.seed(42) # for reproducible shuffle\n",
    "    random.shuffle(dataset_full)\n",
    "    dataset = dataset_full[:num_to_process]\n",
    "    train_size = int(TRAIN_RATIO * len(dataset))\n",
    "    print(f\"Using {len(dataset)} questions: {train_size} train, {len(dataset) - train_size} test.\")\n",
    "\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    feature_extractor.initialize(dataset)\n",
    "    linucb_model = LinUCBModel(model_names=AVAILABLE_LLMS)\n",
    "    cascade = BatchBudgetCascade(feature_extractor, linucb_model)\n",
    "\n",
    "    all_records = []\n",
    "    start_idx = 0\n",
    "    if os.path.exists(RECORDS_PATH):\n",
    "        try:\n",
    "            with open(RECORDS_PATH, 'r') as f: all_records = json.load(f)\n",
    "            processed_keys = {(r.get(\"unique_id\") or r.get(\"question\")) for r in all_records}\n",
    "            dataset = [q for q in dataset if (q.get(\"unique_id\") or q.get(\"question\")) not in processed_keys]\n",
    "            print(f\"Loaded {len(all_records)} existing records. {len(dataset)} new questions to process.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load existing records: {e}\")\n",
    "            all_records = []\n",
    "\n",
    "    if os.path.exists(LINUCB_MODEL_PATH):\n",
    "        linucb_model.load_model_state(LINUCB_MODEL_PATH)\n",
    "\n",
    "    try:\n",
    "        for i, question in enumerate(dataset):\n",
    "            phase = \"test\" if i >= train_size else \"train\"\n",
    "            print(f\"\\n--- Q{i+1}/{len(dataset)} ({phase}) ---\")\n",
    "            print(f\"Question: {question['question'][:100]}...\")\n",
    "            question_record = cascade.run_cascade_single_question(question)\n",
    "            question_record[\"phase\"] = phase\n",
    "            all_records.append(question_record)\n",
    "\n",
    "            if (i + 1) % UPDATE_FREQUENCY == 0:\n",
    "                save_records_with_backup(all_records, RECORDS_PATH)\n",
    "                linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcessing interrupted. Saving progress...\")\n",
    "    finally:\n",
    "        save_records_with_backup(all_records, RECORDS_PATH)\n",
    "        linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
    "        analyze_results(all_records)\n",
    "        print(\"\\nLinUCB Cascade with HotpotQA Dataset completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
