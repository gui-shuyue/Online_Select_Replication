{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import tiktoken\n",
        "import google.generativeai as genai\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import math\n",
        "from collections import defaultdict\n",
        "from openai import OpenAI\n",
        "import math\n",
        "\n",
        "\n",
        "BASE_FEATURE_DIM = 384  # Dimension for question embeddings\n",
        "ANSWER_EMBED_DIM = 384  # Dimension for answer embeddings\n",
        "CASCADE_LENGTH = 4      # Number of attempts in the cascade (K)\n",
        "UPDATE_FREQUENCY = 1    # Update JSON records after every question\n",
        "USE_EMBEDDINGS = True   # Use embeddings instead of TF-IDF\n",
        "\n",
        "# LinUCB parameters\n",
        "ALPHA = 0.675           # Exploration-exploitation trade-off parameter\n",
        "LAMBDA_REG = 0.45       # Regularization parameter for LinUCB matrix initialization\n",
        "\n",
        "# Value Density & Cost Estimation parameters\n",
        "BETA_COST = 1.0         # Controls width of LCB interval for LLM output cost estimation\n",
        "EPSILON_VD = 1e-10      # Small positive value for value density calculation\n",
        "DEFAULT_FALLBACK_COST = 0.0001  # Default cost when insufficient data"
      ],
      "metadata": {
        "id": "U9wlFlmndmcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENROUTER_API_KEY = # Input Openrouter API key hr\n",
        "OPENROUTER_BASE_URL = # Input Openrouter URL\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "   \"mistralai/mistral-small-3.1-24b-instruct\": {\"input_cost\": 0.05 / 1e6, \"output_cost\": 0.15 / 1e6},\n",
        "    \"microsoft/phi-4\": {\"input_cost\": 0.07 / 1e6, \"output_cost\": 0.14 / 1e6},\n",
        "    \"meta-llama/llama-4-maverick\": {\"input_cost\": 0.17 / 1e6, \"output_cost\": 0.16 / 1e6},\n",
        "    \"google/gemini-2.0-flash-001\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
        "    \"openai/gpt-4.1-nano\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
        "    \"deepseek/deepseek-chat\": {\"input_cost\": 0.38 / 1e6, \"output_cost\": 0.89 / 1e6},\n",
        "}"
      ],
      "metadata": {
        "id": "NK2dCs_Rd9t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtsTphYkdJ1H"
      },
      "outputs": [],
      "source": [
        "GRADER_MODEL_NAME = \"google/gemini-2.0-flash-lite-001\"\n",
        "\n",
        "AVAILABLE_LLMS = list(MODELS_CONFIG.keys())\n",
        "LLM_ID_DIM = len(AVAILABLE_LLMS)\n",
        "\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=OPENROUTER_BASE_URL,\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        "    # You might need to add default headers if required by your OpenRouter setup/account\n",
        "    # default_headers={\"HTTP-Referer\": \"YOUR_SITE_URL\", \"X-Title\": \"YOUR_APP_NAME\"}\n",
        ")\n",
        "INPUT_JSON = \"Math500.json\"\n",
        "RECORDS_PATH = \"M92V2.json\"  # The name of output file is called M92\n",
        "LINUCB_MODEL_PATH = \"M92V2.npz\"\n",
        "SUMMARY_STATS_PATH = \"M92V2.txt\"\n",
        "BACKUP_SUFFIX = \".bak\"  # For backing up existing files\n",
        "\n",
        "CONTEXT_FEATURE_DIM = ANSWER_EMBED_DIM + LLM_ID_DIM + ANSWER_EMBED_DIM\n",
        "TOTAL_FEATURE_DIM = BASE_FEATURE_DIM + 1 + CONTEXT_FEATURE_DIM\n",
        "\n",
        "class ModelCostEstimator:\n",
        "    def __init__(self, model_names):\n",
        "        self.stats = {}\n",
        "        for model_name in model_names:\n",
        "            self.stats[model_name] = {\n",
        "                \"overall\": {\"count\": 0, \"total_output_cost\": 0.0, \"sum_sq_output_cost\": 0.0},\n",
        "                \"by_position\": {\n",
        "                    str(i): {\"count\": 0, \"total_output_cost\": 0.0, \"sum_sq_output_cost\": 0.0}\n",
        "                    for i in range(1, CASCADE_LENGTH + 1)\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def update_stats(self, model_name, step, actual_output_cost):\n",
        "        step_str = str(step)\n",
        "        if model_name in self.stats and step_str in self.stats[model_name][\"by_position\"]:\n",
        "            pos_stats = self.stats[model_name][\"by_position\"][step_str]\n",
        "            pos_stats[\"count\"] += 1\n",
        "            pos_stats[\"total_output_cost\"] += actual_output_cost\n",
        "            pos_stats[\"sum_sq_output_cost\"] += actual_output_cost ** 2\n",
        "\n",
        "        if model_name in self.stats:\n",
        "            overall_stats = self.stats[model_name][\"overall\"]\n",
        "            overall_stats[\"count\"] += 1\n",
        "            overall_stats[\"total_output_cost\"] += actual_output_cost\n",
        "            overall_stats[\"sum_sq_output_cost\"] += actual_output_cost ** 2\n",
        "\n",
        "    def estimate_lcb_output_cost(self, model_name, step, beta=BETA_COST, default_cost=DEFAULT_FALLBACK_COST):\n",
        "        \"\"\"Estimate the Lower Confidence Bound (LCB) of output cost for a model at a specific step.\"\"\"\n",
        "        step_str = str(step)\n",
        "\n",
        "        # Try position-specific stats first\n",
        "        if model_name in self.stats and step_str in self.stats[model_name][\"by_position\"]:\n",
        "            pos_stats = self.stats[model_name][\"by_position\"][step_str]\n",
        "            if pos_stats[\"count\"] >= 2:\n",
        "                mean_cost = pos_stats[\"total_output_cost\"] / pos_stats[\"count\"]\n",
        "                variance = (pos_stats[\"sum_sq_output_cost\"] / pos_stats[\"count\"]) - (mean_cost ** 2)\n",
        "                variance = max(0, variance)  # Ensure non-negative variance\n",
        "                std_dev = math.sqrt(variance)\n",
        "                std_error = std_dev / math.sqrt(pos_stats[\"count\"])\n",
        "                lcb_cost = mean_cost - beta * std_error\n",
        "                return max(EPSILON_VD / 10, lcb_cost)  # Ensure positive cost for value density\n",
        "\n",
        "        # Fallback to overall stats if position-specific stats are insufficient\n",
        "        if model_name in self.stats:\n",
        "            overall_stats = self.stats[model_name][\"overall\"]\n",
        "            if overall_stats[\"count\"] >= 2:\n",
        "                mean_cost = overall_stats[\"total_output_cost\"] / overall_stats[\"count\"]\n",
        "                variance = (overall_stats[\"sum_sq_output_cost\"] / overall_stats[\"count\"]) - (mean_cost ** 2)\n",
        "                variance = max(0, variance)\n",
        "                std_dev = math.sqrt(variance)\n",
        "                std_error = std_dev / math.sqrt(overall_stats[\"count\"])\n",
        "                lcb_cost = mean_cost - beta * std_error\n",
        "                return max(EPSILON_VD / 10, lcb_cost)\n",
        "\n",
        "        # Use default cost if there is insufficient data\n",
        "        return default_cost\n",
        "\n",
        "    def save_state(self, file_path):\n",
        "        try:\n",
        "            save_dict = {}\n",
        "            for model_name, model_stats in self.stats.items():\n",
        "                overall = model_stats[\"overall\"]\n",
        "                save_dict[f\"overall_count_{model_name}\"] = overall[\"count\"]\n",
        "                save_dict[f\"overall_total_output_cost_{model_name}\"] = overall[\"total_output_cost\"]\n",
        "                save_dict[f\"overall_sum_sq_output_cost_{model_name}\"] = overall[\"sum_sq_output_cost\"]\n",
        "                for pos, pos_stats in model_stats[\"by_position\"].items():\n",
        "                    save_dict[f\"pos{pos}_count_{model_name}\"] = pos_stats[\"count\"]\n",
        "                    save_dict[f\"pos{pos}_total_output_cost_{model_name}\"] = pos_stats[\"total_output_cost\"]\n",
        "                    save_dict[f\"pos{pos}_sum_sq_output_cost_{model_name}\"] = pos_stats[\"sum_sq_output_cost\"]\n",
        "            np.savez_compressed(file_path, **save_dict)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_state(self, file_path):\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "            for model_name in self.stats:\n",
        "                if f\"overall_count_{model_name}\" in loaded:\n",
        "                    overall = self.stats[model_name][\"overall\"]\n",
        "                    overall[\"count\"] = int(loaded[f\"overall_count_{model_name}\"])\n",
        "                    overall[\"total_output_cost\"] = float(loaded[f\"overall_total_output_cost_{model_name}\"])\n",
        "                    overall[\"sum_sq_output_cost\"] = float(loaded[f\"overall_sum_sq_output_cost_{model_name}\"])\n",
        "                for pos in self.stats[model_name][\"by_position\"]:\n",
        "                    if f\"pos{pos}_count_{model_name}\" in loaded:\n",
        "                        pos_stats = self.stats[model_name][\"by_position\"][pos]\n",
        "                        pos_stats[\"count\"] = int(loaded[f\"pos{pos}_count_{model_name}\"])\n",
        "                        pos_stats[\"total_output_cost\"] = float(loaded[f\"pos{pos}_total_output_cost_{model_name}\"])\n",
        "                        pos_stats[\"sum_sq_output_cost\"] = float(loaded[f\"pos{pos}_sum_sq_output_cost_{model_name}\"])\n",
        "            print(f\"Loaded model cost estimator state from {file_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, feature_dim=BASE_FEATURE_DIM, use_embeddings=USE_EMBEDDINGS):\n",
        "        self.feature_dim = feature_dim\n",
        "        self.use_embeddings = use_embeddings\n",
        "        if use_embeddings:\n",
        "            try:\n",
        "                self.embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
        "                print(\"Initialized sentence transformer embedding model\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error initializing sentence transformer: {e}\\nFalling back to TF-IDF.\")\n",
        "                self.use_embeddings = False\n",
        "        if not self.use_embeddings:\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            self.svd = None\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, questions):\n",
        "        if not self.use_embeddings:\n",
        "            from sklearn.decomposition import TruncatedSVD\n",
        "            all_text = [q[\"question\"] for q in questions]\n",
        "            self.vectorizer.fit(all_text)\n",
        "            dtm = self.vectorizer.transform(all_text)\n",
        "            n_components = min(self.feature_dim, dtm.shape[1])\n",
        "            self.svd = TruncatedSVD(n_components=n_components)\n",
        "            self.svd.fit(dtm)\n",
        "            print(f\"Using TF-IDF with SVD dimensionality reduction to {n_components} features\")\n",
        "            print(f\"Explained variance ratio: {sum(self.svd.explained_variance_ratio_):.4f}\")\n",
        "        self.initialized = True\n",
        "\n",
        "    def extract_features(self, question):\n",
        "        \"\"\"Extract features from a question (no options for Math500).\"\"\"\n",
        "        if not self.initialized:\n",
        "            raise ValueError(\"Feature extractor not initialized. Call initialize() first.\")\n",
        "        text = question[\"question\"]\n",
        "        if self.use_embeddings:\n",
        "            features = self.embedding_model.encode([text])[0]\n",
        "        else:\n",
        "            tfidf_vector = self.vectorizer.transform([text])\n",
        "            features = self.svd.transform(tfidf_vector)[0]\n",
        "            # Ensure we have exactly feature_dim dimensions\n",
        "            if len(features) < self.feature_dim:\n",
        "                padding = np.zeros(self.feature_dim - len(features))\n",
        "                features = np.concatenate([features, padding])\n",
        "        return features\n",
        "\n",
        "    def extract_answer_features(self, answer_text):\n",
        "        if not answer_text:\n",
        "            return np.zeros(ANSWER_EMBED_DIM)\n",
        "        if self.use_embeddings:\n",
        "            try:\n",
        "                features = self.embedding_model.encode([answer_text])[0]\n",
        "                if features is None:\n",
        "                    print(f\"Warning: Embedding for answer text resulted in None. Returning zero vector.\")\n",
        "                    return np.zeros(ANSWER_EMBED_DIM)\n",
        "                # Handle if the embedding doesn't match the expected dimension\n",
        "                if len(features) != ANSWER_EMBED_DIM:\n",
        "                    features = features[:ANSWER_EMBED_DIM] if len(features) > ANSWER_EMBED_DIM else np.concatenate([features, np.zeros(ANSWER_EMBED_DIM - len(features))])\n",
        "                return features\n",
        "            except Exception as e:\n",
        "                print(f\"Error embedding answer: {e}\")\n",
        "                return np.zeros(ANSWER_EMBED_DIM)\n",
        "        else:\n",
        "            # For simplicity, use zero vector if not using embeddings\n",
        "            return np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "    def construct_feature_vector(self, base_features, step_i, failed_answers, failed_llm_ids, model_name_to_index):\n",
        "        \"\"\"\n",
        "        Construct the augmented feature vector for LinUCB.\n",
        "        - base_features: Features extracted from the question\n",
        "        - step_i: Current step in the cascade (1-indexed)\n",
        "        - failed_answers: List of previous failed answer strings\n",
        "        - failed_llm_ids: List of previous failed LLM IDs\n",
        "        - model_name_to_index: Mapping from model names to indices\n",
        "        \"\"\"\n",
        "        normalized_step = np.array([step_i / CASCADE_LENGTH])\n",
        "\n",
        "        # Initialize context features - last answer embedding\n",
        "        last_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "        if step_i > 1 and failed_answers:\n",
        "            last_answer_features = self.extract_answer_features(failed_answers[-1])\n",
        "\n",
        "        # Calculate Last Failed LLM ID One-Hot encoding\n",
        "        last_llm_onehot = np.zeros(LLM_ID_DIM)\n",
        "        if step_i > 1 and failed_llm_ids:\n",
        "            last_llm_name = failed_llm_ids[-1]\n",
        "            if last_llm_name in model_name_to_index:\n",
        "                last_llm_onehot[model_name_to_index[last_llm_name]] = 1.0\n",
        "\n",
        "        # Calculate Average Failed Answer Embedding\n",
        "        avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "        if step_i > 1 and failed_answers:\n",
        "            all_answer_features = [self.extract_answer_features(ans) for ans in failed_answers]\n",
        "            if all_answer_features:\n",
        "                avg_answer_features = np.mean(all_answer_features, axis=0)\n",
        "        if avg_answer_features.shape == (): # Handle case where avg_answer_features might be a scalar\n",
        "            avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "        context_features = np.concatenate([last_answer_features, last_llm_onehot, avg_answer_features])\n",
        "        augmented_features = np.concatenate([base_features, normalized_step, context_features])\n",
        "\n",
        "        # Ensure the final dimension matches expectations\n",
        "        if augmented_features.shape[0] != TOTAL_FEATURE_DIM:\n",
        "            raise ValueError(f\"Constructed feature vector dimension {augmented_features.shape[0]} != expected {TOTAL_FEATURE_DIM}\")\n",
        "        return augmented_features\n",
        "\n",
        "\n",
        "class LinUCBModel:\n",
        "    def __init__(self, model_names, feature_dim=TOTAL_FEATURE_DIM, alpha=ALPHA, lambda_reg=LAMBDA_REG):\n",
        "        self.model_names = model_names\n",
        "        self.feature_dim = feature_dim\n",
        "        self.alpha = alpha\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.model_name_to_index = {name: i for i, name in enumerate(model_names)}\n",
        "        self.cost_estimator = ModelCostEstimator(model_names)\n",
        "        self.models = {\n",
        "            model_name: {\n",
        "                'A': np.identity(feature_dim) * lambda_reg,\n",
        "                'b': np.zeros(feature_dim),\n",
        "                'last_call_time': 0\n",
        "            } for model_name in model_names\n",
        "        }\n",
        "\n",
        "    def update_reward_only(self, model_name, feature_vector, reward):\n",
        "        \"\"\"Update the LinUCB model parameters based on observed reward.\"\"\"\n",
        "        model = self.models[model_name]\n",
        "        model['A'] += np.outer(feature_vector, feature_vector)\n",
        "        model['b'] += feature_vector * reward\n",
        "\n",
        "    def calculate_ucb_scores(self, feature_vector):\n",
        "        \"\"\"Calculate UCB scores for model selection.\"\"\"\n",
        "        scores = {}\n",
        "        for model_name in self.model_names:\n",
        "            model = self.models[model_name]\n",
        "            try:\n",
        "                A_inv = np.linalg.inv(model['A'])\n",
        "                theta = A_inv.dot(model['b'])\n",
        "                ucb_term = self.alpha * np.sqrt(feature_vector.dot(A_inv).dot(feature_vector))\n",
        "                expected_reward = feature_vector.dot(theta)\n",
        "                scores[model_name] = {\n",
        "                    \"p_ia\": float(expected_reward),\n",
        "                    \"e_ia\": float(ucb_term),\n",
        "                    \"ucb_score\": float(expected_reward + ucb_term)\n",
        "                }\n",
        "            except np.linalg.LinAlgError:\n",
        "                scores[model_name] = {\"p_ia\": 0.0, \"e_ia\": 0.0, \"ucb_score\": 0.0}\n",
        "        return scores\n",
        "\n",
        "    def select_model_value_density(self, feature_vector, step, prompt, remaining_budget, is_test_phase, reward_importance, cost_importance, used_llms_in_current_cascade=None):\n",
        "        \"\"\"Select a model based on strategy: pure UCB in train, weighted score in test.\"\"\"\n",
        "        scores = self.calculate_ucb_scores(feature_vector)\n",
        "        if not scores:\n",
        "            return None, {}\n",
        "        if used_llms_in_current_cascade is None:\n",
        "            used_llms_in_current_cascade = []\n",
        "\n",
        "        best_model = None\n",
        "        max_selection_score = -float('inf')\n",
        "\n",
        "        for model_name, score_info in scores.items():\n",
        "            # This except block is expected for OpenRouter models listed in MODELS_CONFIG\n",
        "            try:\n",
        "                deterministic_input_cost = len(prompt) // 4 * MODELS_CONFIG[model_name][\"input_cost\"]\n",
        "            except Exception:\n",
        "                print(f\"Warning: Input cost configuration missing for {model_name}. Using default.\")\n",
        "                deterministic_input_cost = len(prompt) // 4 * DEFAULT_FALLBACK_COST\n",
        "\n",
        "            lcb_output_cost_estimate = self.cost_estimator.estimate_lcb_output_cost(model_name, step)\n",
        "            score_info['total_estimated_cost'] = float(deterministic_input_cost + lcb_output_cost_estimate)\n",
        "            # 'remaining_budget' and 'budget_sufficient' will be updated per model inside phase logic\n",
        "\n",
        "        # --- Phase-specific selection logic ---\n",
        "        if not is_test_phase:\n",
        "            # TRAIN PHASE: Pure UCB selection\n",
        "            for model_name, score_info in scores.items():\n",
        "                score_info['budget_sufficient'] = True # Budget is effectively infinite for selection\n",
        "                current_model_selection_score = -float('inf')\n",
        "                if model_name in used_llms_in_current_cascade:\n",
        "                    score_info['selection_strategy'] = 'Prohibited (Train - Re-selection)'\n",
        "                else:\n",
        "                    current_model_selection_score = score_info[\"ucb_score\"]\n",
        "                    score_info['selection_strategy'] = 'Pure UCB (Train)'\n",
        "                score_info['selection_score'] = float(current_model_selection_score)\n",
        "                if current_model_selection_score > max_selection_score:\n",
        "                    max_selection_score = current_model_selection_score\n",
        "                    best_model = model_name\n",
        "        else:\n",
        "            # TEST PHASE: Budget-aware, value-density selection\n",
        "            # Stage A: Identify budget-eligible models\n",
        "            eligible_models_data_list = []\n",
        "            for model_name, score_info in scores.items():\n",
        "                score_info['remaining_budget'] = float(remaining_budget)\n",
        "                if model_name in used_llms_in_current_cascade:\n",
        "                    score_info['selection_strategy'] = 'Prohibited (Test - Re-selection)'\n",
        "                    score_info['selection_score'] = -float('inf')\n",
        "                    continue\n",
        "                if score_info['total_estimated_cost'] <= remaining_budget:\n",
        "                    score_info['budget_sufficient'] = True\n",
        "                    eligible_models_data_list.append((model_name, score_info['total_estimated_cost'], score_info))\n",
        "                else:\n",
        "                    score_info['budget_sufficient'] = False\n",
        "                    score_info['selection_strategy'] = 'Budget Pruned (Test)'\n",
        "                    score_info['selection_score'] = -float('inf')\n",
        "\n",
        "            if not eligible_models_data_list:\n",
        "                return None, scores # All models were pruned by budget or re-selection\n",
        "\n",
        "            # Stage B: Find max_estimated_cost among ELIGIBLE models for normalization\n",
        "            all_eligible_costs = [cost for _, cost, _ in eligible_models_data_list]\n",
        "            max_eligible_cost = max(all_eligible_costs) if all_eligible_costs else EPSILON_VD\n",
        "            max_eligible_cost = max(max_eligible_cost, EPSILON_VD) # Safeguard against zero cost\n",
        "\n",
        "            # Stage C: Calculate final selection scores for ELIGIBLE models\n",
        "            for model_name, model_cost, score_info_ref in eligible_models_data_list:\n",
        "                ucb_score = score_info_ref[\"ucb_score\"]\n",
        "                cost_norm_factor = 1 + ((model_cost + EPSILON_VD) / (max_eligible_cost + EPSILON_VD))\n",
        "                stable_denominator = 1.0 + max(cost_norm_factor, EPSILON_VD)\n",
        "                current_model_selection_score = ucb_score / stable_denominator\n",
        "\n",
        "                score_info_ref['selection_strategy'] = 'Double/Norm (Test)'\n",
        "                score_info_ref['selection_score_terms'] = {\n",
        "                    'ucb_score_numerator': float(ucb_score),\n",
        "                    'stable_denominator_used': float(stable_denominator),\n",
        "                }\n",
        "                score_info_ref['selection_score'] = float(current_model_selection_score)\n",
        "\n",
        "                if current_model_selection_score > max_selection_score:\n",
        "                    max_selection_score = current_model_selection_score\n",
        "                    best_model = model_name\n",
        "\n",
        "        return best_model, scores\n",
        "\n",
        "    def register_model_call(self, model_name):\n",
        "        self.models[model_name]['last_call_time'] = time.time()\n",
        "\n",
        "    def respect_rate_limit(self, model_name):\n",
        "        \"\"\"Wait if necessary to respect the model's rate limit.\"\"\"\n",
        "        # For the specified OpenRouter models, 'rpm' is not defined, so this block will be skipped.\n",
        "        model_cfg = MODELS_CONFIG.get(model_name)\n",
        "        if model_cfg and \"rpm\" in model_cfg:\n",
        "            model_state = self.models[model_name]\n",
        "            min_seconds_between_calls = 60.0 / model_cfg[\"rpm\"]\n",
        "            time_since_last_call = time.time() - model_state['last_call_time']\n",
        "            if time_since_last_call < min_seconds_between_calls:\n",
        "                time.sleep(min_seconds_between_calls - time_since_last_call)\n",
        "\n",
        "    def save_model_state(self, file_path):\n",
        "        \"\"\"Save the model state to a file using numpy's compressed format.\"\"\"\n",
        "        save_dict = {f'A_{model_name}': model['A'] for model_name, model in self.models.items()}\n",
        "        for model_name, model in self.models.items():\n",
        "            save_dict[f'b_{model_name}'] = model['b']\n",
        "        np.savez_compressed(file_path, **save_dict)\n",
        "        # Save cost estimator state\n",
        "        self.cost_estimator.save_state(file_path.replace('.npz', '_cost_estimator.npz'))\n",
        "\n",
        "    def load_model_state(self, file_path):\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "            for model_name in self.models.keys():\n",
        "                if f'A_{model_name}' in loaded and f'b_{model_name}' in loaded:\n",
        "                    self.models[model_name]['A'] = loaded[f'A_{model_name}']\n",
        "                    self.models[model_name]['b'] = loaded[f'b_{model_name}']\n",
        "            print(f\"Loaded LinUCB model state from {file_path}\")\n",
        "            # Load cost estimator state\n",
        "            cost_estimator_path = file_path.replace('.npz', '_cost_estimator.npz')\n",
        "            if os.path.exists(cost_estimator_path):\n",
        "                self.cost_estimator.load_state(cost_estimator_path)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "class BatchBudgetCascade:\n",
        "    def __init__(self, feature_extractor, linucb_model, cascade_length=CASCADE_LENGTH):\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.linucb_model = linucb_model\n",
        "        self.cascade_length = cascade_length\n",
        "\n",
        "    def format_prompt(self, question, failed_answers=None, failed_llm_ids=None):\n",
        "        prompt = f\"Solve the following math problem: {question['question']}\\n\\n\"\n",
        "        prompt += \"Also provide an explnation in one/serveral very short yet consice complete sentence within 75 words in total.\\n\"\n",
        "        prompt += \"At the end, clearly state your final answer in LaTeX format, enclosed within \\\\boxed{}.\\n\"\n",
        "        prompt += \"For example: 'The final answer is \\\\boxed{x=5}'.\"\n",
        "        if failed_answers and failed_llm_ids:\n",
        "            prompt += \"\\n\\nNote: The following previous attempts were incorrect. Please provide a different solution:\\n\"\n",
        "            for i, answer_info in enumerate(failed_answers):\n",
        "                prompt += f\"- Attempt {i+1} (by {failed_llm_ids[i]}) led to: {answer_info}\\n\"\n",
        "        return prompt\n",
        "\n",
        "    def parse_llm_answer(self, answer_text):\n",
        "        \"\"\"\n",
        "        Modified: Returns the stripped raw text as the answer for grading,\n",
        "        and the full raw text as the explanation (matching Code 1's behavior).\n",
        "        \"\"\"\n",
        "        if not answer_text:\n",
        "            return \"\", \"\"\n",
        "        return answer_text.strip(), answer_text\n",
        "\n",
        "    def grade_with_gemma12b(self, llm_answer_latex, ground_truth_latex):\n",
        "        \"\"\"Grade an answer against the ground truth using the grader model.\"\"\"\n",
        "        if llm_answer_latex is None or llm_answer_latex == ground_truth_latex:\n",
        "            return llm_answer_latex is not None\n",
        "\n",
        "        prompt = f\"Expression 1: {llm_answer_latex}\\nExpression 2: {ground_truth_latex}\\n\\n\"\n",
        "        prompt += \"Expression 2 is the answer and expression is attempt by student,  look at their final answer only which might be boxed, does student get the final expected answerï¼Ÿ Respond with only the word 'True' or 'False'.\"\n",
        "        try:\n",
        "            time.sleep(0.5)  # Simple rate limiting for the grader\n",
        "            api_response = openrouter_client.chat.completions.create(\n",
        "                model=GRADER_MODEL_NAME,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0, # Keep it deterministic for grading\n",
        "                max_tokens=10\n",
        "            )\n",
        "            grader_response_text = api_response.choices[0].message.content.strip().lower()\n",
        "            if \"true\" in grader_response_text:\n",
        "                return True\n",
        "            elif \"false\" in grader_response_text:\n",
        "                return False\n",
        "            else:\n",
        "                print(f\"Warning: Grader returned ambiguous response: {grader_response_text}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling grader model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def calculate_token_cost(self, model_name, prompt, response_text, usage_info=None):\n",
        "        \"\"\"Calculate the actual cost. For OpenRouter, relies on usage_info from API response.\"\"\"\n",
        "        input_tokens, output_tokens, total_cost = 0, 0, 0.0\n",
        "        error_message = None\n",
        "        if model_name in MODELS_CONFIG:\n",
        "            model_cfg = MODELS_CONFIG[model_name]\n",
        "            if usage_info:\n",
        "                input_tokens = usage_info.get(\"prompt_tokens\", 0)\n",
        "                output_tokens = usage_info.get(\"completion_tokens\", 0)\n",
        "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
        "            else:\n",
        "                error_message = f\"Usage info not available for {model_name}. Cost is a rough estimate.\"\n",
        "                input_tokens = len(prompt) // 4\n",
        "                output_tokens = len(response_text) // 4 if response_text else 0\n",
        "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
        "        else:\n",
        "            error_message = f\"Model {model_name} not found in config for cost calculation.\"\n",
        "        result = {\"total_cost\": total_cost}\n",
        "        if error_message:\n",
        "            result[\"error\"] = error_message\n",
        "            print(f\"Cost calculation warning for {model_name}: {error_message}\")\n",
        "        return result\n",
        "\n",
        "    def query_llm(self, model_name, prompt):\n",
        "        raw_response_full, parsed_answer_tuple, cost_data = \"\", (\"\", \"\"), {}\n",
        "        usage_info = None\n",
        "        try:\n",
        "            self.linucb_model.respect_rate_limit(model_name)\n",
        "            if model_name in MODELS_CONFIG:\n",
        "                api_response = openrouter_client.chat.completions.create(\n",
        "                    model=model_name,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                )\n",
        "                raw_response_full = api_response.choices[0].message.content\n",
        "                if api_response.usage:\n",
        "                    usage_info = {\"prompt_tokens\": api_response.usage.prompt_tokens, \"completion_tokens\": api_response.usage.completion_tokens}\n",
        "                cost_data = self.calculate_token_cost(model_name, prompt, raw_response_full, usage_info=usage_info)\n",
        "                parsed_answer_tuple = self.parse_llm_answer(raw_response_full)\n",
        "                return raw_response_full, parsed_answer_tuple, cost_data\n",
        "            else:\n",
        "                raise ValueError(f\"Model {model_name} is not configured in MODELS_CONFIG.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying LLM {model_name}: {e}\")\n",
        "            self.linucb_model.register_model_call(model_name)\n",
        "            cost_data = self.calculate_token_cost(model_name, prompt, \"\", usage_info=None)\n",
        "            return \"\", (\"\", \"\"), cost_data\n",
        "\n",
        "    def run_cascade_single_question(self, question, current_question_budget=float('inf'), is_test_phase=False):\n",
        "        \"\"\"\n",
        "        Run the Value Density-based LinUCB cascade for a single question.\n",
        "        - question: The question to answer\n",
        "        - current_question_budget: Budget limit for this question (used in test phase)\n",
        "        - is_test_phase: Whether we're in the test phase (applies budget constraints)\n",
        "        \"\"\"\n",
        "        base_features = self.feature_extractor.extract_features(question)\n",
        "        failed_answers, failed_llm_ids, used_llms_this_question = [], [], []\n",
        "        question_total_cost, current_attempts_log = 0.0, []\n",
        "        final_status = \"Failure\"  # Default to failure, will update on success\n",
        "        remaining_budget = current_question_budget if is_test_phase else float('inf')\n",
        "\n",
        "        # Value Density Cascade Loop\n",
        "        for i in range(1, self.cascade_length + 1):\n",
        "            print(f\"Step {i}\")\n",
        "            # 1. Construct feature vector using history\n",
        "            x_i = self.feature_extractor.construct_feature_vector(base_features, i, failed_answers, failed_llm_ids, self.linucb_model.model_name_to_index)\n",
        "            # 2. Format prompt with context\n",
        "            prompt = self.format_prompt(question, failed_answers, failed_llm_ids)\n",
        "            # 3. Select model based on learned strategy, respecting budget\n",
        "            chosen_model, scores = self.linucb_model.select_model_value_density(x_i, i, prompt, remaining_budget, is_test_phase, REWARD_IMPORTANCE, COST_IMPORTANCE, used_llms_this_question)\n",
        "\n",
        "            if not chosen_model:\n",
        "                final_status = \"Failure - Budget Exceeded or No Suitable Model\" if is_test_phase else \"Failure - All LLMs Used (Train)\"\n",
        "                print(f\"Stopping cascade: {final_status}\")\n",
        "                break\n",
        "\n",
        "            print(f\"Selected: {chosen_model}, Strategy: {scores[chosen_model].get('selection_strategy', 'N/A')}, Score: {scores[chosen_model].get('selection_score', -inf):.4f}\")\n",
        "            # 4. Query the chosen model\n",
        "            raw_response, (answer_for_grading, explanation_text), cost_data = self.query_llm(chosen_model, prompt)\n",
        "            # 5. Extract costs\n",
        "            actual_total_cost = cost_data.get(\"total_cost\", 0.0)\n",
        "            actual_output_cost = cost_data.get(\"output_cost\", 0.0)\n",
        "            # 6. Update budget\n",
        "            question_total_cost += actual_total_cost\n",
        "            remaining_budget -= actual_total_cost\n",
        "            # 7. Update cost estimator with actual observed output cost\n",
        "            self.linucb_model.cost_estimator.update_stats(chosen_model, i, actual_output_cost)\n",
        "            # 8. Determine outcome using the grader\n",
        "            is_correct = self.grade_with_gemma12b(answer_for_grading, question['ground_truth_answer'])\n",
        "            reward = 1 if is_correct else 0\n",
        "            print(f\"Answer: {answer_for_grading}, Correct: {is_correct}, Cost: ${actual_total_cost:.8f}\")\n",
        "            # 9. Update LinUCB for the chosen model\n",
        "            self.linucb_model.update_reward_only(chosen_model, x_i, reward)\n",
        "            # 10. Log the attempt\n",
        "            current_attempts_log.append({\n",
        "                \"step\": i, \"chosen_model\": chosen_model, \"chosen_model_cost\": cost_data,\n",
        "                \"is_correct\": is_correct, \"reward_ri\": reward, \"llm_answer\": answer_for_grading,\n",
        "                \"llm_explanation\": explanation_text, \"scores_per_arm\": scores\n",
        "            })\n",
        "            # 11. Handle Outcome\n",
        "            if is_correct:\n",
        "                final_status = \"Success\"\n",
        "                print(f\"Success in step {i}!\")\n",
        "                break\n",
        "            else:\n",
        "                # Failure: Update history for the next step\n",
        "                failed_answers.append(answer_for_grading if answer_for_grading else \"Unknown\")\n",
        "                failed_llm_ids.append(chosen_model)\n",
        "                if chosen_model:\n",
        "                    used_llms_this_question.append(chosen_model)\n",
        "\n",
        "        return {\n",
        "            \"question\": question[\"question\"], \"ground_truth_answer\": question[\"ground_truth_answer\"],\n",
        "            \"final_status\": final_status, \"total_cost\": question_total_cost,\n",
        "            \"steps_taken\": len(current_attempts_log), \"attempts\": current_attempts_log,\n",
        "            \"is_test_phase\": is_test_phase, \"question_budget\": current_question_budget if is_test_phase else None\n",
        "        }\n",
        "\n",
        "\n",
        "def load_math500_dataset(json_path):\n",
        "    \"\"\"Load the Math500 dataset from a JSON file with robust error handling.\"\"\"\n",
        "    try:\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            raw_data = json.load(f)\n",
        "        # Assuming data is under the key \"test\"\n",
        "        return [{\n",
        "            \"question\": item[\"problem\"],\n",
        "            \"options\": [], # Math500 has no MCQs\n",
        "            \"ground_truth_answer\": item[\"answer\"],\n",
        "            \"unique_id\": item.get(\"unique_id\", \"Unknown\")\n",
        "        } for item in raw_data[\"test\"]]\n",
        "    except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:\n",
        "        print(f\"Error loading dataset from {json_path}: {e}\")\n",
        "        return []\n",
        "\n",
        "def save_records_with_backup(records, json_path):\n",
        "    if os.path.exists(json_path):\n",
        "        try:\n",
        "            os.replace(json_path, json_path + BACKUP_SUFFIX)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create backup: {e}\")\n",
        "    try:\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(records, f, indent=4)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving records: {e}\")\n",
        "        # Try to restore from backup if save failed\n",
        "        if os.path.exists(json_path + BACKUP_SUFFIX):\n",
        "            try:\n",
        "                os.replace(json_path + BACKUP_SUFFIX, json_path)\n",
        "            except: pass\n",
        "        return False\n",
        "\n",
        "def initialize_json_files():\n",
        "    if not os.path.exists(RECORDS_PATH):\n",
        "        with open(RECORDS_PATH, 'w') as f: json.dump([], f)\n",
        "    else:\n",
        "        try:\n",
        "            with open(RECORDS_PATH, 'r') as f:\n",
        "                if not isinstance(json.load(f), list): raise ValueError\n",
        "        except (json.JSONDecodeError, ValueError):\n",
        "            os.rename(RECORDS_PATH, RECORDS_PATH + BACKUP_SUFFIX)\n",
        "            with open(RECORDS_PATH, 'w') as f: json.dump([], f)\n",
        "\n",
        "\n",
        "def analyze_results(records):\n",
        "    \"\"\"Analyze and print results from the records\"\"\"\n",
        "    if not records:\n",
        "        print(\"No records to analyze\")\n",
        "        return\n",
        "\n",
        "    # Separate records into training and test sets\n",
        "    train_records = [r for r in records if not r.get(\"is_test_phase\", False)]\n",
        "    test_records = [r for r in records if r.get(\"is_test_phase\", True)]\n",
        "\n",
        "    # Extract data for analysis\n",
        "    total_train_questions = len(train_records)\n",
        "    total_test_questions = len(test_records)\n",
        "\n",
        "    train_success = sum(1 for r in train_records if r[\"final_status\"] == \"Success\")\n",
        "    test_success = sum(1 for r in test_records if r[\"final_status\"] == \"Success\")\n",
        "\n",
        "    train_success_rate = train_success / total_train_questions if total_train_questions > 0 else 0\n",
        "    test_success_rate = test_success / total_test_questions if total_test_questions > 0 else 0\n",
        "\n",
        "    # Calculate total steps\n",
        "    total_train_steps = sum(r[\"steps_taken\"] for r in train_records)\n",
        "    total_test_steps = sum(r[\"steps_taken\"] for r in test_records)\n",
        "\n",
        "    # Calculate average steps\n",
        "    avg_train_steps = total_train_steps / total_train_questions if total_train_questions > 0 else 0\n",
        "    avg_test_steps = total_test_steps / total_test_questions if total_test_questions > 0 else 0\n",
        "\n",
        "    # Calculate success by position\n",
        "    train_successes_by_position = [0] * CASCADE_LENGTH\n",
        "    test_successes_by_position = [0] * CASCADE_LENGTH\n",
        "\n",
        "    for record in train_records:\n",
        "        if record[\"final_status\"] == \"Success\":\n",
        "            position = len(record[\"attempts\"]) - 1  # 0-indexed\n",
        "            if position < CASCADE_LENGTH:\n",
        "                train_successes_by_position[position] += 1\n",
        "\n",
        "    for record in test_records:\n",
        "        if record[\"final_status\"] == \"Success\":\n",
        "            position = len(record[\"attempts\"]) - 1  # 0-indexed\n",
        "            if position < CASCADE_LENGTH:\n",
        "                test_successes_by_position[position] += 1\n",
        "\n",
        "    train_per_position_success = [count/total_train_questions for count in train_successes_by_position] if total_train_questions > 0 else [0] * CASCADE_LENGTH\n",
        "    test_per_position_success = [count/total_test_questions for count in test_successes_by_position] if total_test_questions > 0 else [0] * CASCADE_LENGTH\n",
        "\n",
        "    # Calculate costs\n",
        "    train_total_cost = sum(r[\"total_cost\"] for r in train_records)\n",
        "    test_total_cost = sum(r[\"total_cost\"] for r in test_records)\n",
        "\n",
        "    train_avg_cost = train_total_cost / total_train_questions if total_train_questions > 0 else 0\n",
        "    test_avg_cost = test_total_cost / total_test_questions if total_test_questions > 0 else 0\n",
        "\n",
        "    # Calculate cost per successful question\n",
        "    train_success_cost = sum(r[\"total_cost\"] for r in train_records if r[\"final_status\"] == \"Success\")\n",
        "    test_success_cost = sum(r[\"total_cost\"] for r in test_records if r[\"final_status\"] == \"Success\")\n",
        "\n",
        "    train_avg_cost_success = train_success_cost / train_success if train_success > 0 else 0\n",
        "    test_avg_cost_success = test_success_cost / test_success if test_success > 0 else 0\n",
        "\n",
        "    # Count budget exceeded in test phase\n",
        "    budget_exceeded_count = sum(1 for r in test_records if r[\"final_status\"] == \"Failure - Budget Exceeded\")\n",
        "    budget_exceeded_rate = budget_exceeded_count / total_test_questions if total_test_questions > 0 else 0\n",
        "\n",
        "    # Analyze model performance with per-position breakdown\n",
        "    model_metrics = {\n",
        "        \"train\": {model: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0,\n",
        "                          \"by_position\": defaultdict(lambda: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0})}\n",
        "                 for model in AVAILABLE_LLMS},\n",
        "        \"test\": {model: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0,\n",
        "                         \"by_position\": defaultdict(lambda: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0})}\n",
        "                for model in AVAILABLE_LLMS}\n",
        "    }\n",
        "\n",
        "    # Track LCB cost estimation accuracy (output cost only)\n",
        "    lcb_cost_accuracy = {model: {\"total_lcb_estimate\": 0.0, \"total_actual_cost\": 0.0, \"count\": 0}\n",
        "                         for model in AVAILABLE_LLMS}\n",
        "\n",
        "    # Process training records\n",
        "    for record in train_records:\n",
        "        for attempt in record[\"attempts\"]:\n",
        "            model = attempt[\"chosen_model\"]\n",
        "            is_correct = attempt[\"is_correct\"]\n",
        "            cost = attempt[\"chosen_model_cost\"][\"total_cost\"]\n",
        "            position = attempt[\"step\"]\n",
        "\n",
        "            # Update train stats\n",
        "            model_metrics[\"train\"][model][\"calls\"] += 1\n",
        "            model_metrics[\"train\"][model][\"total_cost\"] += cost\n",
        "            model_metrics[\"train\"][model][\"by_position\"][position][\"calls\"] += 1\n",
        "            model_metrics[\"train\"][model][\"by_position\"][position][\"total_cost\"] += cost\n",
        "\n",
        "            if is_correct:\n",
        "                model_metrics[\"train\"][model][\"successes\"] += 1\n",
        "                model_metrics[\"train\"][model][\"by_position\"][position][\"successes\"] += 1\n",
        "\n",
        "    # Process test records\n",
        "    for record in test_records:\n",
        "        for attempt in record[\"attempts\"]:\n",
        "            model = attempt[\"chosen_model\"]\n",
        "            is_correct = attempt[\"is_correct\"]\n",
        "            cost = attempt[\"chosen_model_cost\"][\"total_cost\"]\n",
        "            position = attempt[\"step\"]\n",
        "\n",
        "            # Update test stats\n",
        "            model_metrics[\"test\"][model][\"calls\"] += 1\n",
        "            model_metrics[\"test\"][model][\"total_cost\"] += cost\n",
        "            model_metrics[\"test\"][model][\"by_position\"][position][\"calls\"] += 1\n",
        "            model_metrics[\"test\"][model][\"by_position\"][position][\"total_cost\"] += cost\n",
        "\n",
        "            if is_correct:\n",
        "                model_metrics[\"test\"][model][\"successes\"] += 1\n",
        "                model_metrics[\"test\"][model][\"by_position\"][position][\"successes\"] += 1\n",
        "\n",
        "            # Track LCB output cost estimation accuracy (test phase only)\n",
        "            if \"chosen_model_cost\" in attempt and \"scores_per_arm\" in attempt and model in attempt[\"scores_per_arm\"]:\n",
        "                actual_output_cost = attempt[\"chosen_model_cost\"][\"output_cost\"]\n",
        "                if \"lcb_output_cost_estimate\" in attempt[\"scores_per_arm\"][model]:\n",
        "                    lcb_cost_accuracy[model][\"total_lcb_estimate\"] += attempt[\"scores_per_arm\"][model][\"lcb_output_cost_estimate\"]\n",
        "                    lcb_cost_accuracy[model][\"total_actual_cost\"] += actual_output_cost\n",
        "                    lcb_cost_accuracy[model][\"count\"] += 1\n",
        "\n",
        "    # Calculate success rates and average costs for models\n",
        "    for phase in [\"train\", \"test\"]:\n",
        "        for model, data in model_metrics[phase].items():\n",
        "            if data[\"calls\"] > 0:\n",
        "                data[\"success_rate\"] = data[\"successes\"] / data[\"calls\"]\n",
        "                data[\"avg_cost\"] = data[\"total_cost\"] / data[\"calls\"]\n",
        "\n",
        "                # Calculate position-specific metrics\n",
        "                for pos, pos_data in data[\"by_position\"].items():\n",
        "                    if pos_data[\"calls\"] > 0:\n",
        "                        pos_data[\"success_rate\"] = pos_data[\"successes\"] / pos_data[\"calls\"]\n",
        "                        pos_data[\"avg_cost\"] = pos_data[\"total_cost\"] / pos_data[\"calls\"]\n",
        "                    else:\n",
        "                        pos_data[\"success_rate\"] = 0\n",
        "                        pos_data[\"avg_cost\"] = 0\n",
        "            else:\n",
        "                data[\"success_rate\"] = 0\n",
        "                data[\"avg_cost\"] = 0\n",
        "\n",
        "    # Calculate LCB estimation accuracy\n",
        "    for model, data in lcb_cost_accuracy.items():\n",
        "        if data[\"count\"] > 0:\n",
        "            data[\"avg_lcb_estimate\"] = data[\"total_lcb_estimate\"] / data[\"count\"]\n",
        "            data[\"avg_actual_cost\"] = data[\"total_actual_cost\"] / data[\"count\"]\n",
        "            data[\"lcb_accuracy\"] = data[\"avg_lcb_estimate\"] / data[\"avg_actual_cost\"] if data[\"avg_actual_cost\"] > 0 else 0\n",
        "        else:\n",
        "            data[\"avg_lcb_estimate\"] = 0\n",
        "            data[\"avg_actual_cost\"] = 0\n",
        "            data[\"lcb_accuracy\"] = 0\n",
        "\n",
        "    # Generate summary text\n",
        "    summary = \"=== BUDGET-AWARE LinUCB CASCADE (MATH LATEX RESPONSE) ===\\n\\n\"\n",
        "    summary += f\"LLM Output Cost Beta: {BETA_COST}\\n\\n\"\n",
        "\n",
        "    summary += \"=== TRAIN PHASE RESULTS (30%) ===\\n\"\n",
        "    summary += f\"Total Questions: {total_train_questions}\\n\"\n",
        "    summary += f\"Success Rate: {train_success_rate:.4f}\\n\"\n",
        "    summary += f\"Average Steps (Train): {avg_train_steps:.4f}\\n\"\n",
        "    summary += f\"Average Cost per Question: ${train_avg_cost:.8f}\\n\"\n",
        "    summary += f\"Average Cost per Successful Question: ${train_avg_cost_success:.8f}\\n\"\n",
        "    summary += \"Success Rate by Position:\\n\"\n",
        "    for i, rate in enumerate(train_per_position_success):\n",
        "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== TEST PHASE RESULTS (70%) ===\\n\"\n",
        "    summary += f\"Total Questions: {total_test_questions}\\n\"\n",
        "    summary += f\"Success Rate: {test_success_rate:.4f}\\n\"\n",
        "    summary += f\"Average Steps (Test): {avg_test_steps:.4f}\\n\"\n",
        "    summary += f\"Average Cost per Question: ${test_avg_cost:.8f}\\n\"\n",
        "    summary += f\"Average Cost per Successful Question: ${test_avg_cost_success:.8f}\\n\"\n",
        "    summary += f\"Budget Exceeded Count: {budget_exceeded_count} ({budget_exceeded_rate:.4f})\\n\"\n",
        "    summary += \"Success Rate by Position:\\n\"\n",
        "    for i, rate in enumerate(test_per_position_success):\n",
        "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== MODEL PERFORMANCE (TRAIN PHASE) ===\\n\"\n",
        "    for model, metrics in model_metrics[\"train\"].items():\n",
        "        if metrics[\"calls\"] > 0:\n",
        "            summary += f\"{model}:\\n\"\n",
        "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
        "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
        "\n",
        "            # No need for position breakdown in train phase summary\n",
        "\n",
        "    summary += \"\\n=== MODEL PERFORMANCE (TEST PHASE) ===\\n\"\n",
        "    for model, metrics in model_metrics[\"test\"].items():\n",
        "        if metrics[\"calls\"] > 0:\n",
        "            summary += f\"{model}:\\n\"\n",
        "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
        "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
        "            summary += f\"  By Position:\\n\"\n",
        "\n",
        "            # Add per-position breakdown\n",
        "            for pos in sorted(metrics[\"by_position\"].keys()):\n",
        "                pos_data = metrics[\"by_position\"][pos]\n",
        "                if pos_data[\"calls\"] > 0:\n",
        "                    summary += f\"    Pos {pos}: {pos_data['successes']}/{pos_data['calls']} = {pos_data['success_rate']:.4f}, \"\n",
        "                    summary += f\"Avg Cost: ${pos_data['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== LLM LCB OUTPUT COST ESTIMATION ACCURACY (TEST PHASE) ===\\n\"\n",
        "    for model, data in lcb_cost_accuracy.items():\n",
        "        if data[\"count\"] > 0:\n",
        "            summary += f\"{model}: Avg LCB Est: ${data['avg_lcb_estimate']:.8f}, \"\n",
        "            summary += f\"Avg Actual: ${data['avg_actual_cost']:.8f}, Ratio: {data['lcb_accuracy']:.4f}\\n\"\n",
        "\n",
        "    summary += f\"\\nTotal Overall Cost (Train): ${train_total_cost:.8f}\\n\"\n",
        "    summary += f\"Total Overall Cost (Test): ${test_total_cost:.8f}\\n\"\n",
        "    summary += f\"Total Overall Cost: ${(train_total_cost + test_total_cost):.8f}\\n\"\n",
        "\n",
        "    # Print and save summary\n",
        "    print(summary)\n",
        "\n",
        "    with open(SUMMARY_STATS_PATH, 'w') as f:\n",
        "        f.write(summary)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"Starting LinUCB Cascade with Value Density & Budget (LaTeX Math Response)\")\n",
        "    initialize_json_files()\n",
        "    dataset_full = load_math500_dataset(INPUT_JSON)\n",
        "    if not dataset_full:\n",
        "        print(\"Dataset is empty or could not be loaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        num_to_process = int(input(f\"How many questions to process? (1-{len(dataset_full)}): \"))\n",
        "        dataset = random.sample(dataset_full, min(num_to_process, len(dataset_full)))\n",
        "    except ValueError:\n",
        "        dataset = random.sample(dataset_full, 10)\n",
        "    print(f\"Using {len(dataset)} questions from the Math500 dataset\")\n",
        "\n",
        "    feature_extractor = FeatureExtractor()\n",
        "    feature_extractor.initialize(dataset)\n",
        "    linucb_model = LinUCBModel(model_names=AVAILABLE_LLMS)\n",
        "    cascade = BatchBudgetCascade(feature_extractor, linucb_model)\n",
        "\n",
        "    # Load existing records to determine starting point\n",
        "    all_records = []\n",
        "    if os.path.exists(RECORDS_PATH):\n",
        "        try:\n",
        "            with open(RECORDS_PATH, 'r') as f: all_records = json.load(f)\n",
        "            processed_questions = {r[\"question\"] for r in all_records}\n",
        "            dataset = [q for q in dataset if q[\"question\"] not in processed_questions]\n",
        "            print(f\"Found {len(all_records)} existing records, {len(dataset)} questions remaining\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing records: {e}\")\n",
        "\n",
        "    if os.path.exists(LINUCB_MODEL_PATH):\n",
        "        linucb_model.load_model_state(LINUCB_MODEL_PATH)\n",
        "\n",
        "    train_size = int(0.2 * len(dataset))\n",
        "    print(f\"Train size: {train_size}, Test size: {len(dataset) - train_size}\")\n",
        "    training_total_cost, training_question_count, avg_train_cost = 0.0, 0, 0.0\n",
        "\n",
        "    try:\n",
        "        for idx, question in enumerate(dataset):\n",
        "            is_test_phase = idx >= train_size\n",
        "            # Calculate budget (only in test phase)\n",
        "            current_question_budget = float('inf')\n",
        "            if is_test_phase:\n",
        "                # Use average cost from training phase as the budget for test questions\n",
        "                if training_question_count > 0 and avg_train_cost == 0.0:\n",
        "                    avg_train_cost = training_total_cost / training_question_count\n",
        "                current_question_budget = 0.00014217 # Fixed budget for reproducibility\n",
        "                print(f\"Test phase budget: ${current_question_budget:.8f}\")\n",
        "\n",
        "            print(f\"\\nProcessing question {idx+1}/{len(dataset)} ({'Test' if is_test_phase else 'Train'}): {question['question'][:80]}...\")\n",
        "            question_record = cascade.run_cascade_single_question(question, current_question_budget, is_test_phase)\n",
        "            all_records.append(question_record)\n",
        "\n",
        "            if not is_test_phase:\n",
        "                training_question_count += 1\n",
        "                training_total_cost += question_record[\"total_cost\"]\n",
        "\n",
        "            # Save records and states periodically\n",
        "            if (idx + 1) % UPDATE_FREQUENCY == 0 or idx == train_size - 1:\n",
        "                save_records_with_backup(all_records, RECORDS_PATH)\n",
        "                linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProcessing interrupted. Saving progress...\")\n",
        "    finally:\n",
        "        save_records_with_backup(all_records, RECORDS_PATH)\n",
        "        linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "        analyze_results(all_records)\n",
        "        print(\"LinUCB Cascade completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}