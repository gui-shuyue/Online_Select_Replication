{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import tiktoken\n",
        "import google.generativeai as genai\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from openai import OpenAI\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_FEATURE_DIM = 384  # Dimension for question embeddings\n",
        "ANSWER_EMBED_DIM = 384  # Dimension for answer embeddings\n",
        "CASCADE_LENGTH = 4      # Number of attempts in the cascade (K)\n",
        "UPDATE_FREQUENCY = 1    # Update JSON records after every question\n",
        "USE_EMBEDDINGS = True   # Use embeddings instead of TF-IDF\n",
        "\n",
        "# LinUCB parameters\n",
        "ALPHA = 0.675           # Exploration parameter for LinUCB\n",
        "LAMBDA_REG = 0.45       # Regularization parameter for LinUCB matrix initialization\n",
        "\n",
        "# Value Density & Cost Estimation parameters\n",
        "BETA_COST = 1.0         # Controls width of LCB interval for LLM cost estimation\n",
        "EPSILON_VD = 1e-10      # Small positive value for value density calculation\n",
        "DEFAULT_FALLBACK_COST = 0.01  # Default cost when insufficient data\n",
        "\n",
        "# Question Cost Estimation parameters\n",
        "BETA_COST_QUESTION = 1.0  # Controls width of LCB interval for question cost estimation\n",
        "DEFAULT_FALLBACK_COST_QUESTION = 0.04  # Default cost for questions when insufficient data\n",
        "QUESTION_POOL_MULTIPLIER = 3  # How many times BATCH_SIZE to consider for question selection\n",
        "\n",
        "# Pruning configuration\n",
        "MAX_WORDS = 250\n",
        "REPETITION_THRESHOLD = 3\n",
        "\n",
        "# Batch parameters\n",
        "BATCH_SIZE = 17"
      ],
      "metadata": {
        "id": "LgB_MxmPkCJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPENROUTER_API_KEY =  # Replace with your Openrouterkey\n",
        "OPENROUTER_BASE_URL =  # Replace with your openrouter URL"
      ],
      "metadata": {
        "id": "j3E-rqUtjhSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODELS_CONFIG = {\n",
        "    \"mistralai/mistral-small-3.1-24b-instruct\": {\"input_cost\": 0.05 / 1e6, \"output_cost\": 0.15 / 1e6},\n",
        "    \"microsoft/phi-4\": {\"input_cost\": 0.07 / 1e6, \"output_cost\": 0.14 / 1e6},\n",
        "    \"meta-llama/llama-4-maverick\": {\"input_cost\": 0.17 / 1e6, \"output_cost\": 0.16 / 1e6},\n",
        "    \"google/gemini-2.0-flash-001\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
        "    \"openai/gpt-4.1-nano\": {\"input_cost\": 0.1 / 1e6, \"output_cost\": 0.4 / 1e6},\n",
        "    \"deepseek/deepseek-chat\": {\"input_cost\": 0.38 / 1e6, \"output_cost\": 0.89 / 1e6},\n",
        "}\n",
        "\n",
        "GRADER_MODEL_NAME = \"google/gemini-2.0-flash-lite-001\"\n",
        "\n",
        "AVAILABLE_LLMS = list(MODELS_CONFIG.keys())\n",
        "LLM_ID_DIM = len(AVAILABLE_LLMS)\n",
        "\n",
        "# Feature dimensions are recalculated based on the number of available LLMs\n",
        "CONTEXT_FEATURE_DIM = ANSWER_EMBED_DIM + LLM_ID_DIM + ANSWER_EMBED_DIM\n",
        "TOTAL_FEATURE_DIM = BASE_FEATURE_DIM + 1 + CONTEXT_FEATURE_DIM\n",
        "\n",
        "# Initialize OpenRouter client\n",
        "openrouter_client = OpenAI(\n",
        "    base_url=OPENROUTER_BASE_URL,\n",
        "    api_key=OPENROUTER_API_KEY,\n",
        ")\n",
        "\n",
        "# File paths\n",
        "INPUT_JSON = \"Math500.json\"\n",
        "RECORDS_PATH = \"M93.json\"\n",
        "LINUCB_MODEL_PATH = \"M93.npz\"\n",
        "SUMMARY_STATS_PATH = \"M93.txt\"\n",
        "QUESTION_COST_PATH = \"Question_Cost_Estimator.npz\"\n",
        "BACKUP_SUFFIX = \".bak\"\n",
        "\n",
        "def solve_0_1_knapsack(items, capacity):\n",
        "    \"\"\"\n",
        "    Solves the 0/1 knapsack problem to find the optimal subset of items.\n",
        "    This is used to select a value-maximizing subset of LLMs that fits within a budget.\n",
        "    - items: List of dictionaries, each with 'name', 'value' (UCB score), and 'weight' (LCB cost).\n",
        "    - capacity: Maximum total weight (budget) the knapsack can hold.\n",
        "    \"\"\"\n",
        "    if not items or capacity <= 0:\n",
        "        return []\n",
        "    n = len(items)\n",
        "    # Scale weights by 10000 to handle floating point weights as integers for DP table\n",
        "    scaled_capacity = int(capacity * 10000)\n",
        "    scaled_weights = [int(item['weight'] * 10000) for item in items]\n",
        "    dp = [[0 for _ in range(scaled_capacity + 1)] for _ in range(n + 1)]\n",
        "\n",
        "    for i in range(1, n + 1):\n",
        "        for w in range(scaled_capacity + 1):\n",
        "            if scaled_weights[i-1] <= w:\n",
        "                dp[i][w] = max(dp[i-1][w], items[i-1]['value'] + dp[i-1][w - scaled_weights[i-1]])\n",
        "            else:\n",
        "                dp[i][w] = dp[i-1][w]\n",
        "\n",
        "    # Backtrack to find which items were selected\n",
        "    selected_items = []\n",
        "    w = scaled_capacity\n",
        "    for i in range(n, 0, -1):\n",
        "        if dp[i][w] != dp[i-1][w]:\n",
        "            selected_items.append(items[i-1]['name'])\n",
        "            w -= scaled_weights[i-1]\n",
        "    return selected_items\n",
        "\n",
        "\n",
        "class QuestionCostEstimator:\n",
        "    def __init__(self):\n",
        "        self.stats = {\"count\": 0, \"total_cost\": 0.0, \"sum_sq_cost\": 0.0}\n",
        "\n",
        "    def update_stats(self, actual_question_cost):\n",
        "        self.stats[\"count\"] += 1\n",
        "        self.stats[\"total_cost\"] += actual_question_cost\n",
        "        self.stats[\"sum_sq_cost\"] += actual_question_cost ** 2\n",
        "\n",
        "    def estimate_question_lcb_cost(self, beta=BETA_COST_QUESTION, default_cost=DEFAULT_FALLBACK_COST_QUESTION):\n",
        "        \"\"\"Estimate the Lower Confidence Bound (LCB) of cost for a question.\"\"\"\n",
        "        if self.stats[\"count\"] >= 2:\n",
        "            mean_cost = self.stats[\"total_cost\"] / self.stats[\"count\"]\n",
        "            variance = (self.stats[\"sum_sq_cost\"] / self.stats[\"count\"]) - (mean_cost ** 2)\n",
        "            std_dev = math.sqrt(max(0, variance))\n",
        "            std_error = std_dev / math.sqrt(self.stats[\"count\"])\n",
        "            lcb_cost = mean_cost - beta * std_error\n",
        "            return max(EPSILON_VD / 10, lcb_cost)\n",
        "        return default_cost\n",
        "\n",
        "    def save_state(self, file_path):\n",
        "        try:\n",
        "            np.savez_compressed(file_path, **self.stats)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving question cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_state(self, file_path):\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "            self.stats[\"count\"] = int(loaded[\"count\"])\n",
        "            self.stats[\"total_cost\"] = float(loaded[\"total_cost\"])\n",
        "            self.stats[\"sum_sq_cost\"] = float(loaded[\"sum_sq_cost\"])\n",
        "            print(f\"Loaded question cost estimator state from {file_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading question cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "class ModelCostEstimator:\n",
        "    def __init__(self, model_names):\n",
        "        self.stats = {}\n",
        "        for model_name in model_names:\n",
        "            self.stats[model_name] = {\n",
        "                \"overall\": {\"count\": 0, \"total_cost\": 0.0, \"sum_sq_cost\": 0.0},\n",
        "                \"by_position\": {\n",
        "                    str(i): {\"count\": 0, \"total_cost\": 0.0, \"sum_sq_cost\": 0.0}\n",
        "                    for i in range(1, CASCADE_LENGTH + 1)\n",
        "                }\n",
        "            }\n",
        "\n",
        "    def update_stats(self, model_name, step, actual_cost):\n",
        "        step_str = str(step)\n",
        "        if model_name in self.stats:\n",
        "            # Update position-specific stats\n",
        "            if step_str in self.stats[model_name][\"by_position\"]:\n",
        "                pos_stats = self.stats[model_name][\"by_position\"][step_str]\n",
        "                pos_stats[\"count\"] += 1\n",
        "                pos_stats[\"total_cost\"] += actual_cost\n",
        "                pos_stats[\"sum_sq_cost\"] += actual_cost ** 2\n",
        "            # Update overall stats\n",
        "            overall_stats = self.stats[model_name][\"overall\"]\n",
        "            overall_stats[\"count\"] += 1\n",
        "            overall_stats[\"total_cost\"] += actual_cost\n",
        "            overall_stats[\"sum_sq_cost\"] += actual_cost ** 2\n",
        "\n",
        "    def estimate_lcb_cost(self, model_name, step, beta=BETA_COST, default_cost=DEFAULT_FALLBACK_COST):\n",
        "        \"\"\"Estimate the Lower Confidence Bound (LCB) of cost for a model at a specific step.\"\"\"\n",
        "        step_str = str(step)\n",
        "        # Try position-specific stats first\n",
        "        if model_name in self.stats and step_str in self.stats[model_name][\"by_position\"]:\n",
        "            pos_stats = self.stats[model_name][\"by_position\"][step_str]\n",
        "            if pos_stats[\"count\"] >= 2:\n",
        "                mean_cost = pos_stats[\"total_cost\"] / pos_stats[\"count\"]\n",
        "                variance = (pos_stats[\"sum_sq_cost\"] / pos_stats[\"count\"]) - (mean_cost ** 2)\n",
        "                std_dev = math.sqrt(max(0, variance))\n",
        "                std_error = std_dev / math.sqrt(pos_stats[\"count\"])\n",
        "                return max(EPSILON_VD / 10, mean_cost - beta * std_error)\n",
        "        # Fallback to overall stats\n",
        "        if model_name in self.stats:\n",
        "            overall_stats = self.stats[model_name][\"overall\"]\n",
        "            if overall_stats[\"count\"] >= 2:\n",
        "                mean_cost = overall_stats[\"total_cost\"] / overall_stats[\"count\"]\n",
        "                variance = (overall_stats[\"sum_sq_cost\"] / overall_stats[\"count\"]) - (mean_cost ** 2)\n",
        "                std_dev = math.sqrt(max(0, variance))\n",
        "                std_error = std_dev / math.sqrt(overall_stats[\"count\"])\n",
        "                return max(EPSILON_VD / 10, mean_cost - beta * std_error)\n",
        "        return default_cost\n",
        "\n",
        "    def save_state(self, file_path):\n",
        "        try:\n",
        "            save_dict = {}\n",
        "            for model_name, model_stats in self.stats.items():\n",
        "                overall = model_stats[\"overall\"]\n",
        "                save_dict[f\"overall_count_{model_name}\"] = overall[\"count\"]\n",
        "                save_dict[f\"overall_total_cost_{model_name}\"] = overall[\"total_cost\"]\n",
        "                save_dict[f\"overall_sum_sq_cost_{model_name}\"] = overall[\"sum_sq_cost\"]\n",
        "                for pos, pos_stats in model_stats[\"by_position\"].items():\n",
        "                    save_dict[f\"pos{pos}_count_{model_name}\"] = pos_stats[\"count\"]\n",
        "                    save_dict[f\"pos{pos}_total_cost_{model_name}\"] = pos_stats[\"total_cost\"]\n",
        "                    save_dict[f\"pos{pos}_sum_sq_cost_{model_name}\"] = pos_stats[\"sum_sq_cost\"]\n",
        "            np.savez_compressed(file_path, **save_dict)\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving model cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_state(self, file_path):\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "            for model_name in self.stats:\n",
        "                if f\"overall_count_{model_name}\" in loaded:\n",
        "                    overall = self.stats[model_name][\"overall\"]\n",
        "                    overall[\"count\"] = int(loaded[f\"overall_count_{model_name}\"])\n",
        "                    overall[\"total_cost\"] = float(loaded[f\"overall_total_cost_{model_name}\"])\n",
        "                    overall[\"sum_sq_cost\"] = float(loaded[f\"overall_sum_sq_cost_{model_name}\"])\n",
        "                for pos in self.stats[model_name][\"by_position\"]:\n",
        "                    if f\"pos{pos}_count_{model_name}\" in loaded:\n",
        "                        pos_stats = self.stats[model_name][\"by_position\"][pos]\n",
        "                        pos_stats[\"count\"] = int(loaded[f\"pos{pos}_count_{model_name}\"])\n",
        "                        pos_stats[\"total_cost\"] = float(loaded[f\"pos{pos}_total_cost_{model_name}\"])\n",
        "                        pos_stats[\"sum_sq_cost\"] = float(loaded[f\"pos{pos}_sum_sq_cost_{model_name}\"])\n",
        "            print(f\"Loaded model cost estimator state from {file_path}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model cost estimator state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, feature_dim=BASE_FEATURE_DIM, use_embeddings=USE_EMBEDDINGS):\n",
        "        self.feature_dim = feature_dim\n",
        "        self.use_embeddings = use_embeddings\n",
        "        if use_embeddings:\n",
        "            try:\n",
        "                self.embedding_model = SentenceTransformer('BAAI/bge-small-en-v1.5')\n",
        "                print(\"Initialized sentence transformer embedding model\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error initializing sentence transformer: {e}\")\n",
        "                print(\"Falling back to TF-IDF vectorization\")\n",
        "                self.use_embeddings = False\n",
        "        if not self.use_embeddings:\n",
        "            # Initialize TF-IDF vectorizer without restricting features yet\n",
        "            self.vectorizer = TfidfVectorizer()\n",
        "            # We'll use SVD for dimensionality reduction later\n",
        "            self.svd = None\n",
        "\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, questions):\n",
        "        \"\"\"Initialize the vectorizer with the corpus of questions\"\"\"\n",
        "        if not self.use_embeddings:\n",
        "            # Combine question text and options for each question\n",
        "            all_text = [q[\"problem\"] for q in questions] # Use \"problem\" field, no \"options\"\n",
        "\n",
        "\n",
        "            # Fit the vectorizer on all training texts\n",
        "            self.vectorizer.fit(all_text)\n",
        "\n",
        "            # Get the document-term matrix for the entire corpus\n",
        "            dtm = self.vectorizer.transform(all_text)\n",
        "\n",
        "            # Use SVD for dimensionality reduction to get feature_dim dense features\n",
        "            from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "            # Determine the appropriate dimension to reduce to (min of feature_dim and actual feature count)\n",
        "            n_components = min(self.feature_dim, dtm.shape[1])\n",
        "\n",
        "            self.svd = TruncatedSVD(n_components=n_components)\n",
        "            self.svd.fit(dtm)\n",
        "\n",
        "            print(f\"Using TF-IDF with SVD dimensionality reduction to {n_components} features\")\n",
        "            print(f\"Explained variance ratio: {sum(self.svd.explained_variance_ratio_):.4f}\")\n",
        "\n",
        "        self.initialized = True\n",
        "\n",
        "    def extract_features(self, question):\n",
        "        \"\"\"Extract features from a question and its options\"\"\"\n",
        "        if not self.initialized:\n",
        "            raise ValueError(\"Feature extractor not initialized. Call initialize() first.\")\n",
        "\n",
        "        text = question[\"problem\"] # Use \"problem\" field, no \"options\"\n",
        "\n",
        "        if self.use_embeddings:\n",
        "            # Use sentence transformer for embeddings\n",
        "            features = self.embedding_model.encode([text])[0]\n",
        "        else:\n",
        "            # First get TF-IDF vector\n",
        "            tfidf_vector = self.vectorizer.transform([text])\n",
        "            # Then apply SVD transformation\n",
        "            features = self.svd.transform(tfidf_vector)[0]\n",
        "            # Ensure we have exactly feature_dim dimensions\n",
        "            if len(features) < self.feature_dim:\n",
        "                padding = np.zeros(self.feature_dim - len(features))\n",
        "                features = np.concatenate([features, padding])\n",
        "        return features\n",
        "\n",
        "    def extract_answer_features(self, answer_text):\n",
        "        \"\"\"Extract features from an answer string\"\"\"\n",
        "        if not answer_text:\n",
        "            return np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "        if self.use_embeddings:\n",
        "            try:\n",
        "                # Use sentence transformer for embeddings\n",
        "                features = self.embedding_model.encode([answer_text])[0]\n",
        "\n",
        "                # Handle if the embedding doesn't match the expected dimension\n",
        "                if len(features) != ANSWER_EMBED_DIM:\n",
        "                    if len(features) > ANSWER_EMBED_DIM:\n",
        "                        features = features[:ANSWER_EMBED_DIM]\n",
        "                    else:\n",
        "                        padding = np.zeros(ANSWER_EMBED_DIM - len(features))\n",
        "                        features = np.concatenate([features, padding])\n",
        "\n",
        "                return features\n",
        "            except Exception as e:\n",
        "                print(f\"Error embedding answer: {e}\")\n",
        "                return np.zeros(ANSWER_EMBED_DIM)\n",
        "        else:\n",
        "            # For simplicity, use zero vector if not using embeddings\n",
        "            return np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "    def construct_feature_vector(self, base_features, step_i, failed_answers, failed_llm_ids, model_name_to_index):\n",
        "        \"\"\"\n",
        "        Construct the augmented feature vector for LinUCB\n",
        "\n",
        "        Parameters:\n",
        "        - base_features: Features extracted from the question\n",
        "        - step_i: Current step in the cascade (1-indexed)\n",
        "        - failed_answers: List of previous failed answer strings\n",
        "        - failed_llm_ids: List of previous failed LLM IDs\n",
        "        - model_name_to_index: Mapping from model names to indices\n",
        "\n",
        "        Returns:\n",
        "        - Augmented feature vector\n",
        "        \"\"\"\n",
        "        # Normalize the step (1-indexed -> [0,1] range)\n",
        "        normalized_step = np.array([step_i / CASCADE_LENGTH])\n",
        "\n",
        "        # Initialize context features - last answer embedding\n",
        "        if step_i == 1 or not failed_answers:\n",
        "            # No prior context for the first step\n",
        "            last_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "        else:\n",
        "            # Extract features from the most recent failed answer\n",
        "            last_answer = failed_answers[-1]\n",
        "            last_answer_features = self.extract_answer_features(last_answer)\n",
        "\n",
        "        # Calculate Last Failed LLM ID One-Hot encoding\n",
        "        last_llm_onehot = np.zeros(LLM_ID_DIM)\n",
        "        if step_i > 1 and failed_llm_ids:\n",
        "            last_llm_name = failed_llm_ids[-1]\n",
        "            if last_llm_name in model_name_to_index:\n",
        "                last_llm_index = model_name_to_index[last_llm_name]\n",
        "                last_llm_onehot[last_llm_index] = 1.0\n",
        "\n",
        "        # Calculate Average Failed Answer Embedding\n",
        "        avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "        if step_i > 1 and failed_answers:\n",
        "            all_answer_features = [self.extract_answer_features(ans) for ans in failed_answers]\n",
        "            if all_answer_features:\n",
        "                avg_answer_features = np.mean(all_answer_features, axis=0)\n",
        "\n",
        "        # Handle case where avg_answer_features might be a scalar\n",
        "        if avg_answer_features.shape == ():\n",
        "            avg_answer_features = np.zeros(ANSWER_EMBED_DIM)\n",
        "\n",
        "        # Concatenate all context features\n",
        "        context_features = np.concatenate([\n",
        "            last_answer_features,    # Dim: ANSWER_EMBED_DIM\n",
        "            last_llm_onehot,         # Dim: LLM_ID_DIM\n",
        "            avg_answer_features      # Dim: ANSWER_EMBED_DIM\n",
        "        ])\n",
        "\n",
        "        # Final feature vector\n",
        "        augmented_features = np.concatenate([\n",
        "            base_features,           # Dim: BASE_FEATURE_DIM\n",
        "            normalized_step,         # Dim: 1\n",
        "            context_features         # Dim: CONTEXT_FEATURE_DIM\n",
        "        ])\n",
        "\n",
        "        # Ensure the final dimension matches expectations\n",
        "        if augmented_features.shape[0] != TOTAL_FEATURE_DIM:\n",
        "            # This should not happen if dimensions are calculated correctly\n",
        "            raise ValueError(f\"Constructed feature vector dimension {augmented_features.shape[0]} != expected {TOTAL_FEATURE_DIM}\")\n",
        "\n",
        "        return augmented_features\n",
        "\n",
        "\n",
        "class LinUCBModel:\n",
        "    def __init__(self, model_names, feature_dim=TOTAL_FEATURE_DIM, alpha=ALPHA,\n",
        "                 lambda_reg=LAMBDA_REG, beta_cost=BETA_COST,\n",
        "                 default_fallback_cost=DEFAULT_FALLBACK_COST):\n",
        "        self.model_names = model_names\n",
        "        self.feature_dim = feature_dim\n",
        "        self.alpha = alpha\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.beta_cost = beta_cost\n",
        "        self.default_fallback_cost = default_fallback_cost\n",
        "\n",
        "        self.model_name_to_index = {name: i for i, name in enumerate(model_names)}\n",
        "\n",
        "        # Initialize cost estimator\n",
        "        self.cost_estimator = ModelCostEstimator(model_names)\n",
        "\n",
        "        # Initialize model parameters\n",
        "        self.models = {}\n",
        "        for model_name in model_names:\n",
        "            self.models[model_name] = {\n",
        "                'A': np.identity(feature_dim) * lambda_reg,\n",
        "                'b': np.zeros(feature_dim),\n",
        "                'last_call_time': 0\n",
        "            }\n",
        "\n",
        "    def update_reward_only(self, model_name, feature_vector, reward):\n",
        "        \"\"\"\n",
        "        Update the LinUCB model parameters based on observed reward\n",
        "        Parameters:\n",
        "        - model_name: Name of the model to update\n",
        "        - feature_vector: Feature vector used for selection\n",
        "        - reward: Binary reward (1 for success, 0 for failure)\n",
        "        \"\"\"\n",
        "        model = self.models[model_name]\n",
        "        # Update A matrix\n",
        "        model['A'] += np.outer(feature_vector, feature_vector)\n",
        "        # Update b vector\n",
        "        model['b'] += feature_vector * reward\n",
        "\n",
        "    def get_predicted_reward(self, model_name, feature_vector):\n",
        "        \"\"\"\n",
        "        Get just the predicted reward (p_ia) for a given model and feature vector\n",
        "\n",
        "        Parameters:\n",
        "        - model_name: Name of the model\n",
        "        - feature_vector: Feature vector for prediction\n",
        "\n",
        "        Returns:\n",
        "        - Predicted reward (p_ia)\n",
        "        \"\"\"\n",
        "        model = self.models[model_name]\n",
        "        try:\n",
        "            # Calculate theta using the model's A matrix and b vector\n",
        "            theta = np.linalg.solve(model['A'], model['b'])\n",
        "            # Calculate predicted reward\n",
        "            p_ia = feature_vector.dot(theta)\n",
        "            return float(p_ia)\n",
        "        except np.linalg.LinAlgError:\n",
        "            try:\n",
        "                # Alternative way to calculate theta if direct solve fails\n",
        "                A_inv = np.linalg.inv(model['A'])\n",
        "                theta = A_inv.dot(model['b'])\n",
        "                p_ia = feature_vector.dot(theta)\n",
        "                return float(p_ia)\n",
        "            except:\n",
        "                # Return 0.0 if calculation completely fails\n",
        "                return 0.0\n",
        "\n",
        "    def calculate_ucb_scores(self, feature_vector):\n",
        "        \"\"\"\n",
        "        Calculate UCB scores for model selection\n",
        "\n",
        "        Parameters:\n",
        "        - feature_vector: Feature vector used for model selection\n",
        "\n",
        "        Returns a dictionary with scores for each model\n",
        "        \"\"\"\n",
        "        scores = {}\n",
        "\n",
        "        for model_name in self.model_names:\n",
        "            model = self.models[model_name]\n",
        "            try:\n",
        "                L = np.linalg.cholesky(model['A'])\n",
        "                theta = np.linalg.solve(model['A'], model['b'])\n",
        "                z = np.linalg.solve(L, feature_vector)\n",
        "                ucb_term = self.alpha * np.sqrt(np.sum(z**2))\n",
        "                expected_reward = feature_vector.dot(theta)\n",
        "                ucb_score = expected_reward + ucb_term\n",
        "                scores[model_name] = {\n",
        "                    \"p_ia\": float(expected_reward),\n",
        "                    \"e_ia\": float(ucb_term),\n",
        "                    \"ucb_score\": float(ucb_score)\n",
        "                }\n",
        "            except np.linalg.LinAlgError:\n",
        "                try:\n",
        "                    A_inv = np.linalg.inv(model['A'])\n",
        "                    theta = A_inv.dot(model['b'])\n",
        "                    ucb_term = self.alpha * np.sqrt(feature_vector.dot(A_inv).dot(feature_vector))\n",
        "                    expected_reward = feature_vector.dot(theta)\n",
        "                    ucb_score = expected_reward + ucb_term\n",
        "                    scores[model_name] = {\n",
        "                        \"p_ia\": float(expected_reward),\n",
        "                        \"e_ia\": float(ucb_term),\n",
        "                        \"ucb_score\": float(ucb_score)\n",
        "                    }\n",
        "                except:\n",
        "                    scores[model_name] = {\n",
        "                        \"p_ia\": 0.0,\n",
        "                        \"e_ia\": 0.0,\n",
        "                        \"ucb_score\": 0.0\n",
        "                    }\n",
        "        return scores\n",
        "\n",
        "    def register_model_call(self, model_name):\n",
        "        \"\"\"Register that a model was called and update its last call time\"\"\"\n",
        "        self.models[model_name]['last_call_time'] = time.time()\n",
        "    def respect_rate_limit(self, model_name):\n",
        "        \"\"\"Wait if necessary to respect the model's rate limit.\"\"\"\n",
        "        # MODELS_CONFIG now refers to the OpenRouter models config, which doesn't have 'rpm'\n",
        "        model_cfg = MODELS_CONFIG.get(model_name)\n",
        "\n",
        "        # Only apply client-side RPM-based waiting if 'rpm' is defined for the model.\n",
        "        # For the specified OpenRouter models, 'rpm' is not defined, so this block will be skipped.\n",
        "        if model_cfg and \"rpm\" in model_cfg:\n",
        "            model_state = self.models[model_name] # 'model' is a local var, 'self.models' holds states\n",
        "            rpm = model_cfg[\"rpm\"]\n",
        "\n",
        "            min_seconds_between_calls = 60.0 / rpm\n",
        "\n",
        "            time_since_last_call = time.time() - model_state['last_call_time']\n",
        "            if time_since_last_call < min_seconds_between_calls:\n",
        "                sleep_time = min_seconds_between_calls - time_since_last_call\n",
        "                time.sleep(sleep_time)\n",
        "    def save_model_state(self, file_path):\n",
        "        \"\"\"Save the model state to a file using numpy's compressed format\"\"\"\n",
        "        save_dict = {}\n",
        "        for model_name, model in self.models.items():\n",
        "            save_dict[f'A_{model_name}'] = model['A']\n",
        "            save_dict[f'b_{model_name}'] = model['b']\n",
        "\n",
        "        # Save to compressed numpy format\n",
        "        np.savez_compressed(file_path, **save_dict)\n",
        "\n",
        "        # Save cost estimator state\n",
        "        cost_estimator_path = file_path.replace('.npz', '_cost_estimator.npz')\n",
        "        self.cost_estimator.save_state(cost_estimator_path)\n",
        "\n",
        "    def load_model_state(self, file_path):\n",
        "        \"\"\"Load the model state from a file\"\"\"\n",
        "        try:\n",
        "            loaded = np.load(file_path)\n",
        "\n",
        "            for model_name in self.models.keys():\n",
        "                if f'A_{model_name}' in loaded and f'b_{model_name}' in loaded:\n",
        "                    self.models[model_name]['A'] = loaded[f'A_{model_name}']\n",
        "                    self.models[model_name]['b'] = loaded[f'b_{model_name}']\n",
        "\n",
        "            print(f\"Loaded LinUCB model state from {file_path}\")\n",
        "\n",
        "            # Load cost estimator state\n",
        "            cost_estimator_path = file_path.replace('.npz', '_cost_estimator.npz')\n",
        "            if os.path.exists(cost_estimator_path):\n",
        "                self.cost_estimator.load_state(cost_estimator_path)\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model state: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "\n",
        "class BatchBudgetCascade:\n",
        "    def __init__(self, feature_extractor, linucb_model, cascade_length=CASCADE_LENGTH):\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.linucb_model = linucb_model\n",
        "        self.cascade_length = cascade_length\n",
        "        self.used_models_for_this_question = []\n",
        "\n",
        "    def grade_with_gemma12b(self, llm_raw_answer, ground_truth_latex):\n",
        "        \"\"\"Grade an answer against the ground truth using the grader model via OpenRouter.\"\"\"\n",
        "        if not llm_raw_answer or not llm_raw_answer.strip():\n",
        "            return False\n",
        "        if llm_raw_answer == ground_truth_latex:\n",
        "            return True\n",
        "\n",
        "        prompt = f\"Expression 1: {llm_raw_answer}\\nExpression 2: {ground_truth_latex}\\n\\n\"\n",
        "        prompt += \"Expression 2 is the answer and expression is attempt by student, look at their final answer only which might be boxed, does student get the final expected answerï¼Ÿ Respond with only the word 'True' or 'False'.\"\n",
        "        try:\n",
        "            time.sleep(0.5) # Simple rate limiting for the grader\n",
        "            api_response = openrouter_client.chat.completions.create(\n",
        "                model=GRADER_MODEL_NAME,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0, max_tokens=10\n",
        "            )\n",
        "            grader_response_text = api_response.choices[0].message.content.strip().lower()\n",
        "            if \"true\" in grader_response_text: return True\n",
        "            elif \"false\" in grader_response_text: return False\n",
        "            else:\n",
        "                print(f\"Warning: Grader returned ambiguous response: {grader_response_text}\")\n",
        "                return False\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling grader model {GRADER_MODEL_NAME} via OpenRouter: {e}\")\n",
        "            return False\n",
        "\n",
        "    def format_prompt(self, question, failed_answers=None, failed_llm_ids=None):\n",
        "        prompt = f\"Solve the following math problem: {question['problem']}\\n\\n\"\n",
        "        prompt += \"Also provide an explnation in one/serveral very short yet consice complete sentence within 75 words in total.\\n\"\n",
        "        prompt += \"At the end, clearly state your final answer in LaTeX format, enclosed within \\\\boxed{}.\\n\"\n",
        "        prompt += \"For example: 'The final answer is \\\\boxed{x=5}'.\"\n",
        "        if failed_answers and failed_llm_ids:\n",
        "            prompt += \"\\nNote: The following previous attempts were incorrect. Please provide a different solution:\\n\"\n",
        "            for i in range(min(len(failed_answers), len(failed_llm_ids))):\n",
        "                prompt += f\"- Attempt {i+1} (by {failed_llm_ids[i]}) led to: {failed_answers[i]}\\n\"\n",
        "        return prompt\n",
        "\n",
        "    def calculate_token_cost(self, model_name, prompt, response_text, usage_info=None):\n",
        "        \"\"\"Calculate the actual cost. For OpenRouter, relies on usage_info from API response.\"\"\"\n",
        "        total_cost = 0.0\n",
        "        error_message = None\n",
        "        if model_name in MODELS_CONFIG:\n",
        "            model_cfg = MODELS_CONFIG[model_name]\n",
        "            if usage_info:\n",
        "                input_tokens = usage_info.get(\"prompt_tokens\", 0)\n",
        "                output_tokens = usage_info.get(\"completion_tokens\", 0)\n",
        "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
        "            else:\n",
        "                error_message = f\"Usage info not available for {model_name}. Cost is a rough estimate.\"\n",
        "                input_tokens = len(prompt) // 4\n",
        "                output_tokens = len(response_text) // 4 if response_text else 0\n",
        "                total_cost = (input_tokens * model_cfg[\"input_cost\"]) + (output_tokens * model_cfg[\"output_cost\"])\n",
        "        else:\n",
        "            error_message = f\"Model {model_name} not found in config for cost calculation.\"\n",
        "        result = {\"total_cost\": total_cost}\n",
        "        if error_message:\n",
        "            result[\"error\"] = error_message\n",
        "            print(f\"Cost calculation warning for {model_name}: {error_message}\")\n",
        "        return result\n",
        "\n",
        "    def query_llm(self, model_name, prompt):\n",
        "        \"\"\"Query the specified LLM and return its response and cost.\"\"\"\n",
        "        answer_text, cost_data = \"\", {}\n",
        "        try:\n",
        "            self.linucb_model.respect_rate_limit(model_name)\n",
        "            if model_name in MODELS_CONFIG:\n",
        "                api_response = openrouter_client.chat.completions.create(\n",
        "                    model=model_name,\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                )\n",
        "                answer_text = api_response.choices[0].message.content.strip()\n",
        "                usage_info = api_response.usage.model_dump() if api_response.usage else None\n",
        "                cost_data = self.calculate_token_cost(model_name, prompt, answer_text, usage_info=usage_info)\n",
        "            else:\n",
        "                raise ValueError(f\"Model {model_name} is not configured in MODELS_CONFIG.\")\n",
        "            self.linucb_model.register_model_call(model_name)\n",
        "            return answer_text, answer_text, cost_data # Returns (raw_response, parsed_answer, cost_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying LLM {model_name}: {e}\")\n",
        "            self.linucb_model.register_model_call(model_name)\n",
        "            cost_data = self.calculate_token_cost(model_name, prompt, \"\", usage_info=None)\n",
        "            return \"\", None, cost_data\n",
        "\n",
        "    def run_cascade_single_question(self, question, question_cost_estimator, is_training=False, initial_question_budget=None):\n",
        "        \"\"\"\n",
        "        Run the cascade with knapsack-based model selection for a single question.\n",
        "        - is_training: If True, use infinite budget and no LLM reselection.\n",
        "        - initial_question_budget: Budget for this question (used in testing).\n",
        "        \"\"\"\n",
        "        base_features = self.feature_extractor.extract_features(question)\n",
        "        failed_answers, failed_llm_ids, self.used_models_for_this_question = [], [], []\n",
        "        question_total_cost, current_attempts_log = 0.0, []\n",
        "        final_status = \"Failure\"\n",
        "\n",
        "        # Set budget based on training/testing mode\n",
        "        if is_training:\n",
        "            current_remaining_budget = float('inf')\n",
        "            print(f\"Training Mode - Unlimited Budget\")\n",
        "        else:\n",
        "            current_remaining_budget = initial_question_budget if initial_question_budget is not None else question_cost_estimator.estimate_question_lcb_cost()\n",
        "            print(f\"Test Mode - Initial Budget: ${current_remaining_budget:.8f}\")\n",
        "\n",
        "        # Knapsack-based Cascade Loop\n",
        "        for i in range(1, self.cascade_length + 1):\n",
        "            print(f\"Step {i}\")\n",
        "            if not is_training and current_remaining_budget <= EPSILON_VD:\n",
        "                print(f\"  Insufficient budget remaining (${current_remaining_budget:.8f}). Aborting cascade.\")\n",
        "                break\n",
        "\n",
        "            x_i = self.feature_extractor.construct_feature_vector(base_features, i, failed_answers, failed_llm_ids, self.linucb_model.model_name_to_index)\n",
        "            available_llms_for_step = [m for m in AVAILABLE_LLMS if m not in self.used_models_for_this_question] if is_training else list(AVAILABLE_LLMS)\n",
        "\n",
        "            # Prepare items for knapsack by estimating value (UCB) and weight (LCB cost)\n",
        "            knapsack_items, llm_info_for_step = [], {}\n",
        "            ucb_scores = self.linucb_model.calculate_ucb_scores(x_i)\n",
        "            knapsack_capacity = float('inf') if is_training else current_remaining_budget\n",
        "\n",
        "            for model_name in available_llms_for_step:\n",
        "                if model_name not in ucb_scores: continue\n",
        "                lcb_cost = self.linucb_model.cost_estimator.estimate_lcb_cost(model_name, i)\n",
        "                if is_training or lcb_cost <= knapsack_capacity:\n",
        "                    knapsack_items.append({\"name\": model_name, \"value\": ucb_scores[model_name][\"ucb_score\"], \"weight\": lcb_cost})\n",
        "                llm_info_for_step[model_name] = {\"ucb_score\": ucb_scores[model_name][\"ucb_score\"], \"lcb_cost\": lcb_cost}\n",
        "\n",
        "            if not knapsack_items:\n",
        "                print(\"  No models fit within the remaining budget for this step.\")\n",
        "                continue\n",
        "\n",
        "            # In training, select best UCB; in testing, use knapsack solver\n",
        "            if is_training:\n",
        "                chosen_llm_names = [max(knapsack_items, key=lambda x: x['value'])['name']]\n",
        "            else:\n",
        "                chosen_llm_names = solve_0_1_knapsack(knapsack_items, knapsack_capacity)\n",
        "\n",
        "            if not chosen_llm_names:\n",
        "                print(\"  Knapsack solution is empty. No combination fits within budget.\")\n",
        "                continue\n",
        "\n",
        "            # Execute chosen subset sequentially, ordered by UCB score\n",
        "            step_successful = False\n",
        "            chosen_llms_sorted = sorted(chosen_llm_names, key=lambda name: llm_info_for_step[name][\"ucb_score\"], reverse=True)\n",
        "            print(f\"  Attempting LLMs in UCB score order: {chosen_llms_sorted}\")\n",
        "\n",
        "            for model_name in chosen_llms_sorted:\n",
        "                lcb_cost = llm_info_for_step[model_name][\"lcb_cost\"]\n",
        "                if not is_training and current_remaining_budget < lcb_cost:\n",
        "                    print(f\"    Skipping {model_name}: Estimated cost (${lcb_cost:.8f}) exceeds remaining budget (${current_remaining_budget:.8f}).\")\n",
        "                    continue\n",
        "\n",
        "                prompt = self.format_prompt(question, failed_answers, failed_llm_ids)\n",
        "                raw_response, parsed_answer, cost_data = self.query_llm(model_name, prompt)\n",
        "                actual_cost = cost_data.get(\"total_cost\", 0.0)\n",
        "                question_total_cost += actual_cost\n",
        "                if not is_training:\n",
        "                    current_remaining_budget -= actual_cost\n",
        "\n",
        "                self.linucb_model.cost_estimator.update_stats(model_name, i, actual_cost)\n",
        "                is_correct = self.grade_with_gemma12b(raw_response, question['answer'])\n",
        "                reward = 1 if is_correct else 0\n",
        "                self.linucb_model.update_reward_only(model_name, x_i, reward)\n",
        "                self.used_models_for_this_question.append(model_name)\n",
        "                print(f\"    {model_name}: Correct={is_correct}, Cost=${actual_cost:.8f}\")\n",
        "\n",
        "                if is_correct:\n",
        "                    step_successful = True\n",
        "                    final_status = \"Success\"\n",
        "                    break # Success, break from inner sequential loop\n",
        "\n",
        "            current_attempts_log.append({\"step\": i, \"chosen_subset\": chosen_llm_names, \"is_successful\": step_successful})\n",
        "            if step_successful:\n",
        "                break # Success, break from outer cascade loop\n",
        "\n",
        "        question_cost_estimator.update_stats(question_total_cost)\n",
        "        return {\n",
        "            \"problem\": question[\"problem\"], \"ground_truth_answer\": question[\"answer\"],\n",
        "            \"final_status\": final_status, \"total_cost\": question_total_cost,\n",
        "            \"initial_question_budget\": initial_question_budget if not is_training else \"Unlimited\",\n",
        "            \"steps_taken\": len(current_attempts_log), \"knapsack_steps\": current_attempts_log,\n",
        "            \"training_mode\": is_training\n",
        "        }"
      ],
      "metadata": {
        "id": "Bha20TRxjgLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_dataset(json_path):\n",
        "    \"\"\"Load the MCQ dataset from a JSON file with robust error handling\"\"\"\n",
        "    try:\n",
        "        with open(json_path, 'r', encoding='utf-8') as f:\n",
        "            raw_data = json.load(f) # Load the raw JSON object\n",
        "\n",
        "        if \"test\" in raw_data and isinstance(raw_data[\"test\"], list):\n",
        "            data = raw_data[\"test\"] # Extract the list of questions from the \"test\" key\n",
        "        else:\n",
        "            print(f\"Warning: 'test' key not found or not a list in {json_path}. Assuming flat list structure or empty.\")\n",
        "            data = raw_data if isinstance(raw_data, list) else [] # Fallback or handle as error\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Dataset file {json_path} not found.\")\n",
        "        return []\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from {json_path}. File may be corrupted.\")\n",
        "        try:\n",
        "            with open(json_path, 'r') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Simple recovery attempt - find all complete JSON objects\n",
        "            import re\n",
        "            pattern = r'\\{[^{}]*\\}'\n",
        "            matches = re.findall(pattern, content)\n",
        "\n",
        "            if matches:\n",
        "                print(f\"Attempted to recover {len(matches)} JSON objects.\")\n",
        "                recovered_data = []\n",
        "                for match in matches:\n",
        "                    try:\n",
        "                        obj = json.loads(match)\n",
        "                        recovered_data.append(obj)\n",
        "                    except:\n",
        "                        pass\n",
        "                return recovered_data\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        return []\n",
        "\n",
        "def save_records_with_backup(records, json_path):\n",
        "    \"\"\"Save records to a JSON file with backup of previous file\"\"\"\n",
        "    # Create backup of existing file if it exists\n",
        "    if os.path.exists(json_path):\n",
        "        backup_path = json_path + BACKUP_SUFFIX\n",
        "        try:\n",
        "            os.replace(json_path, backup_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create backup: {e}\")\n",
        "\n",
        "    # Save new data\n",
        "    try:\n",
        "        with open(json_path, 'w') as f:\n",
        "            json.dump(records, f, indent=4)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving records: {e}\")\n",
        "\n",
        "        # Try to restore from backup if save failed\n",
        "        if os.path.exists(json_path + BACKUP_SUFFIX):\n",
        "            try:\n",
        "                os.replace(json_path + BACKUP_SUFFIX, json_path)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        return False\n",
        "\n",
        "def initialize_json_files():\n",
        "    \"\"\"Initialize the JSON files for records with validation\"\"\"\n",
        "    if not os.path.exists(RECORDS_PATH):\n",
        "        with open(RECORDS_PATH, 'w') as f:\n",
        "            json.dump([], f)  # Empty array\n",
        "    else:\n",
        "        # Validate existing file\n",
        "        try:\n",
        "            with open(RECORDS_PATH, 'r') as f:\n",
        "                data = json.load(f)\n",
        "            if not isinstance(data, list):\n",
        "                os.rename(RECORDS_PATH, RECORDS_PATH + BACKUP_SUFFIX)\n",
        "                with open(RECORDS_PATH, 'w') as f:\n",
        "                    json.dump([], f)\n",
        "        except json.JSONDecodeError:\n",
        "            os.rename(RECORDS_PATH, RECORDS_PATH + BACKUP_SUFFIX)\n",
        "            with open(RECORDS_PATH, 'w') as f:\n",
        "                json.dump([], f)\n",
        "\n",
        "def analyze_results(records_to_analyze, question_cost_estimator=None):\n",
        "    \"\"\"Analyze and print results from the records\"\"\"\n",
        "    if not records_to_analyze:\n",
        "        print(\"No records to analyze\")\n",
        "        return\n",
        "\n",
        "    # Extract data for analysis\n",
        "    total_questions = len(records_to_analyze)\n",
        "    successful_questions = sum(1 for r in records_to_analyze if r[\"final_status\"] == \"Success\")\n",
        "    success_rate = successful_questions / total_questions if total_questions > 0 else 0\n",
        "\n",
        "    # Calculate success by position (knapsack step)\n",
        "    successes_by_position = [0] * CASCADE_LENGTH\n",
        "    for record in records_to_analyze:\n",
        "        if record[\"final_status\"] == \"Success\":\n",
        "            # Find the successful step (step is 1-indexed)\n",
        "            for i, step in enumerate(record[\"knapsack_steps\"]):\n",
        "                if step[\"is_successful\"]:\n",
        "                    # Convert to 0-indexed for array\n",
        "                    position = i\n",
        "                    if position < CASCADE_LENGTH:\n",
        "                        successes_by_position[position] += 1\n",
        "                    break\n",
        "\n",
        "    per_position_success = [count/total_questions for count in successes_by_position]\n",
        "\n",
        "    # Calculate average steps and costs\n",
        "    total_steps = sum(r[\"steps_taken\"] for r in records_to_analyze)\n",
        "    total_cost = sum(r[\"total_cost\"] for r in records_to_analyze)\n",
        "\n",
        "    avg_steps = total_steps / total_questions if total_questions > 0 else 0\n",
        "    avg_cost = total_cost / total_questions if total_questions > 0 else 0\n",
        "\n",
        "    # Calculate average cost for successful questions only\n",
        "    if successful_questions > 0:\n",
        "        success_cost = sum(r[\"total_cost\"] for r in records_to_analyze if r[\"final_status\"] == \"Success\")\n",
        "        avg_cost_success = success_cost / successful_questions\n",
        "    else:\n",
        "        avg_cost_success = 0\n",
        "\n",
        "    # Analyze model performance - modified for knapsack approach\n",
        "    model_metrics = {model: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0, \"by_position\": defaultdict(lambda: {\"calls\": 0, \"successes\": 0, \"total_cost\": 0.0})}\n",
        "                    for model in AVAILABLE_LLMS}\n",
        "\n",
        "    # Track LCB cost estimation accuracy\n",
        "    lcb_cost_accuracy = {model: {\"total_lcb_estimate\": 0.0, \"total_actual_cost\": 0.0, \"count\": 0}\n",
        "                         for model in AVAILABLE_LLMS}\n",
        "\n",
        "    # Track knapsack metrics\n",
        "    knapsack_metrics = {\n",
        "        \"total_steps\": 0,\n",
        "        \"avg_models_per_step\": 0,\n",
        "        \"models_selected_count\": 0,\n",
        "        \"budget_utilization\": 0.0,\n",
        "        \"total_budget_allocated\": 0.0,\n",
        "        \"by_position\": defaultdict(lambda: {\"steps\": 0, \"models_selected\": 0, \"budget_allocated\": 0.0, \"actual_cost\": 0.0})\n",
        "    }\n",
        "\n",
        "    for record in records_to_analyze:\n",
        "        for step in record[\"knapsack_steps\"]:\n",
        "            position = step[\"step\"]  # 1-indexed\n",
        "            knapsack_metrics[\"total_steps\"] += 1\n",
        "            knapsack_metrics[\"models_selected_count\"] += len(step[\"chosen_subset_by_knapsack\"])\n",
        "\n",
        "            # Handle the case where step_budget can be \"Unlimited\" string\n",
        "            if step[\"knapsack_capacity_for_step\"] != \"Unlimited\":\n",
        "                step_budget = float(step[\"knapsack_capacity_for_step\"])\n",
        "                knapsack_metrics[\"total_budget_allocated\"] += step_budget\n",
        "\n",
        "                # Update position-specific budget stats\n",
        "                pos_stats = knapsack_metrics[\"by_position\"][position]\n",
        "                pos_stats[\"budget_allocated\"] += step_budget\n",
        "            else:\n",
        "                # Skip budget calculations for unlimited budget steps\n",
        "                step_budget = float('inf')\n",
        "\n",
        "            # Update position-specific knapsack stats\n",
        "            pos_stats = knapsack_metrics[\"by_position\"][position]\n",
        "            pos_stats[\"steps\"] += 1\n",
        "            pos_stats[\"models_selected\"] += len(step[\"chosen_subset_by_knapsack\"])\n",
        "            pos_stats[\"actual_cost\"] += step[\"actual_cost_for_step\"]\n",
        "\n",
        "            # Process individual LLM attempts\n",
        "            for attempt in step[\"llm_attempts\"]:\n",
        "                model = attempt[\"knapsack_model\"]\n",
        "                is_correct = attempt[\"is_correct\"]\n",
        "                cost = attempt[\"actual_cost\"]\n",
        "\n",
        "                # Track LCB cost estimation accuracy\n",
        "                if \"lcb_cost_estimate\" in attempt:\n",
        "                    lcb_cost_accuracy[model][\"total_lcb_estimate\"] += attempt[\"lcb_cost_estimate\"]\n",
        "                    lcb_cost_accuracy[model][\"total_actual_cost\"] += cost\n",
        "                    lcb_cost_accuracy[model][\"count\"] += 1\n",
        "\n",
        "                # Update overall model stats\n",
        "                model_metrics[model][\"calls\"] += 1\n",
        "                model_metrics[model][\"total_cost\"] += cost\n",
        "                if is_correct:\n",
        "                    model_metrics[model][\"successes\"] += 1\n",
        "\n",
        "                # Update position-specific stats\n",
        "                model_metrics[model][\"by_position\"][position][\"calls\"] += 1\n",
        "                model_metrics[model][\"by_position\"][position][\"total_cost\"] += cost\n",
        "                if is_correct:\n",
        "                    model_metrics[model][\"by_position\"][position][\"successes\"] += 1\n",
        "\n",
        "    # Calculate average models per step\n",
        "    if knapsack_metrics[\"total_steps\"] > 0:\n",
        "        knapsack_metrics[\"avg_models_per_step\"] = knapsack_metrics[\"models_selected_count\"] / knapsack_metrics[\"total_steps\"]\n",
        "        if knapsack_metrics[\"total_budget_allocated\"] > 0:\n",
        "            knapsack_metrics[\"budget_utilization\"] = sum(r[\"total_cost\"] for r in records_to_analyze) / knapsack_metrics[\"total_budget_allocated\"]\n",
        "\n",
        "    # Calculate success rates and average costs for models\n",
        "    for model, data in model_metrics.items():\n",
        "        if data[\"calls\"] > 0:\n",
        "            data[\"success_rate\"] = data[\"successes\"] / data[\"calls\"]\n",
        "            data[\"avg_cost\"] = data[\"total_cost\"] / data[\"calls\"]\n",
        "        else:\n",
        "            data[\"success_rate\"] = 0\n",
        "            data[\"avg_cost\"] = 0\n",
        "\n",
        "        for position, pos_data in data[\"by_position\"].items():\n",
        "            if pos_data[\"calls\"] > 0:\n",
        "                pos_data[\"success_rate\"] = pos_data[\"successes\"] / pos_data[\"calls\"]\n",
        "                pos_data[\"avg_cost\"] = pos_data[\"total_cost\"] / pos_data[\"calls\"]\n",
        "            else:\n",
        "                pos_data[\"success_rate\"] = 0\n",
        "                pos_data[\"avg_cost\"] = 0\n",
        "\n",
        "    # Calculate LCB estimation accuracy\n",
        "    for model, data in lcb_cost_accuracy.items():\n",
        "        if data[\"count\"] > 0:\n",
        "            data[\"avg_lcb_estimate\"] = data[\"total_lcb_estimate\"] / data[\"count\"]\n",
        "            data[\"avg_actual_cost\"] = data[\"total_actual_cost\"] / data[\"count\"]\n",
        "            data[\"lcb_accuracy\"] = data[\"avg_lcb_estimate\"] / data[\"avg_actual_cost\"] if data[\"avg_actual_cost\"] > 0 else 0\n",
        "        else:\n",
        "            data[\"avg_lcb_estimate\"] = 0\n",
        "            data[\"avg_actual_cost\"] = 0\n",
        "            data[\"lcb_accuracy\"] = 0\n",
        "\n",
        "    # Calculate position-specific knapsack metrics\n",
        "    for position, data in knapsack_metrics[\"by_position\"].items():\n",
        "        if data[\"steps\"] > 0:\n",
        "            data[\"avg_models_per_step\"] = data[\"models_selected\"] / data[\"steps\"]\n",
        "\n",
        "            if data[\"budget_allocated\"] > 0:\n",
        "                data[\"avg_budget\"] = data[\"budget_allocated\"] / data[\"steps\"]\n",
        "                data[\"budget_utilization\"] = data[\"actual_cost\"] / data[\"budget_allocated\"]\n",
        "            else:\n",
        "                data[\"avg_budget\"] = \"Unlimited\"\n",
        "                data[\"budget_utilization\"] = 0\n",
        "\n",
        "            data[\"avg_cost\"] = data[\"actual_cost\"] / data[\"steps\"]\n",
        "\n",
        "    # Analyze batch performance\n",
        "    batch_metrics = []\n",
        "    for i in range(0, len(records_to_analyze), BATCH_SIZE):\n",
        "        batch = records_to_analyze[i:i+BATCH_SIZE]\n",
        "        batch_success = sum(1 for r in batch if r[\"final_status\"] == \"Success\")\n",
        "        batch_success_rate = batch_success / len(batch) if batch else 0\n",
        "        batch_cost = sum(r[\"total_cost\"] for r in batch)\n",
        "        batch_avg_cost = batch_cost / len(batch) if batch else 0\n",
        "\n",
        "        batch_metrics.append({\n",
        "            \"batch_idx\": i // BATCH_SIZE,\n",
        "            \"batch_size\": len(batch),\n",
        "            \"success_count\": batch_success,\n",
        "            \"success_rate\": batch_success_rate,\n",
        "            \"total_cost\": batch_cost,\n",
        "            \"avg_cost\": batch_avg_cost\n",
        "        })\n",
        "\n",
        "    # Generate summary text\n",
        "    dataset_type = \"TRAIN\" if records_to_analyze and records_to_analyze[0].get(\"dataset\") == \"train\" else \"TEST\" if records_to_analyze and records_to_analyze[0].get(\"dataset\") == \"test\" else \"OVERALL\"\n",
        "    summary = f\"=== LinUCB CASCADE WITH KNAPSACK SELECTION - {dataset_type} SET RESULTS ===\\n\\n\"\n",
        "    summary += f\"Batch Size: {BATCH_SIZE}\\n\"\n",
        "    summary += f\"LLM Cost Beta: {BETA_COST}\\n\"\n",
        "    summary += f\"Question Cost Beta: {BETA_COST_QUESTION}\\n\"\n",
        "    summary += f\"Cascade Length (K): {CASCADE_LENGTH}\\n\\n\"\n",
        "\n",
        "    summary += \"=== OVERALL RESULTS ===\\n\"\n",
        "    summary += f\"Total Questions: {total_questions}\\n\"\n",
        "    summary += f\"Success Rate: {success_rate:.4f}\\n\"\n",
        "    summary += f\"Average Steps: {avg_steps:.4f}\\n\"\n",
        "    summary += f\"Average Cost per Question: ${avg_cost:.8f}\\n\"\n",
        "    summary += f\"Average Cost per Successful Question: ${avg_cost_success:.8f}\\n\"\n",
        "    summary += \"Success Rate by Position:\\n\"\n",
        "    for i, rate in enumerate(per_position_success):\n",
        "        summary += f\"  Position {i+1}: {rate:.4f}\\n\"\n",
        "\n",
        "    # Add question cost estimation info if available\n",
        "    if question_cost_estimator and question_cost_estimator.stats[\"count\"] > 0:\n",
        "        avg_question_cost = question_cost_estimator.stats[\"total_cost\"] / question_cost_estimator.stats[\"count\"]\n",
        "        estimated_lcb = question_cost_estimator.estimate_question_lcb_cost()\n",
        "\n",
        "        summary += \"\\n=== QUESTION COST ESTIMATION ===\\n\"\n",
        "        summary += f\"Total Questions Processed: {question_cost_estimator.stats['count']}\\n\"\n",
        "        summary += f\"Average Question Cost: ${avg_question_cost:.8f}\\n\"\n",
        "        summary += f\"Current LCB Question Cost Estimate: ${estimated_lcb:.8f}\\n\"\n",
        "\n",
        "    # Add knapsack metrics\n",
        "    summary += \"\\n=== KNAPSACK METRICS ===\\n\"\n",
        "    summary += f\"Average Models Selected per Step: {knapsack_metrics['avg_models_per_step']:.4f}\\n\"\n",
        "    if knapsack_metrics[\"total_budget_allocated\"] > 0:\n",
        "        summary += f\"Overall Budget Utilization: {knapsack_metrics['budget_utilization']:.4f}\\n\"\n",
        "    summary += \"By Position:\\n\"\n",
        "    for position, data in sorted(knapsack_metrics[\"by_position\"].items()):\n",
        "        if data[\"steps\"] > 0:\n",
        "            summary += f\"  Position {position}: {data['avg_models_per_step']:.2f} models/step, \"\n",
        "            if data[\"avg_budget\"] != \"Unlimited\":\n",
        "                summary += f\"Budget=${data['avg_budget']:.8f}, \"\n",
        "                summary += f\"Utilization={data['budget_utilization']:.4f}, \"\n",
        "            else:\n",
        "                summary += f\"Budget=Unlimited, \"\n",
        "            summary += f\"Cost=${data['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== MODEL PERFORMANCE ===\\n\"\n",
        "    for model, metrics in model_metrics.items():\n",
        "        if metrics[\"calls\"] > 0:\n",
        "            summary += f\"\\n{model}:\\n\"\n",
        "            summary += f\"  Overall: {metrics['successes']}/{metrics['calls']} = {metrics['success_rate']:.4f}\\n\"\n",
        "            summary += f\"  Average Cost: ${metrics['avg_cost']:.8f}\\n\"\n",
        "            summary += \"  By Position:\\n\"\n",
        "            for position, pos_data in sorted(metrics[\"by_position\"].items()):\n",
        "                if pos_data[\"calls\"] > 0:\n",
        "                    summary += f\"    Pos {position}: {pos_data['successes']}/{pos_data['calls']} = {pos_data['success_rate']:.4f}, Avg Cost: ${pos_data['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== LLM LCB COST ESTIMATION ACCURACY ===\\n\"\n",
        "    for model, data in lcb_cost_accuracy.items():\n",
        "        if data[\"count\"] > 0:\n",
        "            summary += f\"{model}: Avg LCB Est: ${data['avg_lcb_estimate']:.8f}, Avg Actual: ${data['avg_actual_cost']:.8f}, Ratio: {data['lcb_accuracy']:.4f}\\n\"\n",
        "\n",
        "    summary += \"\\n=== BATCH PERFORMANCE ===\\n\"\n",
        "    for batch in batch_metrics:\n",
        "        summary += f\"Batch {batch['batch_idx']}: {batch['success_count']}/{batch['batch_size']} = {batch['success_rate']:.4f}, Total Cost: ${batch['total_cost']:.8f}, Avg Cost: ${batch['avg_cost']:.8f}\\n\"\n",
        "\n",
        "    summary += f\"\\nTotal Overall Cost (All Questions): ${total_cost:.8f}\\n\"\n",
        "\n",
        "    # Print and save summary\n",
        "    print(summary)\n",
        "\n",
        "    # Save to appropriate file based on dataset type\n",
        "    summary_file_path = SUMMARY_STATS_PATH.replace(\".txt\", f\"_{dataset_type}.txt\")\n",
        "    with open(summary_file_path, 'w') as f:\n",
        "        f.write(summary)\n",
        "\n",
        "\n",
        "def select_questions_by_lcb_cost(remaining_questions, question_cost_estimator, batch_size, pool_multiplier=QUESTION_POOL_MULTIPLIER):\n",
        "    \"\"\"\n",
        "    Select the next batch of questions based on LCB cost estimates\n",
        "\n",
        "    Parameters:\n",
        "    - remaining_questions: List of unprocessed questions\n",
        "    - question_cost_estimator: The estimator for question costs\n",
        "    - batch_size: Number of questions to select\n",
        "    - pool_multiplier: How many times batch_size to consider\n",
        "\n",
        "    Returns:\n",
        "    - Selected questions for the next batch\n",
        "    \"\"\"\n",
        "    # Determine the pool size - consider at least batch_size questions\n",
        "    pool_size = min(len(remaining_questions), batch_size * pool_multiplier)\n",
        "    if pool_size <= batch_size:\n",
        "        # If we don't have enough questions for a meaningful pool, return the first batch_size\n",
        "        return remaining_questions[:batch_size]\n",
        "\n",
        "    # Get the candidate pool\n",
        "    candidate_pool = remaining_questions[:pool_size]\n",
        "\n",
        "    # Calculate LCB cost estimate for each question\n",
        "    question_costs = []\n",
        "    for i, question in enumerate(candidate_pool):\n",
        "        lcb_cost = question_cost_estimator.estimate_question_lcb_cost()\n",
        "        question_costs.append((i, lcb_cost))\n",
        "\n",
        "    # Sort by LCB cost (ascending)\n",
        "    question_costs.sort(key=lambda x: x[1])\n",
        "\n",
        "    # Get the indices of the lowest-cost questions\n",
        "    selected_indices = [idx for idx, _ in question_costs[:batch_size]]\n",
        "    selected_indices.sort()  # Sort indices to preserve original order\n",
        "\n",
        "    # Select the questions\n",
        "    selected_questions = [candidate_pool[idx] for idx in selected_indices]\n",
        "\n",
        "    # Get the remaining questions (those not in the pool + those in pool but not selected)\n",
        "    non_selected_pool_indices = set(range(pool_size)) - set(selected_indices)\n",
        "    non_selected_pool = [candidate_pool[idx] for idx in non_selected_pool_indices]\n",
        "    remaining_after_pool = remaining_questions[pool_size:]\n",
        "\n",
        "    # Update the remaining_questions list (in-place)\n",
        "    remaining_questions.clear()\n",
        "    remaining_questions.extend(selected_questions)  # Put selected questions first\n",
        "    remaining_questions.extend(non_selected_pool)   # Then non-selected from pool\n",
        "    remaining_questions.extend(remaining_after_pool)  # Then the rest\n",
        "\n",
        "    return selected_questions\n"
      ],
      "metadata": {
        "id": "gOKttQvHkXec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVVz9ADfilkN"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"Starting LinUCB Cascade with Knapsack Selection\")\n",
        "    initialize_json_files()\n",
        "    dataset = load_dataset(INPUT_JSON)\n",
        "    if not dataset:\n",
        "        print(\"Dataset is empty or could not be loaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "    feature_extractor = FeatureExtractor()\n",
        "    feature_extractor.initialize(dataset)\n",
        "    linucb_model = LinUCBModel(model_names=AVAILABLE_LLMS)\n",
        "    question_cost_estimator = QuestionCostEstimator()\n",
        "    if os.path.exists(QUESTION_COST_PATH):\n",
        "        question_cost_estimator.load_state(QUESTION_COST_PATH)\n",
        "    cascade = BatchBudgetCascade(feature_extractor, linucb_model)\n",
        "\n",
        "    all_records = []\n",
        "    if os.path.exists(RECORDS_PATH):\n",
        "        try:\n",
        "            with open(RECORDS_PATH, 'r') as f: all_records = json.load(f)\n",
        "            processed_questions = {r[\"problem\"] for r in all_records}\n",
        "            dataset = [q for q in dataset if q[\"problem\"] not in processed_questions]\n",
        "            print(f\"Loaded {len(all_records)} existing records. {len(dataset)} new questions to process.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not load existing records: {e}\")\n",
        "\n",
        "    if os.path.exists(LINUCB_MODEL_PATH):\n",
        "        linucb_model.load_model_state(LINUCB_MODEL_PATH)\n",
        "\n",
        "    train_size = int(len(dataset) * 0.2)\n",
        "    train_dataset, test_dataset = dataset[:train_size], dataset[train_size:]\n",
        "    print(f\"Split dataset into {len(train_dataset)} training and {len(test_dataset)} testing questions.\")\n",
        "\n",
        "    try:\n",
        "        # --- Training Phase ---\n",
        "        print(\"\\n=== PROCESSING TRAINING SET ===\")\n",
        "        for idx, question in enumerate(train_dataset):\n",
        "            print(f\"\\nProcessing Training Question #{idx + 1}/{len(train_dataset)}\")\n",
        "            question_record = cascade.run_cascade_single_question(question, question_cost_estimator, is_training=True)\n",
        "            question_record[\"dataset\"] = \"train\"\n",
        "            all_records.append(question_record)\n",
        "            if (idx + 1) % UPDATE_FREQUENCY == 0:\n",
        "                save_records_with_backup(all_records, RECORDS_PATH)\n",
        "                linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "                question_cost_estimator.save_state(QUESTION_COST_PATH)\n",
        "\n",
        "        # --- Testing Phase ---\n",
        "        print(\"\\n=== PROCESSING TEST SET ===\")\n",
        "        for idx, question in enumerate(test_dataset):\n",
        "            print(f\"\\nProcessing Test Question #{idx + 1}/{len(test_dataset)}\")\n",
        "            # Using a fixed, tuned budget for reproducibility in the test phase\n",
        "            test_budget = 0.00014217\n",
        "            question_record = cascade.run_cascade_single_question(question, question_cost_estimator, is_training=False, initial_question_budget=test_budget)\n",
        "            question_record[\"dataset\"] = \"test\"\n",
        "            all_records.append(question_record)\n",
        "            if (idx + 1) % UPDATE_FREQUENCY == 0:\n",
        "                save_records_with_backup(all_records, RECORDS_PATH)\n",
        "                linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "                question_cost_estimator.save_state(QUESTION_COST_PATH)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProcessing interrupted. Saving progress...\")\n",
        "    finally:\n",
        "        save_records_with_backup(all_records, RECORDS_PATH)\n",
        "        linucb_model.save_model_state(LINUCB_MODEL_PATH)\n",
        "        question_cost_estimator.save_state(QUESTION_COST_PATH)\n",
        "\n",
        "        # Analyze and report results\n",
        "        train_records = [r for r in all_records if r.get(\"dataset\") == \"train\"]\n",
        "        test_records = [r for r in all_records if r.get(\"dataset\") == \"test\"]\n",
        "        print(\"\\n--- FINAL ANALYSIS ---\")\n",
        "        analyze_results(train_records, question_cost_estimator)\n",
        "        analyze_results(test_records, question_cost_estimator)\n",
        "        print(\"\\nLinUCB Cascade with Knapsack Selection completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}